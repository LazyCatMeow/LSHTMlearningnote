# (PART) 等級線性迴歸模型 analysis of hierarchical and other dependent data {-}


# 相互依賴數據及簡單的應對方案 {#Hierarchical}

> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
>
> ~ Sir Ronald Aylmer Fisher


```{block2, note-thankslinda, type='rmdnote'}
The Analysis of Hierarchical and Other Dependent Data lectures were orgainised and taught by Professor [Linda Sharples](https://www.lshtm.ac.uk/aboutus/people/sharples.linda), and Dr. [Edmund Njeru Njagi](https://www.lshtm.ac.uk/aboutus/people/njagi.edmund-njeru).
```


```{r Hier-Session01, child = ('10-Hierarchical/Session01.Rmd')}
```


## Practical Hierarchical 01

```{r Hier-Practical01, child = ('10-Hierarchical/Practical01.Rmd')}
```


# 隨機截距模型 random intercept model {#random-intercept}

```{r Hier-Session02, child = ('10-Hierarchical/Session02.Rmd')}
```




## Practical Hierarchical 02

```{r Hier-Practical02, child = ('10-Hierarchical/Practical02.Rmd')}
```




# 隨機截距模型中加入共變量 random intercept model with covariates {#random-inter-cov}


```{r Hier-Session03, child = ('10-Hierarchical/Session03.Rmd')}
```


## Practical Hierarchical 03

```{r Hier-Practical03, child = ('10-Hierarchical/Practical03.Rmd')}
```



# 隨機回歸系數模型  random coefficient model {#random-coefficient}


```{r Hier-Session04, child = ('10-Hierarchical/Session04.Rmd')}
```


## Practical Hierarchical 04

```{r Hier-Practical04, child = ('10-Hierarchical/Practical04.Rmd')}
```




# 縱向研究數據 longitudinal data 1 {#longitudinal1}
 
 
```{r Hier-Session05, child = ('10-Hierarchical/Session05.Rmd')}
```




## Practical 05-Hier



```{r Hier-Practical05, child = ('10-Hierarchical/Practical05.Rmd')}
```



# 縱向研究數據 longitudinal data 2 {#longitudinal2}

本章沒有代碼，學會如何用矩陣標記法寫下你的多元混合效應模型。

## 邊際結構 marginal structures

至此，我們接觸過的各種混合效應模型其實代表的是數據不同的邊際結構關系 (marginal relations)。

### 隨機截距模型

縱向數據中，數據可能是平衡或不平衡數據，簡單的隨機截距模型可以標記如下: 

$$
Y_{ij} = (\beta_0 + u_{0j}) + \beta_1 t_{ij} + e_{ij}
$$

這個模型隱含着如下的條件關系 (conditional relation):

$$
\begin{aligned}
Y_{ij} | t_{ij}, u_{0j} & \sim N(\beta_0 + \beta_1t_{ij} + u_{0j}, \sigma^2_e)\\ 
 u_{0j}|t_{ij} & \sim N(0, \sigma^2_u) \\
 \text{Var}(Y_{ij} | t_{ij}, u_{0j})  & = \sigma^2_e
\end{aligned}
$$

也就是說，觀測值 $Y_{ij}$ 以時間 $t$，和隨機截距 $u_0$ 爲條件的方差，只取決於 $\sigma^2_e$。所以，屬於同一層 (同一患者不同測量時間) 的測量值，以該層 (患者) 的截距爲條件 (conditional on $u_j$) 的協方差是 $\text{Cov} (Y_{ij}, Y_{i*j}|t_{ij}, t_{i*j}, u_j) = 0$。

$Y_{ij}$ 針對 $u_j$ 的邊際期望 (marginal espectation with respect to $u_j$): 

$$
E(Y_{ij}|t_{ij}) = \beta_0 + \beta_1 t_{ij}
$$

其方差爲 $\text{Var}(Y_{ij}|t_{ij}) = \sigma^2_u + \sigma^2_e$，同一層 (同一患者) 的兩個不同時刻測量值之間的邊際協方差就是 $\text{Cov}(Y_{ij}, Y_{i*j}|t_{ij},t_{i*j}) = \sigma^2_u$。

### 隨機系數模型

模型的數學標記是

$$
Y_{ij} = (\beta_0 + u_{0j}) + (\beta_1 + u_{1j})t_{ij} + e_{ij}
$$

等同於

$$
Y_{ij} = (\beta_0 + \beta_1t_{ij}) + (u_{0j} + u_{1j}t_{ij}) + e_{ij}
$$


其**條件關系**是 

$$
Y_{ij}|t_{ij},u_{0j},u_{1j} \sim N( \beta_0 + \beta_1t_{ij} + u_{0j} + u_{1j}t_{ij}, \sigma^2_e)
$$

其中， $\mathbf{u}_j|t_{ij} \sim N(0, \mathbf{\Sigma}_u)$，且

$$
\mathbf{\sum}_{\mathbf{u}}  =\left( \begin{array}{cc}
              \sigma^2_{u_{00}} & \sigma_{u_{01}} \\
              \sigma_{u_{01}}   & \sigma^2_{u_{11}} \\
              \end{array} \right)
\\
\text{Cov} (Y_{ij}, Y_{i*j}|t_{ij}, t_{i*j}, u_{oj}, u_{1j}) = 0
$$

其所指的$Y_{ij}$的邊際分布: 

$$
\begin{aligned}
E(Y_{ij}|t_{ij})   & = \beta_0 + \beta_1t_{ij} \\
\text{Var}(Y_{ij}) & = \sigma^2_{u_{00}}  +2\sigma_{u_{01}}t_{ij} + \sigma^2_{u_{11}}t_{ij}^2 + \sigma^2_e \\
\text{Cov}(Y_{ij}, Y_{i*j}) & = \text{Cov}(u_{0j} + u_{1j}t_{ij} + e_{ij}, u_{0j} + u_{1j}t_{i*j} + e_{i*j}) \\
                   & = \sigma^2_{u_{00}} + \sigma_{u_{01}}(t_{ij} + t_{i*j}) + \sigma^2_{u_{11}}t_{ij}t_{i*j} \text{ (for } i \neq i*) \\
\text{Cov}(Y_{ij}, Y_{i*j*}) & = \text{Cov}(u_{0j} + u_{1j}t_{ij} + e_{ij}, u_{0j*} + u_{1j*}t_{i*j*} + e_{i*j*}) \\ 
& = 0 \text{ (for } j \neq j*) 
\end{aligned}
$$

也就是說**同一層 (同一患者) 的不同測量值之間的協方差不爲零，是時間的函數**。

## 矩陣記法

如果數據本身是**平衡數據**，可以用如下的矩陣標記混合效應模型，

- $j$ 是每個患者 (第二層級)，$\mathbf{Y}_j, \mathbf{e}_j$ 向量被定義爲: 

$$
\begin{aligned}
\mathbf{Y}_j & =  \left( \begin{array}{c}
Y_{1j} \\
Y_{2j} \\
\cdots \\
\cdots \\
Y_{nj}
\end{array}
\right) \\
\mathbf{e}_j & =  \left( \begin{array}{c}
e_{1j} \\
e_{2j} \\
\cdots \\
\cdots \\
e_{nj}
\end{array}
\right) \\
\end{aligned}
$$

用三次測量時間 $t_1, t_2, t_3$ (以簡便標記) 來繼續接下來的推導，定義矩陣 $\mathbf{T}, \mathbf{\beta}, \mathbf{u}_j$: 

$$
\mathbf{T} = \left(\begin{array}{c}
1 & t_1 \\
1 & t_2 \\
1 & t_3 
\end{array}
\right) \\
\mathbf{\beta} = \left( \begin{array}{c}
\beta_0 \\
\beta_1 
\end{array}
\right) \\
\mathbf{u}_j = \left(\begin{array}{c}
u_{0j} \\
u_{1j} 
\end{array}
\right)
$$

如此經過利用定義好的向量，我們就可以把模型用矩陣標記來記錄，從無窮無盡的下標中解放出來: 

$$
\mathbf{Y = T\beta + Tu + e} \\ 
\text{Where } \mathbf{u} \sim N(0, \mathbf{\Sigma}_u) \\ 
              \mathbf{e} \sim N(0, \sigma^2_e\mathbf{I})
$$

那麼 

$$
\text{Var}(\mathbf{Y}) = \mathbf{T\Sigma}_u\mathbf{T}^T + \sigma^2_e \mathbf{I}
$$

## 混合效應模型的一般化公式

前面的例子用的雖然是時間做解釋變量 (縱向數據)，但是也可以推廣到一般的混合效應模型: 

$$
\mathbf{Y = T\beta + Zu + e}
$$

其中 $\mathbf{Z}$ 是類似 $\mathbf{T}$ 的共變量矩陣。類似地，$\mathbf{Y}$ 的方差是: 

$$
\text{Var}(\mathbf{Y}) = \mathbf{Z\Sigma}_u\mathbf{Z}^T + \mathbf{\Sigma}_e \\
\mathbf{Y} \sim N(\mathbf{T\beta}, \mathbf{Z\Sigma}_u\mathbf{Z}^T + \mathbf{\Sigma}_e )
$$

這就是一個多元線性混合效應回歸模型，大多數情況下，$\mathbf{\Sigma}_e = \sigma^2_e\mathbf{I}$。


## 其他可選擇的方差協方差矩陣特徵 

學會了上面的矩陣標記以後，就應該了解在這樣的多元混合效應模型中，對於層內方差，協方差矩陣的 $\mathbf{\Sigma_u}$ 結構初步假設是相當重要的。目前爲止我們接觸過的模型的方差協方差矩陣結構列舉如下 (爲了簡便標記都用$3\times3$ 的矩陣來表示): 

- 復合對稱結構 (compound symmetry structure - compound symmetry model) 又名爲可交換結構 (exchangeable structure)

$$
\mathbf{\sum}_{\mathbf{u}}  =\left( \begin{array}{cc}
              \sigma^2_{u} + \sigma^2_e & \sigma^2_{u}             &  \sigma^2_{u} \\
              \sigma^2_{u}              & \sigma^2_{u} + \sigma^2_e& \sigma^2_{u}  \\
              \sigma^2_{u}              & \sigma^2_{u} & \sigma^2_{u} + \sigma^2_e \\
              \end{array} \right)
$$

- 隨機系數結構 random coefficient (RC) structure

$$
\mathbf{\sum}_{\mathbf{u}}  =\left( \begin{array}{cc}
              \sigma^2_{u_{00}} + \sigma^2_e       & \sigma^2_{u_{00}} + \sigma_{u_{01}} &  \sigma^2_{u_{00}} + 2\sigma_{u_{01}} \\
              \sigma^2_{u_{00}} + \sigma_{u_{01}}  & \sigma^2_{u_{00}} + 2\sigma_{u_{01}} + \sigma^2_{u_{11}} + \sigma^2_e& \sigma^2_{u_{00}} + 3\sigma_{u_{01}} + 2\sigma^2_{u_{11}}  \\
              \sigma^2_{u_{00}} + 2\sigma_{u_{01}} & \sigma^2_{u_{00}} + 3\sigma_{u_{01}} + 2\sigma^2_{u_{11}} & \sigma^2_{u_{00}} + 4\sigma_{u_{01}} + 4\sigma^2_{u_{11}}+\sigma^2_e \\
              \end{array} \right)
$$

除了這兩個結構以外其他常見方差寫方差結構還有: 

- 自回歸結構 (autoregressive structure): 

$$
\frac{\phi}{1-\alpha^2} \left(\begin{array}{ccc}
1 & \alpha & \alpha^2 \\
\alpha & 1  & \alpha \\
\alpha^2 & \alpha  & 1
\end{array}
\right)
$$

- 無固定結構 (unstructure): 


$$
\left(\begin{array}{ccc}
\sigma_{11} & \sigma_{12}  &\sigma_{13} \\
\sigma_{21} & \sigma_{22}  &\sigma_{23} \\
\sigma_{31} & \sigma_{32}  &\sigma_{33}
\end{array}
\right)
$$

最後不要忘記了還有完全獨立結構 (不需要任何復雜模型或校正其數據間的依賴性): 

$$
\sigma^2\left(\begin{array}{ccc}
1 & 0  & 0 \\
0 & 1  & 0 \\
0 & 0  & 1
\end{array}
\right)
$$

## 其他要點評論

- 各種結構模型之間的相互比較 

    - 似然比檢驗法 the likelihood ratio test (LRT) <br> 前提是模型的固定結構不發生改變，兩個嵌套式模型之間的比較是可以使用死然比檢驗的。缺點是統計學效能可能不太理想 (low power) 
    - 模型的比較指標 information criteria <br> 就算是同一個數據，如果不同的協方差結構矩陣模型的固定效應部分也不同，似然比檢驗也不使用，這時候應該求助於赤池信息量 (Akaike's Information Criterion, AIC)，或者貝葉斯信息量 (Bayesian Criterion, BIC) 的比較。這兩個信息量都是使用的模型的似然減去相應模型的參數數量作爲評判標準。差別是 BIC 對參數的調整更加大些。但是，沒人可以保證這些信息會永遠相互認證，他們可能出現互相矛盾，也沒人可以保證使用這些信息的比較可以證明你的模型是"最佳"模型。
    
## 不平衡數據

- 有缺失值的數據，我們無法使用已知的協方差結構矩陣; 
- 隨機效應模型，隨機系數模型可以用於不平衡數據，所以即使有缺失值，我們可以從混合效應模型的結果來推測數據暗示我們數據中存在着怎樣的協方差結構;


## Practical 06-Hier


# 縱向研究數據 longitudinal data 3 {#longitudinal3}

## 第一層級的異質性 level 1 heterogeneity

目前爲止，我們使用討論過的模型，其實還默認另一個前提條件: 第一層級和第二層級的隨機誤差的方差是固定不變的 (level 1 and level 2 error variance are constant)。但是實際上我們可以把這個條件放寬，讓模型允許第一層級隨機誤差的方差根據某個解釋變量而不同，使得模型更加接近數據，這種模型被命名爲 **復雜第一層級方差模型 (complex level 1 variation)**。下面繼續使用 Asian growth data 來做說明。該數據測量了幾百名亞洲兒童在0-3歲之間幾個時間點的體重。現在我們來允許其第一層級 (每一個兒童在不同時間點測量的體重) 誤差方差隨着性別的不同而變化: $\sigma_e = f(\text{gender})$。這裏的方程爲了防止標準差變成負的而使用對數函數: 

$$
\text{log} (\sigma_e) = \delta_1I_{\text{gender = boy}} + \delta_2I_{\text{gender=girl}}
$$

這個加入了第一層級方差隨機性的模型在 R 裏可以這樣擬合: 

```{r Hier07-01, cache=TRUE}
M_growth_l1 <- lme(fixed = weight ~ age + age2 + gender, random = ~ age | id, weights = varIdent(form=~1|gender), data = growth, method = "REML", na.action = na.omit)
summary(M_growth_l1)

# 和之間默認男女兒童的誤差方差相等時的模型做比較
# 沒有顯著差異 (p = 0.09)
anova(M_growth_l1, M_growth_mix)
```

## 第二層級異質性 level 2 heterogeneity

我們還可以在模型中允許第二層級的結構不一樣，這等同於認爲這是一個三個層級的模型，其中第二層級分裂成男孩和女孩。

```{r Hier07-02, cache=TRUE}
M_growth_l2 <- lme(fixed = weight ~ age + age2 + gender, 
                   random = ~ age*gender | id,
                   data = growth, method = "REML", na.action = na.omit)
summary(M_growth_l2)

growth <- growth %>%
  mutate(boy = as.numeric(gender == "Boys"), 
         girl = as.numeric(gender == "Girls")) %>%
  mutate(age_boy = age*boy, 
         age_girl = age*girl)         

#M <- lmer(weight ~ age + age2 + girl + (age_boy |id) + (age_girl| id), data = growth, REML = TRUE)

#growth <- growth %>%
#  mutate(boy = ifelse(gender == "Boys", 1, 0), 
#         girl = ifelse(gender == "Girls", 1, 0), 
#         age_boy = age*boy, 
#         age_girl = age*girl)
#M_growth_l22 <- lme(fixed = weight ~ age + age2 + girl, 
#                    random = list( ~ girl + age_girl | id, 
#                                   ~ boy + age_boy | id),
#                   data = growth, method = "REML", na.action = na.omit)
#summary(M_growth_l22)
M_growth <- lme(fixed = weight ~ age + age2 + gender, random = ~ age|id, data = growth, method = "REML",na.action = na.omit) 
anova(M_growth_l2, M_growth)
```


## 分析策略

進行統計建模之前，請思考你想從數據中探尋什麼問題的答案? 

1. 是想了解某一個共變量在層內 (同一個體不同時間，或者統一學校不同學生之間) 的條件效應 (conditional effect)?
2. 是想探索層內和層間數據的變化程度?
3. 是想了解一個共變量的邊際效應 (marginal effect) 嗎?

如果是 1 或 2 兩個問題的話，請使用混合效應模型。如果是 1，但是那個共變量卻不是定義於層水平的，那就只好放棄回到簡單的固定效應模型。如果是 3，需要考慮使用 GEE。

### 模型選擇和建模步驟

詳細請參考 [@Verbeke1997]。

當擬合一個混合效應模型時，意味着均值的結構和協方差的結構可以被確定 (an appropriate mean structure as well as covariance structure is specified)。協方差結構，解釋了均值結構無法解釋的數據隨機變化，所以二者之間彼此高度互相依賴。另外，適當的協方差模型對於用數據進行人羣參數的有效統計推斷過程是必不可少的。

- 第一步: 

由於固定效應部分不能完美解釋數據的變異，所以協方差結構就是用來輔助解釋這部分數據變異的輔助工具。建模的起點就應該是，先建立一個飽和 (甚至是過飽和 overelaborated) 的模型給均值結構 (固定效應部分)，從而確保之後要增加的隨機效應部分不受固定效應部分的擬合錯誤影響。所以，開始建模時，要先把所有可能考慮到的固定效應全部加入模型中去 (包括連續變量的二次方形式/或其他非線性關系，包括所有變量之間的交互作用)。這樣做其實是使用過度飽和的參數使得均值結構在模型中盡量在後面加入隨機效應之前保持不變。在可選的那些數據結構中，我們也應當考慮到數據中不同層級結構可能存在的異質性。要注意的是，隨機效應部分，不能也不應該在沒有把所有可能的一次方程結構都考慮進去之後 (a random effect for the linear effect of time)，就上馬二次方程/或更高次方程的隨機效應(a random effect for the quadratic effect of time)。
然後我們把飽和模型的殘差 (residuals)，異常值 (outliers)，擬合值 (fitted values)，和可能的 (potential) 隨機效應模型作出的這些殘差，異常值，擬合值之間進行比較。

- 第二步:

一旦你在飽和模型的條件下，確認好了隨機效應應該有的形式，接下來就是逐步精簡模型固定效應部分的過程: 

1. 用 Wald 檢驗 (當使用 REML 時)，或者 LRT (使用 ML 時) 來精簡化固定效應部分。
2. 反復檢查殘差，異常值，以及擬合值跟觀測值
3. 使用模型的預測軌跡和觀測值的點做視覺比較
4. 用人話把你的模型解釋給老奶奶聽懂






# 廣義估計方程式 Generalized Estimating Equation
 
# 聚類分析 Cluster analysis/unsupervised learning  {#cluster-ana}

目前爲止，在等級回歸模型部分中，我們接觸到的回歸模型和可能存在相互依賴性的數據，都是建立在我們能夠觀察到或者實驗設計上已知的數據層級結構的前提下的。這樣的層級可以是空間上的，或者時間上的。處在相同層級的研究對象之間存在相關性，換句話說就是：層級內部的對象之間，比起層級之間的對象具有更多的相似性。

但是，在許多情況下，我們其實是無法事先知道數據的內部層級（聚類）結構的。而且我們可能需要儘可能多的獲取數據，並且從測量的數據中學習。學習數據變量與變量之間的相關性(correlation)，變量與變量之間的協方差(covariance)，個體與個體之間的相似性，從而根據獲取的數據來判斷數據內部是否存在不同的層級結構。這樣的一種對數據結構進行探索的過程，在機器學習(maching learning)中也是常常使用的，它又被叫做**非監督學習 (unsupervised learning)**。

之所以把這類尋找數據分類分層結構的過程叫做非監督學習，其實，是爲了和現在越來越豐富，多到令人髮指的那些被歸類於**監督學習(supervised learning)**的方法作爲相互對照。在監督學習中，數據內部的分層，聚類結構是事先知道的，也就是事先能夠測量或者被定義好的。事先被定義好了的數據層級結構中，我們可以使用多元變量分析，來對某些個體的特徵加以分類，也就是給數據中的未知成員分配**已知的分組**的過程。

在醫學中常見的非監督學習過程實例之一是，對於一個（全部相同疾病的）隊列研究中的受試者進行了大量的生物標幟物(biomarker)的測量與收集，可以是血液樣本的 biomarker 的測量，也可以是每名受試者的全部DNA信息。研究者希望通過這些患者的信息對他們進行同一疾病不同等級（類別，或者進程）的分類。那麼研究者需要利用這些收集來的患者信息，建立一套儘可能完善的分類的系統。

另外一個例子是，我們收集了前列腺癌患者的前列腺組織，利用基因轉錄組學 (transcriptomics) 的方法測量了每名患者成千上萬的組織內基因表達。研究者希望通過這些數據來分析，提取，並且分辨這些前列腺癌患者中可能存在的分類，或者亞型。研究者也希望知道這些分析獲得的亞型，是否會和某些已知的癌症的亞型相似或者相重合。

在商業領域中，聚類分析也是不罕見的。例如你爲某商業公司工作，那麼食品供應商可能會上門來要求你把購買食物的顧客進行類別區分，從而提供給食物供應商們一些線索，讓他們能夠更加精準的定位廣告投放人羣。



在統計學，和機器學習領域中，有許多不同的手法，可以用來輔助建立這種分類的規則，它們通常又被叫做判別分析法(discriminant analysis methods)。我們這一章和下一章着重討論

1. 聚類分析法 (cluster analysis)
2. 主成分分析法 (principal component analysis)

## 聚類分析過程

聚類分析法是一種分析不同統計測量值之間相似/差異程度的描述性分析過程。

爲什麼我們總是想對具有相似性質的事物進行歸類？其實，對事物進行區分和歸類，或者打上一些標籤，是人類文明在學習並且理解周圍的世界，從而促進科學發展的核心問題之一。在原始社會，對相似事物進行歸類有時候甚至事關生死。例如人類最初需要判定某些食物的共同特徵，區分哪些是含有毒性的，哪類動物可能是兇猛殘忍的。我們從嬰兒時期開始學習語言，學習事物/事件/人物的名稱，這其實也是一個學習對周圍的世界進行區分的學習過程。古代希臘文明的先賢哲學家亞里士多德曾經主張，人類的本能之一，就是不停地想對這個我們生活的世界發生的事情看到的事物進行類別的區分，尋找相似的特徵，區別不一樣的性質。在生物學中，甚至有由亞里士多德的學生[泰奧夫拉斯托斯(Theophrastos)](https://zh.wikipedia.org/wiki/%E6%B3%B0%E5%A5%A7%E5%BC%97%E6%8B%89%E6%96%AF%E6%89%98%E6%96%AF)創立的專門對生物進行分類的學科，生物分類學 (taxonomy)，後被瑞典人生物學家[卡爾林納斯 (Carl Linnaeus)](https://en.wikipedia.org/wiki/Carl_Linnaeus)進一步發揚光大。18世紀末，[Michel Adanson](https://en.wikipedia.org/wiki/Michel_Adanson)又爲人類引入了多元分析(polythetic)的分類系統概念，取代了之前使用單一因素(monothetic)對事物進行簡單分類的思想。很顯然，生物分類學在人類文明史中扮演了重要的角色。你應該很容易能想到達爾文提出的進化論，就是建立在前人對動植物進行了事無鉅細的分類和整理的基礎之上建立起來的重大理論突破。俄國科學家[門捷列夫](https://zh.wikipedia.org/wiki/%E5%BE%B7%E7%B1%B3%E7%89%B9%E9%87%8C%C2%B7%E4%BC%8A%E4%B8%87%E8%AF%BA%E7%BB%B4%E5%A5%87%C2%B7%E9%97%A8%E6%8D%B7%E5%88%97%E5%A4%AB)發現化學元素週期性，並且製作出了世界上第一章元素週期表，也爲人類理解原子世界奠定了基石。


在對事物進行分類這個任務上，聚類分析(cluster analysis)，和判別分析是相同的。有時候在已知對象的分類情況時我們仍然傾向於使用聚類分析的方法，用它來描述數據的一些特徵。同時也能有助於判定之後可能進行的判別分析是否準確。

簡單歸納，對分類描述過程進行量化的主要步驟有以下幾個：

1. 對於採集來的樣本數據 (statistical sample)，我們儘可能多的對它們的特徵變量進行測量。

2. 根據第一步獲得的變量信息，定義一個能夠幫助我們判定對象與對象之間相似點或者不同程度的測量指標。

3. 對這個測量指標制定一個區分的規則，或者叫做歸類的標準。

4. 對樣本進行分類。

5. 採集更多的樣本，對分類規則進行調整和完善。


### 連續型變量 continuous variables in cluster analysis

我們想象手裏的數據是一個矩陣 $X$，它的維度是 $n \times p$，用 $x_{ik}$，來表示第 $i$ 名觀察對象 $(i = 1, \dots, n)$ 的第 $k$ 個變量 $(k = 1, \dots, p)$ 的值。如果這些被測量的變量全部都是連續型變量的話，每個變量可以被使用幾何學的形式表達的 $p$ 個維度的其中一個平面上。當然，當維度超過3時，人類的無知大腦常常就無法進行有效的想象和推理，我們這裏使用簡單的三個變量，也就是三維空間來表示三個測量獲得的連續型變量：

例如我們測量了三名學生的身高，體重，以及前臂長。數據分別是：Angelo (190, 75, 30)；Dimitris (170, 75, 25)；Soren (170, 65, 30)。

```{r cluster00, echo=FALSE, cache=TRUE, fig.asp=.7, fig.width=6, fig.cap='A physical 3D space showing measurements of three variables.', fig.align='center', out.width='80%'}
library(plotly)

clus_data <- data.frame(Name = c("Angelo", "Dimitris", "Soren"), 
                        Height = c(190, 175, 170), 
                        Weight = c(75, 75, 65), 
                        Forearm = c(30, 25, 30))
p <- plot_ly(clus_data, x = ~Height, y = ~Weight, z = ~Forearm, color = ~Name) %>% 
  add_markers() %>% 
  layout(xaxis = list(range = c(0, 200))) %>% 
  layout(scene = list(xaxis = list(title = "Height (cm)"), 
                      yaxis = list(title = "Weight (cm)"), 
                      zaxis = list(title = "Forearm (cm)")))
p

```


在這個三維立體空間，我們需要定義一個變量用於丈量點與點之間的距離。其中最自然的就是歐幾里德(Euclidean)幾何距離:

$$
d_{ij} = \{\sum_{k = 1}^p(x_{ik} - x_{jk})^2\}^{\frac{1}{2}} 
$$


- 歐幾里德幾何距離又被稱爲 **L2 度量衡 (L2 metric)**。按照這個距離的定義，那麼 Angelo 和 Dimitris 之間的歐幾里德幾何距離就是：

$$
\begin{aligned}
& \{(190 - 175)^2 + (75 - 75)^2 + (30 - 25)^2 \}^{\frac{1}{2}} \\
= & \sqrt{15^2 + 0^2 + 5^2} \\ 
= & \sqrt{240} = 15.5
\end{aligned}
$$

- 曼哈頓距離 (Manhattan distance)：別名城市區塊度量衡 (cityblock metric)，或者**L1 度量衡**

$$
d_{ij} = \sum_{k = 1}^p |x_{ik} - x_{jk}|
$$

按照曼哈頓距離來定義的話，Angelo 和 Dimitris 之間的距離就是：

$$
|190 - 175| + |75 - 75| + |30 - 25| = 15 + 0 + 5 = 20
$$


後來人們發現上面提到的這兩種幾何學距離其實是閔科夫斯基度量衡 (Minkowski metric) 在 L=1 和 L=2 時的特殊情況。

閔科夫斯基度量衡的一般形式表達爲: 

$$
d_{ij} = \{ \sum_{k = 1}^p |x_{ik} - x_{jk}|^\ell \}^\frac{1}{\ell}
$$

閔科夫斯基度量衡試圖給差距較大的測量值之間增加權重用於區分彼此。不論是使用那種距離定義，這些測量距離的度量衡都具有如下的數學性質 (mathematical properties)：

1. 兩點之間的距離大於等於零, positivity <br> $d_{ij} \geqslant 0$，如果 $d_{ij} = 0$，那麼對於任何一個 $k = 1, \dots, p$，它們都是相等的 $x_{ik} = x_{jk}$。
2. 對稱性, symmetry <br> $d_{ij} = d_{ji}$
3. 三角形不等性, triangle inequality <br> $d_{ij} \leqslant d_{ih} + d_{hj}$

### 二分類或者分類型變量之間的距離 distances for binary/categorical variables

假如變量本身並不是連續型的，那麼閔科夫斯基度量衡並不適用，因爲二分量只能取0或者1。如下表所表示的，我們把 **i,j** 兩名對象的所有二分類變量進行下面的歸納總結：

| i/j | 1 | 0 |
|:---:|:-:|:-:|
|  1  | a | b |
|  0  | c | d |

其中，

- a 表示 i, j 兩名研究對象的二分類變量中，同時取 1 的變量的個數，
- b 表示 i, j 兩名研究對象的二分類變量中，i 取 1 但是 j 取 0 的變量的個數，
- c 表示 i, j 兩名研究對象的二分類變量中，j 取 1 但是 i 取 0 的變量的個數，
- d 表示 i, j 兩名研究對象的二分類變量中，同時取 0 的變量的個數。


根據這個總結表格，常用的表示兩個對象之間距離的數學度量是： 

1. 簡單匹配係數 (simple matching coefficient, SMC)，單純地計算所有的變量之中互相不一致的變量所佔的百分比： $$d_{ij} = \frac{b + c}{a+b+c+d}$$

2. 亞卡爾距離係數 (Jaccard coefficient)，則是把簡單匹配係數的分母中，d 的部分拿掉：$$d_{ij} = \frac{b + c}{a + b + c}$$

[其中亞卡爾距離係數更適合用於測量一些表達某些特質存在/不存在時兩名對象之間的距離測量 (see the "Difference with the simple matching coefficient (SMC)" session in the Wikipedia)](https://en.wikipedia.org/wiki/Jaccard_index)。



另外值得注意的是，在測量二分類變量距離的時候，三角形不等性的特質不一定會得到滿足。 (Please note that in general for dichotomous variables, the triangle inequality does not hold.)

用來計算測量對象之間距離的方法，和度量衡其實層出不窮，這裏只是簡單介紹了幾種。其餘的還有比如說由 [@Gower1971] 提出的 [Gower Index](https://cran.r-project.org/web/packages/gower/vignettes/intro.html)，該指標可以同時把測量有連續型變量和分類型變量，二分類變量等都包含進來。值得提醒的是，如果是討論非連續型測量值的對象距離，我們常常用它們之間的相似性(similarities) $s_{ij}$，而不太關注異質性 (dissimilarities) $d_{ij}$，但其是它們之間的簡單轉換關係就是 $d_{ij} = 1 - s_{ij}$。

### 定義分類方法

確定了用於衡量異質性 (dissimilarity) 距離的指標之後，我們就需要來定義分類的方法。首先把這個事先定下來的距離指標應用到我們的多元變量數據矩陣 (multivariate data matrix $\mathbf{X}$) (dimension: $n\times p$, where n indicates number of people, p indicates number of observed variables). 獲得一個形狀爲 $n\times n$ 的距離矩陣 $\mathbf{D}$ (對應上面三條數學性質中的第二條，對稱性 $d_{ij} = d_{ji}$)。獲得觀察對象的距離矩陣 $\mathbf{D}$ 之後需要決定的就是如何給對象進行分組的策略。該分組策略需要能使觀察對象被分組後，組內的對象相對組外對象更加相似，或者組外對象相對組內對象更加不同 (a sensible strategy would be to look for sets of units such that all units in that set are relatively similar to each other but relatively different from all units outside that set)。所以，用於分組策略的算法要有一定的可行性，它還要能夠量化對象之間的相對相似性 (relative similarity) 從而能夠完成以下任務：

1. 決定哪些人/對象被聚類到同一組中 (which pairs of units to join together into a cluster)

2. 每次聚類過程完成以後，重複相同的策略和算法，也就是重新計算新組成的聚類和剩餘的對象之間的距離。

3. 循環往復前兩個步驟直至全部的對象/個體都被分到各自的聚類 (cluster)。

事實上重複上述步驟，最終會把每個個體都分配到一個單獨的聚類中，也就是每個個體本身，那其實就跟沒有做聚類分析沒有區別，也沒有意義了。於是我們需要把聚類分析的過程通過圖形的方式展示出來。這樣的圖形被叫做**樹狀圖 (dendrogram)**，可以在視覺上輔助我們做出要給對象分成多少個聚類的決定。在希臘語中(Greek)，dendron 是樹的意思，樹狀圖的形狀常見的如下圖 (\@ref(fig:cluster01)) 所示，座標軸之一是所有的觀測對象的編號，另一個座標軸則是度量每個聚類或者觀測對象個體之間的距離。

```{r cluster01, cache=TRUE, fig.asp=.7, fig.width=7, fig.cap='Example of dendrogram vertically oriented, with 50 statistical units (average linkage method and Euclidean distance measure).', fig.align='center', out.width='80%', echo=FALSE}
plant <- read_dta("backupfiles/plant.dta")
plant <- plant[, 1:4]

# prepare hierarchical cluster
hc <-  hclust(dist(plant), "ave")


plot(hc, cex = 0.8, hang = -1, 
     main = "", ylab = "L2 dissimilarity measure", 
     xlab = "No. of specimen")
```

那麼回到之前如何決定聚類數量的問題上來，我們有兩種手段來輔助：

1. 層級法 (hierarchical methods)：聚合法，agglomerative； 或者分裂法， divisive。
2. 分區算法 (partitioning methods)。

層級法中的**聚合法 (agglomerative)**是指，從聚類分析的開始階段，每個獨立的對象自成一個聚類 (cluster)，所以起步於 n 個統計單位 (n statistical units)，之後的每一步聚類過程則是將度量距離相近的對象合併成爲一個聚類，直至最終所有個體歸爲唯一一個聚類。所以可以想象爲從各個枝葉彙總到一個樹幹走向各個枝葉的過程。

層級法中的**分裂法 (divisive)**則是和聚合法的聚類方向反過來，它起始於將所有觀察對象視爲唯一一個聚類，之後每一步聚類過程是將和大部分對象不太相似的個體從聚類中分裂出去，直至最終每個獨立的對象自成一個聚類。所以可以想象成從一個樹幹走向枝葉的過程。

分裂法其實十分消耗計算機的運算能力，因爲當樣本量較大時，一個 $k$ 種聚類的步驟就需要比較 $2^{k-1} -1$ 種不同的分區之間的距離。




# 主成分分析 Principal Component Analysis  {#PCA}

> A big computer, a complex algorithm and a long time does not equal science.
> ~ Robert Gentleman


```{block2, note-text, type='rmdnote'}
PCA lecture was taught by Professor [Luigi Palla](https://scholar.google.co.uk/citations?hl=en&user=p-cHaf0AAAAJ&view_op=list_works&sortby=pubdate).
```


```{r Hier-Session11, child = ('10-Hierarchical/Session11.Rmd')}
```



## Cluster analysis/PCA practical 

本次練習完成時，你將學會：

1. 如何使用聚類分析，和主成分分析法來探索一組多變量數據之間的關係；
2. 理解並懂得如何選取合適的距離測量尺度，和聚類分析方法；
3. 繪製並能夠解釋由多層聚類分析算法 (hierarchical clustering algorithm) 獲得的樹狀圖；
4. 使用主成分分析法對數據進行座標轉換，計算多個變量之間的方差，協方差矩陣，懂得如何判斷保留主成分的個數；
5. 通過把數據繪製在較低維度的主成分座標軸上來判斷數據中可能存在的潛在分層/分組。


### 使用的數據和簡單背景知識

假設你是一名生物測量技術公司的統計師，現在有這樣一組數據，包含了對某植物測量的4種生物標幟物(biomarkers)。據報道，這四種成分或許能減少你公司生產的某藥物引起的副作用。爲了嘗試分析該植物的生物特性，從該植物的50個不同樣本中，測量了這4種生物標幟物的濃度。你的任務之一是對數據進行初步分析，彙報任何你找到的可能存在的顯著特徵差異。

1. 在R裏讀入你的數據，看看這4種生物標幟物的簡單統計量和分佈，它們用的是相同的測量單位嗎？


```{r pca-1, cache=TRUE}
plant <- read_dta("backupfiles/plant.dta")
plant <- plant[, 1:4]
head(plant)
epiDisplay::summ(plant)
psych::describe(plant)
```

觀察這四個生物標幟物的簡單統計量，似乎可以認爲它們使用的應該是相似或者相同的測量單位。它們的均值在53至61之間，標準差分佈在45-51之間，而且最大值最小值之間的範圍也十分接近。

2. 這些生物標幟物能否單獨提供關於該植物的某部分特徵信息呢？思考我們該如何回答這個問題（提示：計算這些指標直接的相關係數）

我們可以通過計算這四個生物標幟物濃度測量值之間的相關係數，來觀察它們之間是否具有相似性或者是否提供了部分相似的信息。

```{r pca-2, cache=TRUE}
cor(plant)
```

從相關係數矩陣的計算結果來看，平均地，這四個生物標幟物濃度之間具有一定程度的相關性。其中，生物標幟物1和3之間呈現了四者之間最高的樣本相關係數 $(r_{13} = 0.5941)$，生物標幟物1和4之間的相關係數則最小 $(r_{14} = 0.2677)$。

3. 請描述前一步中我們計算的相關係數矩陣的維度(dimension)。

該相關係數矩陣的維度是 $4\times4$，事實上，這個矩陣的維度是由我們想要觀察分析的樣本中測量變量的個數決定的（在這裏就是四個生物標幟物）。但是這個相關係數的矩陣並不適合用於做聚類分析 (cluster analysis)，因爲相關係數本身反映的是變量之間的關係 (between variables)，並非觀察對象 (between subjects) 之間的距離(即，不是我們關心的用來把50個樣本進行分組歸類的距離變量)。

4. 再次思考問題1.的答案，思考並選擇合適的測量不同樣本個體之間距離 (distance) 的度量衡。嘗試使用簡單的聚類分析命令對樣本進行分類。

由於每個生物標記物在所有樣本中的數值基本在相似的比例或者刻度，每個標幟物在這個樣本中的標準差/方差數值也較爲相近。我們嘗試用連續變量最常見的均值測量距離指標:

```{r pca-3, cache=TRUE, fig.asp=.7, fig.width=7, fig.cap='Dendrogram for L2_avlink cluster analysis', fig.align='center', out.width='80%'}
# prepare hierarchical cluster
hc <-  hclust(dist(plant), "ave")


plot(hc, cex = 0.8, hang = -1, 
     main = "", ylab = "L2 dissimilarity measure", 
     xlab = "No. of specimen")
```

可以觀察到，樣本編號 31, 27, 17, 48, 8, 30, 3, 14, 6, 42 很快就聚合成爲一組。且這些樣本和其他樣本被聚類在不同組的過程一直維持到差異性達到100以上。我們還可以注意到，聚類過程中其他的分支呈現相對對稱的形狀。


5. 從簡單的歐幾里得距離改成歐幾里得距離平方來測量樣本之間的距離的話，圖形會變成什麼樣？

```{r pca-4, cache=TRUE, fig.asp=.7, fig.width=7, fig.cap='Dendrogram for L2sq_avlink cluster analysis', fig.align='center', out.width='80%'}
hc <- hclust(dist(plant)^2)

plot(hc, cex = 0.8, hang = -1, 
     main = "", ylab = "L2squared dissimilarity measure", 
     xlab = "No. of specimen", sub = "")
```

當使用歐幾里得距離的平方作爲樣本間隔的度量衡時，我們發現聚類的過程其實總體來說和使用歐幾里得距離本身並無本質上的區別。只是在差異性較低的地方聚類加速 (squeeze the dissimilarities at the lower end)，並且在較大的聚類區分之間變得更加明顯，視覺效果上更容易區分。

如果說，不用歐幾里得平方，而是使用簡單的曼哈頓距離 (L1 度量衡)，那麼圖形則又會呈現爲:

```{r pca-5, cache=TRUE, fig.asp=.7, fig.width=7, fig.cap='Dendrogram for L1_avlink cluster analysis', fig.align='center', out.width='80%'}

plot(cluster::agnes(plant, metric = "manhattan", stand = F), which.plots = 2, hang = -1, 
     xlab = "No. of specimen", main = "", ylab = "L1 dissimilarity measure", sub = "", cex = 0.8)
```

可以看出，當使用曼哈頓距離做度量衡時，聚類的過程和之前的沒有本質上的區別，但是圖形的樹狀分支上似乎不再左右對稱。

6. 接下來使用歐幾里得距離做度量衡，與上面的嘗試不同，這裏我們嘗試用完全連接，和單連接


# 缺失數據 Missing data 1 


```{r Hier-Session10, child = ('10-Hierarchical/Session10.Rmd')}
```

## Practical 10-Hier

```{r Hier-Practical10, child = ('10-Hierarchical/Practical10.Rmd')}
```


# 缺失數據 Missing data 2

# Further issues
