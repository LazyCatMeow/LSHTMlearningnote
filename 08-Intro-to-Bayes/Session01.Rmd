本章節之目的：

- 介紹 (啓發) 貝葉斯推斷 Bayesian inference 的基本概念。並且與概率論 frequentist inference 推斷實例作比較。
- 介紹共軛分佈的概念 conjugate distributions。用單一參數家族 (single parameter family) ，特別是二項分佈的圖形來描述共軛分佈；用方差已知的正態分佈均值來描述共軛分佈。
- 介紹貝葉斯預測分佈 Bayesian prediction distribution。

推薦書目：

1. ["Principles of Statistical Inference"](https://books.google.co.jp/books?id=nRgtGZXi2KkC&dq=principles+of+statistical+inference&lr=&source=gbs_navlinks_s) by D.R. Cox [@cox2006principles]
2. ["Bayesian Data Analysis"](https://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954) by Gelman, Carlin, Stern, Dunson, Vehtari, and Rubin [@gelman2013bayesian], [website for the book](http://www.stat.columbia.edu/~gelman/book/)
3. ["Bayesian Biostatistics"](https://books.google.co.uk/books?id=WV7KVjEQnJMC&printsec=frontcover&dq=Bayesian+biostatistics&hl=zh-CN&sa=X&ved=0ahUKEwjct_3vqcbXAhXFJ8AKHar3BbQQ6AEIJjAA#v=onepage&q=Bayesian biostatistics&f=false) by Vehtari and Rubin [@lesaffre2012bayesian]


貝葉斯統計推斷，提供了不同於概率論推斷的另一種考察和解決問題的思路。所有的思考，都源於貝葉斯定理 Bayes' Theorem (Section \@ref(Bayes-Definition))。起源於英國統計學家[托馬斯貝葉斯 (Thomas Bayes)](https://en.wikipedia.org/wiki/Thomas_Bayes) 死後被好友 [Richard Price](https://en.wikipedia.org/wiki/Richard_Price) 整理發表的論文: ["An essay towards solving a problem in the doctrine of chances."](www.stat.ucla.edu/history/essay.pdf)

概率論推斷與貝葉斯推斷的中心都圍繞似然 likelihood (Section \@ref(likelihood-definition)) 的概念。然而二者對似然提供的信息之理解和解釋完全不同。即在對於觀察數據提供的信息的理解，和如何應用已有信息來影響未來決策（或提供預測）的問題上常常被認爲是統計學中形成鮮明對比的兩種哲學理念。過去幾個世紀二者之間孰優孰劣的爭論相當激烈。但是，從實際應用的角度來看，我們目前更關心哪種思維能更加實用地描述和模擬真實世界。幸運地是，多數情況下，二者的差距不大。所以無法簡單地從一個實驗或者一次爭論中得出誰更出色的結論。現在的統計學家們通常不再如同信仰之爭那樣的互相水火不容，而是從實用性角度來判斷一些實際情況下，採用哪種思想能使計算過程更加簡便或者計算結果更加接近真實情況。

請思考如下的問題：
什麼是概率？ What is probability?

1. 概率論思想下的定義：某事件在**多次重複觀察**實驗結果中發生次數所佔的比例。<br> The probability of an event is the limit of its relative frequency in a large number of trials."
2. 貝葉斯思想下的定義：概率是你相信某事件會發生的可能性。 <br> Probability is a measure of the degree of belief
about an event.


## 概率論推斷的複習

思考不同場景：

- 場景 A：假如我們在監測一個製造鐵絲的工廠，需要測量該工廠生產的鐵絲的強度。
- 場景 B：假如我們正在進行一個大型隊列研究，該研究是關於心臟病和與之相關的某個危險因子的評價。數據來源是家庭醫生的診療數據庫。
- 場景 C：假如一名警察凌晨三點在空無一人的街頭巡邏時，突然聽見防盜自動警鈴的報警聲。他立刻循聲望去，對面街上的珠寶店玻璃碎了一地。一個戴着巴拉克拉瓦頭套的人正揹着一個大包從破碎玻璃窗中爬出。該警察毫不猶豫地判定該人就是劫匪，立刻將其逮捕。

在這些場景下，請用概率論思想思考如下幾個問題：

1. 事件是什麼？
2. 如何解讀總體參數？
3. 如何使用參數進行概率推斷？
4. 用經典概率論時，有什麼缺點嗎？

- 場景 A：
    1. 事件：該工廠製造的鐵絲，長期以來的強度大小是多少。
    2. 總體參數：鐵絲的真實強度，或者與鐵絲強度相關的特性。
    3. 概率推斷：我們進行鐵絲的強度實驗，即從該工廠已經生產的鐵絲中大量抽取樣本逐一進行強度檢測。用相應的概率模型來模擬抽取的樣本數據，並且使用極大似然估計找到最能體現抽樣數據的參數估計，然後對獲得的極大似然估計進行95%信賴區間的計算。然後如果我們重複這樣相同的實驗無數次，那麼我們計算的所有的信賴區間中，有95%包含了真實的鐵絲強度大小。
    4. 在鐵絲強度測量的場景中，經典概率論顯得十分自然，因爲我們真的可以重複這樣的實驗很多很多次以獲得想要的參數的精確估計。

- 場景 B：
    1. 事件：由於我們用的是整個隊列研究的數據。所以從概率論的角度來看，本事件就是假定我們可以在人數無限多的人羣中重複同樣的隊列研究。
    2. 總體參數：我們感興趣的心臟病相關危險因子，在抽取該隊列作爲樣本的人羣中的真實值大小。
    3. 概率推斷：我們用泊松分佈的概率模型來模擬人羣中從開始觀察時起，至心臟病發病這段時間內和該危險因子之間的關係大小。然後用傳統的極大似然估計法計算獲得 HR, OR 等值來表示危險因素和心臟病的關係。
    4. 缺點：實際情況是，經費時間和人力資源的限制下，我們無法“重複相同的隊列研究”。而且該對列本身可能就是十分獨特的，比如只有男性，或者有年齡限制，或者其他的特性使隊列本身在理論上就是不可能被重複的。所以，在這樣的場景下，用經典的概率論思想作統計推斷常常會被認爲是不自然不妥當的。

- 場景 C：
    1. 事件：警察無數次在同樣的時間同樣的地點巡邏時，聽見防盜自動警鈴的報警聲，他看見頭戴巴拉克拉瓦頭套的人從破碎的玻璃窗中爬出......
    2. 總體參數：在無數次上面描述的場景時，發生盜竊案的真實概率。
    3. 概率推斷：使用某種可以描述該事件（巡邏時。。。發生盜竊案的概率）的數學模型，我們用極大似然估計來計算發生盜竊案概率的估計和95%信賴區間，**然後警察同志再來決定是否要去抓眼前這個頭戴巴拉克拉瓦頭套的人**。
    4. 缺點：經典概率論在如此場景下很明顯是完全不適用的。<br>1) 這裏經典概率論思維下的概率實際上無法準確定義，充其量是一種發生盜竊案可能性的估計。<br>2) 在如此場景下，警察會根據已經觀察到的現象（已知信息），來判斷一場盜竊案發生的概率是多少。

通過上面不同場景的下的思考，應該能看到傳統概率論中始終假設我們可以**重複相同的實驗**多次，然後從長遠來估測相關事件發生的概率。許多場景下，即使事件概率能被準確定義，我們是很難知道我們關心的參數的分佈的，從而導致我們常常要用到漸進法估算 (asymptotic approximation)。

## 貝葉斯概率推理/逆概率 Bayesian reasoning/inverse probability

首先，不得不承認的一個事實是，**所有的概率都是條件概率**。

- 要麼是根據已知的信息。
- 要麼是一般性大家都接受的某種假設條件。

其次，概率，並不是“長遠”地重複觀察獲得的事件發生頻率。相反地，概率的大小取決與**你自己**和**你感興趣的話題（事件）**。思考下列例子：

1. 明天會下雨嗎？
2. 阿森納下一場比賽會贏還是會輸？
3. 你的期末考試能不能過？

### 演繹推理 deductive reasoning 和 三段論 weak syllogisms


數學要用到邏輯，假設我們用 $A,B,C$ 標記不同的事件。

- 如果 $A\Rightarrow B$ (事件 A 可以推導出事件 B)
- 那麼當我們知道“事件 B 爲真”時，雖然B不一定能倒推回 A，但是我們會相信**事件 A 很可能發生了**。

例如，A 表示“正在下雨”這件事，B 表示 “天上有烏雲”。那麼從邏輯學上來說，$A\Rightarrow B$ 。然而有烏雲本身不一定會下雨。但是會讓我們覺得下雨的可能性增加了。

再來思考警察巡邏的例子。A 表示 “在珠寶店正在發生盜竊案”，B 表示 “一個頭戴巴拉克拉瓦頭套的人正在從玻璃窗中爬出”。也是一樣的道理。

所以警察薯熟在做判斷的時候，需要判斷 Pr(A|B)。他需要如下的信息：

1. 珠寶店發生盜竊案的前提下，有個人從碎玻璃窗中爬出來的概率。
2. 該警察薯熟正處於的環境（半夜三點無人的街頭，等場景）

所以，看到這裏是不是覺得貝葉斯使用的是我們的“常識”在思考決斷問題？因爲我們的先驗概率 (prior) 至關重要。這是我們的背景知識和解釋參數似然（推斷）的依據。

### 如何給可能性定量 Quantifying plausibility

進行可能性定量之前，R.T. Cox 制定了如下的規則[@Cox1946]：

1. $\text{plausibility}(A)$ 是一個有邊界的實數；
2. 傳遞性，transitivity：如果
    - $\text{plaus}(C)>\text{plaus}(B)$ and
    - $\text{plaus}(B)>\text{plaus}(A)$ then
    - $\text{plaus}(C)>\text{plaus}(A)$
3. 一致性，consistency：事件 $A$ 發生的可能性只取決於所有與 $A$ 直接相關的信息，而不包括那些推理到與 $A$ 相關信息之前的信息。<br> The plausibility of proposition $A$ depends only on the relevant information on $A$ and not on the path of reasoning followed to arrive at $A$.

R.T. Cox 證明了他提出的這些規則可以完全適用於所有的可能性計算，而且可能性 (plausibility) 的這些規則和概率 (probability) 的微積分計算完全一致。

所以利用上面的可能性規則，我們可以對條件概率進行更深層次的定義：

$$\text{Pr}(A|B)=\frac{\text{Pr}(B|A)\text{Pr}(A)}{\text{Pr}(B)}\propto \text{Pr}(B|A)\text{Pr}(A)$$

用文字表述爲：

<center>
事後概率 $\propto$ 似然 $\times$ 先驗概率
</center>


其中：

- **事後概率，posterior probability**：$B$ 發生的條件下, $A$ 發生的概率；
- $\propto$ ：與...成正比；
- **似然，likelihood**：$A$ 發生的條件下，$B$ 發生的概率；
- **先驗概率，prior probability**：事件 $A$ 發生的概率。

這就是**貝葉斯定理**。這個定理也告訴我們爲什麼貝葉斯論證在18，19世紀時被叫做“逆概率推理, inverse probability reasoning”。因爲似然 ($A$ 發生的條件下，$B$ 發生的概率) 在與先驗概率相乘以後，概率發生了逆轉--事後概率 ($B$ 發生的條件下, $A$ 發生的概率)。

回頭再來看之前的珠寶店盜竊案：

- 事件 $A$：珠寶店正在發生盜竊案；
- 事件 $B$：一個頭戴巴拉克拉瓦頭套的人正在從玻璃窗中爬出。

所以：

- $\text{Pr}(A)=$ 珠寶店發生盜竊案的概率 -- 先驗概率 (prior probability);
- $\text{Pr}(B|A)=$ 當珠寶店發生盜竊案時，觀察到“一個頭戴巴拉克拉瓦頭套的人正在從玻璃窗中爬出”事件的可能性 -- 似然 (likelihood);
- $\text{Pr}(A|B)$ 當觀察到“一個頭戴巴拉克拉瓦頭套的人正在從玻璃窗中爬出”事件時，倒推珠寶店發生了盜竊案的概率 -- 事後概率 (posterior probability)。

用例子來解釋貝葉斯推理之後你會發現，其實貝葉斯思想也是純粹的概率理論。與經典概率論不同的是，**我們沒有必要認爲某些事件發生的概率需要被重複實驗驗證**。貝葉斯對整個世界的理解源於我們每個人自己認爲的事件發生概率 (personalisitic probability)，或者叫信念度（degree of belief）。


## 貝葉斯推理的統計學實現

在經典概率論中，概率分佈的標記 $f_X(x;\theta)$ 的涵義爲：
對於一個隨機變量 $X$，它在我們假設的某種固定的真實（上帝才知道是多少的）參數 $\theta$ 的分佈框架下，不斷重複相同的實驗之後獲得的概率分佈。

在貝葉斯統計推理中，一切都被看作是一個服從概率分佈的隨機變量。利用貝葉斯定理，我們將先驗隨機概率分佈 (prior probability distribution)，和觀察數據作條件概率 (condition on the observed data)，從而獲得事後概率分佈 (posterior probability distribution)。

### 醫學診斷測試 diagnostic testing

貝葉斯推理最常用的實例是在診斷測試中，即當一個人拿着陽性的檢驗報告結果來找你，你如何判斷這個人有多大的概率真的患有該疾病。

用 $D$ 標記患病， $\bar{D}$ 標記不患病；$T$ 標記檢查結果爲陽性，$\bar{T}$ 標記檢查結果爲陰性。那麼，陽性檢查結果時，真的患病的概率 $\text{Pr}(D|T)$：

$$
\begin{aligned}
\text{Pr}(D|T) &= \frac{\text{Pr}(T|D)\text{Pr}(D)}{\text{Pr}(T)}\\
&=\frac{\text{Pr}(T|D)\text{Pr}(D)}{\text{Pr}(T|D)\text{Pr}(D)+\text{Pr}(T|\bar{D})\text{Pr}(\bar{D})}
\end{aligned}
$$

其中分母的轉換用到了 Law of Total Probability (L.T.P):

$$
\begin{aligned}
\text{Pr}(T) &= \text{Pr}(T \cap D) + \text{Pr}(T \cap \bar{D}) \\
&= \text{Pr}(T|D)\text{Pr}(D)+\text{Pr}(T|\bar{D})\text{Pr}(\bar{D})
\end{aligned}
$$

所以說，貝葉斯定理在這裏告訴我們，要計算 $\text{Pr}(D|T)$ 我們只需要下列幾個信息：

1. 患病率： $\text{Pr}(D)$
2. 檢測手段的敏感度 (sensitivity)： $\text{Pr}(T|D)$
3. 檢測手段的 1 - 特異度 (specificity)： $\text{Pr}(T|\bar{D})=1-\text{Pr}(\bar{T}|\bar{D})$


### HIV 檢查時的應用

假設人羣中患病率爲 $1/1000$，所用的 HIV 檢測手段的敏感度爲 $0.99$， 特異度爲 $0.98$。試計算該檢測HIV手段的事後概率（即拿到陽性結果時，患病的概率 $\text{Pr}(D|T)$）。

**解**

令 $D=\text{HIV positive}, \bar{D}=\text{HIV negative}\\
T=\text{test postive}, \bar{T}=\text{test negative}$

$$
\begin{aligned}
\text{Pr}(D|T) &= \frac{\text{Pr}(T|D)\text{Pr}(D)}{\text{Pr}(T|D)\text{Pr}(D)+\text{Pr}(T|\bar{D})\text{Pr}(\bar{D})} \\
&= \frac{0.99\times0.001}{0.99\times0.001+(1-0.98)\times0.999} \\
&= 0.0472
\end{aligned}
$$

如果 特異度能達到 $0.99$

$$
\begin{aligned}
\text{Pr}(D|T) &= \frac{\text{Pr}(T|D)\text{Pr}(D)}{\text{Pr}(T|D)\text{Pr}(D)+\text{Pr}(T|\bar{D})\text{Pr}(\bar{D})} \\
&= \frac{0.99\times0.001}{0.99\times0.001+(1-0.99)\times0.999} \\
&= 0.0901
\end{aligned}
$$

如果特異度能達到 $0.999$

$$
\begin{aligned}
\text{Pr}(D|T) &= \frac{\text{Pr}(T|D)\text{Pr}(D)}{\text{Pr}(T|D)\text{Pr}(D)+\text{Pr}(T|\bar{D})\text{Pr}(\bar{D})} \\
&= \frac{0.99\times0.001}{0.99\times0.001+(1-0.999)\times0.999} \\
&= 0.497
\end{aligned}
$$

可見，對於像 HIV 這樣人羣中患病率較爲罕見的疾病，其檢驗手段的敏感度，特異度都要達到極高才能讓檢驗結果可靠，即拿到陽性結果的人的確患有該疾病。其中當敏感度爲 $0.99$，特異度爲 $0.999$ 時，才能讓這樣的檢驗手段達到接近一半的可靠程度 (即只有接近一半的陽性結果是真陽性)。

注意本例爲貝葉斯理論的特例，即我們使用的是一個固定的先驗概率 (prior) 和似然 (likelihood)。一般情況下，先驗概率和似然會有自己的概率分佈 (probability distribution)，而很少會是一個固定的值， 其相應的事後概率 (posterior) 也擁有概率分佈，並且使用它本身的均值和方差來描述。


### 說點小歷史



```{r Bayes00, cache=TRUE, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap='Sir Ronald Fisher', fig.align='center', out.width='50%'}
knitr::include_graphics(paste0(bugpath, "/img/Fisher.jpg"))
```

[Ronald Aylmer Fisher (1890-1962)](https://en.wikipedia.org/wiki/Ronald_Fisher) 推動了統計學在20世紀前半頁的重大發展。他鞏固了概率論統計學堅實的基礎，並且積極提倡這一套理論[@Fisher1922]。但是 Fisher 本人對於統計學的“統計學意義, level of significance” 的認識卻是隨着時間和他年齡的變化而變化的：

```{r Bayes01, cache=TRUE, echo=FALSE, warning=FALSE, eval=FALSE}
dt <- read.csv("backupfiles/Fisher.csv", header = T)
kable(dt, "html",align = "c",caption = "Fisher's interpretation of 'level of significance' and the Neyman-Pearson interpretation") %>%
  kable_styling(bootstrap_options = c("striped", "bordered"))
```

<table class="table table-striped table-bordered" style="margin-left: auto; margin-right: auto;">
<caption>表 40.1: Fisher's interpretation of 'level of significance' and the Neyman-Pearson interpretation</caption>
 <thead><tr>
<th style="text-align:center;"> 早期 Fisher (1935) </th>
   <th style="text-align:center;"> 晚期 Fisher (1956) </th>
   <th style="text-align:center;"> Neyman and Pearson </th>
  </tr></thead>
<tbody><tr>
<td style="text-align:left;"> 統計學有意義的水平（傳統上使用 $\alpha=5\%$），必須在實施統計檢驗之前就被決定。因此，統計學意義的水平是相應統計學檢驗本身的性質之一。<br>
Thus, the level of significance is a property of the _test_. </td>
   <td style="text-align:left;"> 統計學意義的水平，應該被精確計算並且在報告中明確 $p$ 值的大小，故統計學意義的水平本身是在實施了統計檢驗之後計算的。它應該是屬於觀察數據的固有性質。 <br>
Here the level of significance is a property of the _data_. </td>
   <td style="text-align:left;"> $\alpha$ 和 $\beta$ 作爲統計檢驗的第一類錯誤和第二類錯誤指標，應該在實施統計檢驗之前被決定。所以 $\alpha, \beta$ 是屬於統計檢驗的性質。<br>
Yet, to determine $\alpha, \beta$ no convention is required, but rather a cost-benefit estimation of the severity of the two kinds of error. </td>
  </tr></tbody>
</table>


隨着马尔科夫蒙特卡洛 (Markov-Chain Monte Carlo, MCMC) 法的廣泛應用，貝葉斯統計學在事後概率計算上（計算量超大的）棘手問題，得到了解決。
