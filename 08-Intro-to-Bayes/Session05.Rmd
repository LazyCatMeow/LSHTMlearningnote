## 把不需要的噪音參數平均出去 Averaging over 'nuisance parameters'

從數學上表達聯合事後分佈 (joint posterior distribution) 和邊際事後分佈 (marginal posterior distribution) 其實不困難，可以想像我們想要了解的參數 $\theta$ 其實是有兩個部分的，用 $\theta = (\theta_1, \theta_2)$ 表示。更進一步想像，我們其實真正只對這兩部分參數中的其中一個 $\theta_1$ 感興趣。那麼另一個就是所謂的“噪音參數 nuisance parameter”。例如我們在使用正（常）態分佈數據時，有兩個未知的參數，均值 $\mu$，和方差 $\sigma^2$。但是實際上有時候我們可能只對其中一個感興趣，更多時候僅僅是均值，也有的時候是方差。

$$
y | \mu, \sigma^2 \sim N(\mu, \sigma^2)
$$

那麼我們就需要尋找，其中一部分的參數在收集到的數據的條件下的分佈，$p(\theta_1 | y)$。

這裡需要仔細解釋一下如何求上述的數據條件參數分佈。當給定了數據 $y$，通過微分思想，可以認為，未知參數 $\theta$ 的期望值（或均值），可以通過在 $y$ 的每一個小段的區間上的值的均值來獲得：

$$
E_{p(\theta | y)}[\theta] \approx \frac{1}{S}\sum_{S = 1}^S \theta^{(S)}
$$

當這個區間 $S  \rightarrow 0$，也就是趨近於零時，上面的式子就轉化成了一個積分方程式：


$$
E_{p(\theta | y)}[\theta] \approx \frac{1}{S}\sum_{S = 1}^S \theta^{(S)} = \int\theta p(\theta | y)
$$

那麼當參數不只一個的時候，它們的事後聯合分佈 (Joint posterior distribution)，可以認為是： 

$$
p(\theta_1, \theta_2 | y) \propto p(y | \theta_1, \theta_2) p(\theta_1, \theta_2)
$$


之後的任務是要把 $p(\theta_1, \theta_2 | y)$ 中的噪音參數 $\theta_2$ 通過積分的方法去除掉。類似地，使用微分思想，我們把 $p(\theta_2|y)$ 這個分佈分割成無數小的區間來計算每個區間裡的 $\theta_1$，再求它的均值：

$$
p(\theta_1 |y) \approx \frac{1}{S} \sum_{S = 1}^S p(\theta_1, \theta_2^{(S)} | y)
$$
其中，$\theta_2$ 可以使用蒙特卡洛 (Monte Carlo) 過程從 $p(\theta_2 | y)$ 中隨機採集。當這個無數小的區間的面積無限趨近於零時，$S \rightarrow 0$，上面的方程式就變成了一個關於 $\theta_2$ 的積分方程式：

$$
p(\theta_1 |y) \approx \frac{1}{S} \sum_{S = 1}^S p(\theta_1, \theta_2^{(S)} | y) = \int p(\theta_1, \theta_2 | y) d\theta_2
$$

這個過程被叫做**邊際化 (marginalization)**。進一步地，上面的積分方程中間的 $p(\theta_1, \theta_2 | y)$ 又可以被理解成由兩個部分組成：一個是 $p(\theta_1 | \theta_2, y)$，即增加了噪音參數 $\theta_2$ 的條件事後分佈 (conditional posterior distribution given the nuisance parameter)；另一個是 $p(\theta_2 | y)$，也就是給定了數據 $y$ 之後不同的 $\theta_2$ 的取值的權重 (weighting function for the different possible values of $\theta_2$)：


$$
p(\theta_1 | y) = \int p(\theta_1 |\theta_2 , y) p(\theta_2 | y) d\theta_2
$$


## 未知均值也未知方差的正（常）態分佈數據 normal data with unknown mean and variance

作為一個經典的例子，我們思考從數據中估計一個未知的平均值。假設該數據是一維的 $n$ 個獨立樣本 $y$ 服從正（常）態分佈 $N(\mu, \sigma^2)$。

### 無信息先驗概率分佈 noninformative prior distribution

根據 [Jeffreys prior](https://en.wikipedia.org/wiki/Jeffreys_prior) 對無信息先驗概率分佈的定義，我們給這個未知均值也未知方差的數據的兩個參數的無信息先驗概率分佈分別是：均值 $\mu$ 使用均一分佈 (uniform distribution)，方差 $\sigma^2$ 使用 $\sigma^{-2}$：

$$
p(\mu, \sigma^2) \propto \sigma^{-2}
$$

於是，該數據的聯合事後概率分佈可以被推導為：

$$
\begin{aligned}
p(\mu, \sigma^2 | y) & \propto \sigma^{-2} \prod_{i =1}^n \frac{1}{\sqrt{2\pi} \sigma} \exp\left( -\frac{1}{2\sigma^2}(y_i - \mu)^2 \right) \\
& = \sigma^{-n - 2} \exp\left(-\frac{1}{2\sigma^2}\sum_{i =1}^n (y_i - \mu)^2 \right) \\
& = \sigma^{-n - 2} \exp\left(-\frac{1}{2\sigma^2}\sum_{i =1}^n (y_i^2 - 2y_i\mu + \mu^2) \right) \\
& = \sigma^{-n - 2} \exp\left(-\frac{1}{2\sigma^2}\sum_{i =1}^n (y_i^2 - 2y_i\mu + \mu^2 -\bar{y}^2 + \bar{y}^2 - 2y_i\bar{y} + 2y_i\bar{y} ) \right) \\
& = \sigma^{-n - 2} \exp\left(-\frac{1}{2\sigma^2}\left[\sum_{i =1}^n (y_i^2 - 2y_i\bar{y} + \bar{y}^2) + \sum_{i = 1}^n(\mu^2 - 2y_i\mu - \bar{y}^2 + 2y_i\bar{y})  \right]\right) \\
& = \sigma^{-n - 2} \exp\left(-\frac{1}{2\sigma^2}\left[\sum_{i =1}^n (y_i^2 - 2y_i\bar{y} + \bar{y}^2) + n(\mu^2 - 2\bar{y}\mu - \bar{y}^2 + 2\bar{y}^2)  \right]\right) \\
& = \sigma^{-n - 2} \exp\left(-\frac{1}{2\sigma^2}\left[\sum_{i =1}^n (y - \bar{y})^2 + n(\bar{y} - \mu)^2  \right]\right) \\
\color{darkred}{\text{Where } \bar{y}} & \color{darkred}{= \frac{1}{n}\sum_{i = 1}^n y_i} \\
& = \sigma^{-n - 2} \exp\left(-\frac{1}{2\sigma^2}\left[(n - 1)s^2 + n(\bar{y} - \mu)^2  \right]\right) \\
\color{darkred}{\text{Where } s^2} & \color{darkred}{= \frac{1}{n - 1}\sum_{i = 1}^n (y_i - \bar{y})^2} \\
\end{aligned}
$$

此時，其實我們有兩種邊際分佈選擇，一種是把方差作為噪音參數，另一種是把均值作為噪音參數。先是 $\mu$ 的邊際分佈，它是通過把 $\sigma$ 積分出去獲得的：

$$
p(\mu | y) = \int p(\mu, \sigma^2 | y) d\sigma^2
$$

其次是 $\sigma^2$ 的邊際分佈，它是通過把 $\mu$ 積分出去獲得的：

$$
p(\sigma^2 | \mu) = \int p(\mu, \sigma^2 | y) d \mu
$$

下面我們來推導把均值積分出去的過程：

$$
\begin{aligned}
p(\sigma^2 | y) & \propto \int p(\mu, \sigma^2 | y) d \mu \\ 
& \propto \int \sigma^{-n - 2} \exp\left(-\frac{1}{2\sigma^2}\left[(n - 1)s^2 + n(\bar{y} - \mu)^2  \right]\right)  d\mu \\ 
\text{Move terms } & \text{does not include } \mu \text{ outside of the intergral } \\
& \propto  \sigma^{-n - 2} \exp\left( -\frac{1}{2\sigma^2}(n - 1)s^2 \right) \int\exp\left( -\frac{n}{2\sigma^2}(\bar{y} - \mu)^2 \right) d\mu \\
\because  \int \frac{1}{\sqrt{2\pi\sigma^2}}  & \exp\left(- \frac{1}{2\sigma^2}(y - \theta)  \right)d\theta = 1 \text{ integration of the pdf of a normal distribution} \\
& \propto  \sigma^{-n - 2} \exp\left( -\frac{1}{2\sigma^2}(n - 1)s^2 \right) \times \sqrt{\frac{2\pi\sigma^2}{n}} \\
& \propto (\sigma^2)^{-(\frac{n -1}{2} + 1)} \exp\left( -\frac{(n -1 )s^2}{2\sigma^2} \right)
\end{aligned}
$$
你會發現這個邊際分佈的概率函數竟然成了一個負卡方分佈：

$$
p(\sigma^2 | y) = \text{Inv-}\chi^2 (\sigma^2 | n - 1, s^2)
$$

 
 對比正常態分佈數據已知均值，求它方差的邊際分佈的時候我們用到的負卡方分佈的特徵值如下 （參考 Chapter \@ref(unknownvarBayes)）：
 
 $$
 \begin{aligned}
 \sigma^2 | y & \sim \text{Inv-}\chi^2(\sigma^2| n, v) \\
 \text{Where } v & = \frac{1}{n}\sum_{i =1}^n(y_i - \theta)^2
 \end{aligned}
 $$
 
 
 而此時我們推導出的未知均值，需要用樣本均值來估計時的方差邊際分佈為：
 
 $$
 \begin{aligned}
 \sigma^2 | y & \sim \text{Int-}\chi^2(n - 1, s^2) \\
 \text{Where } s^2 & = \frac{1}{n -1}\sum_{i = 1}^n(y_i - \bar{y})^2
 \end{aligned}
 $$

也就是說，在均值未知時，由於此時需要對均值作額外的估計，它本身的不確定性使得方差在估計的時候也增加了不確定性（雙側尾部變厚）。

所以，一個未知均值也未知方差的正常態分佈數據，我們採集兩個未知參數的事後概率分佈的過程其實可以描述成為下面的過程：


$$
\begin{aligned}
\text{The posterior} & \text{ joint distribution is:} \\
p(\mu, \sigma^2 | y) & = \color{darkred}{p(\mu | \sigma^2, y)} \color{darkgreen}{p(\sigma^2 | y)} \\
\color{darkgreen}{p(\sigma^2 | y)} & = \text{Inv-}\chi^2 (\sigma^2 | n-1, s^2) \\
(\sigma^2)^{(s)} & \sim \color{darkgreen}{p(\sigma^2 | y)} \\
\color{darkred}{p(\mu | \sigma^2, y)} & = N(\mu | \bar{y}, \sigma^2 /n) \color{grey}{\propto \exp\left( -\frac{n}{2\sigma^2}(\bar{y}  - \mu)^2 \right)} \\
\mu^{(s)} & \sim \color{darkred}{p(\mu | \sigma^2, y)} \\
\mu^{(s)}, (\sigma^2)^{(s)} & \sim p(\mu, \sigma^2 | y)
\end{aligned}
$$


### 均值的事後邊際概率分佈 marginal posterior distribution of $\mu$


通常我們在均值和方長二者之間會更關心總體的均值 (population mean) $\mu$。那麼通過把方差從二者的事後聯合分佈中積分出去的方法，可以獲得：

$$
\begin{aligned}
p(\mu | y) & = \int_0^\infty p(\mu, \sigma^2 |y) d\sigma^2 \\
 & \propto \int_0^\infty \sigma^{-n - 2} \exp\left(-\frac{1}{2\sigma^2}\left[(n - 1)s^2 + n(\bar{y} - \mu)^2  \right]\right)  d\sigma^2
\end{aligned}
$$

對上述函數進行簡化的過程中，需要用到下面的轉換輔助理解其過程：


$$
A = (n - 1)s^2 + n(\bar{y} - \mu)^2 \text{ and } z = \frac{A}{2\sigma^2}
$$

那麼上面的公式可以被修改成關於 $z$ 的積分函數：

$$
\begin{aligned}
p(\mu | y) & \propto A^{-n/2} \int_0^\infty z^{(n - 2)/2} \exp(-z)dz \\
\color{gray}{\text{Recognize }}& \color{gray}{\text{Gamma intergral }\Gamma(u) = \int_0^\infty x^{u-1}\exp(-x)dx} \\
\color{gray}{\text{Although t}}& \color{gray}{\text{here are different power terms, }} \\
\color{gray}{\text{but we kno}}& \color{gray}{\text{w this intergral will give us a constant value.}} \\
\color{gray}{\text{therefore o}}& \color{gray}{\text{nly the } A \text{ part is left in the proporional portions}} \\
& \propto \left[ (n - 1)s^2 + n(\bar{y} - \mu)^2 \right]^{-n/2} \\
& \propto \left[ 1 + \frac{n(\bar{y} - \mu)^2}{(n - 1)s^2} \right]^{-n/2}  \\
\Rightarrow p(\mu | y) & \propto t_{n-1}(\mu | \bar{y}, \frac{s^2}{n}) \color{gray}{\text{ is a Student's } t \text{ distribution}}
\end{aligned}
$$
我們成功推導出未知方差未知均值的正常態數據的事後概率分佈是一個 $t$ 分佈。



