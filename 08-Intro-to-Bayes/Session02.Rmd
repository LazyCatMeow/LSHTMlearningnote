
從前一章節我們可以深切體會到，貝葉斯統計是如何讓我們的先驗概率，在觀察到數據之後，更新信息，獲得事後概率 (這是一個通過數據自我學習，進化的過程)。(How a prior belief about an event can be updated, given data, to a posterior belief.)

所以說，在貝葉斯模型中，我們期待使用觀察數據來學習，以增加現有的對相關參數的知識和信息。本章我們把重點放在二項分佈，用二項分佈作爲單一參數模型來瞭解怎樣推導事後分佈。

## 從二進制數據中估計概率 Estimating a probability from binomial data

在簡單的二項分佈模型中，我們討論的目的是希望從一系列的“柏努利試驗 Bernoulli trials”的結果來估計一個未知的總體百分比 (unknown population proportion)。既 $y_1, y_2, \dots, y_n$ 的結果是 $0,1$。所以二項分佈的採樣模型可以寫作是：

$$
p(y | \theta) = \text{Bin}(y | n, \theta) = {n \choose y}\theta^y(1- \theta)^{n -y}
$$




```{example 08-intro-to-Bayes-1}
**估計新生兒中女嬰的比例。**
```

作為二項分佈模型最典型的應用之一，估計新生兒中男女性別的比例，也就是男嬰或者女嬰的百分比估計是長期以來茶餘飯後的話題之一。二百年前，歐洲人群中就有好事者估計過，新生兒中女嬰的比例應該是小於50%的。現在大家更加關心的其實是，有哪些因素可能會影響新生兒中女嬰的比例。如今，大多數科學界認可的歐洲人口中新生兒女嬰的比例是 0.485。

在這個例子中，我們定義

- 這個未知的新生兒女嬰比例是 $\theta$。當然可能有人不同意，或者更樂意使用男女新生兒人數比例 $\phi = \frac{(1 - \theta)}{\theta}$，來描述這個問題，這個大同小異。
- $y$ 是新生兒中女嬰的人數。
- $n$ 是新生兒總人數。

當我們決定使用二項分佈模型來模擬上述情境的數據時，我們默認了新生兒性別比例這一條件是決定性別的唯一條件，也就是說，除了這個 $\theta$ 之外，性別是完全隨機且互相獨立的（此處需要先忽略掉一胞多胎，和同一家庭因素等可能對新生兒性別造成的影響）。那麼應用了貝葉斯定理之後，我們可以推導獲得 $\theta$ 的事後概率分佈是

$$
\begin{equation}
p(\theta | y) \propto \theta^y (1 - \theta)^{n - y}
\end{equation}
(\#eq:betaposterior)
$$

由於 $n,y$ 都是常數項，僅由他們構成的最前面的部分就是一個不會根據 $\theta$ 取值發生改變的部分，所以在計算事後概率分佈的事後，我們可以把這部分忽略掉，使用它的非。根據 [Beta 分佈的性質](https://distribution-explorer.github.io/continuous/beta.html)，不難發現，公式 \@ref(eq:betaposterior) 其實就是一個參數為 $y + 1, n - y + 1$ 的 Beta 分佈：

$$
\begin{equation}
\theta | y \sim \text{Beta}(y + 1, n - y + 1)
\end{equation}
(\#eq:betaposteriornew)
$$


### 事後概率分佈是數據和先驗概率分佈之間的妥協

貝葉斯推斷的過程是從先驗概率分佈 $p(\theta)$ 過渡到事後概率分佈 $p(\theta | y)$ 的過程。我們應該會很自然地認為，這先驗和事後之間的概率分佈必然存在這某種聯繫。例如，我們可能會很純粹的認為，因為事後概率分佈增加了手上數據的信息，那麼它相應地應該比先驗概率分佈不那麼分散（less variable than the prior）。這個思考過程可以通過下面的數學關係來呈現：

- $\theta$ 的事後概率分佈的期望 $E(\theta | y)$ 和它的先驗概率分佈的期望 $E(\theta)$ 之間的關係：

$$
\begin{equation}
E(\theta) = E(E(\theta | y))
\end{equation}
(\#eq:posteriorE)
$$
事後概率分佈的期望 公式\@ref(eq:posteriorE) 很好理解。先驗概率分佈的 $\theta$ 均值 $E(\theta)$，等於事後概率分佈的 $\theta$，在所有觀察點 $y$ 時的值的平均值。

- $\theta$ 的事後概率分佈的方差 $\text{var}(\theta | y)$ 和先驗概率分佈方差之間的關係

$$
\begin{equation}
\text{var}(\theta)  = E(\text{var}(\theta | y)) + \text{var}(E(\theta | y))
\end{equation}
(\#eq:posterorvar)
$$

方差關係的方程式 \@ref(eq:posterorvar) 其實說的是先驗概率分佈的方差，在獲取了新的數據 $y$ 之後被分成了兩部分。一個是事後概率分佈本身的方差的均值 $E(\text{var}(\theta | y))$，還有一部分是事後概率分佈均值的方差 $\text{var}(E(\theta | y))$。

**證明方差關係的公式 \@ref(eq:posterorvar)** 參考概率論部分期望與方差的關係 (Chapter \@ref(expectation))

$$
\begin{aligned}
E(\text{var}(\theta | y)) + \text{var}(E(\theta | y)) & = E(E(\theta^2 | y) - (E(\theta | y))^2) + E([E(\theta | y)]^2) - [E(E(\theta | y))]^2 \\ 
& = E(\theta^2) - E([E(\theta | y)]^2) +  E([E(\theta | y)]^2) - [E(\theta)]^2 \\
& = E(\theta^2) - [E(\theta)]^2 \\ 
& = \text{var}(\theta)
\end{aligned}
$$

理解了上述關係之後，可以說，事後概率分佈的方差總是比先驗概率分佈的方差要小一些。如果能想方設法使事後概率分佈均值的方差 $\text{var}(E(\theta | y))$ 這部分佔據更大部分，我們就能提高對 $\theta$ 的事後估計精確度。(the greater the latter variation, the more the potential for reducing our uncertainty with regard to $\theta$).

這也是貝葉斯統計推斷的基本特徵之一，事後概率分佈的中心，一般是位於先驗概率分佈的中心，和觀察數據的中心之間的某個地方，它是先驗概率分佈和數據之間相互妥協的結果。



```{example 08-intro-to-Bayes-2}
**估計有前置胎盤的母親生下的新生兒中女嬰的比例。**
```

一個早期來自德國的研究觀察到，980 名孕期有前置胎盤的母親生下的新生兒中，有 437 名是女嬰。那麼，如何來比較這個觀察數據和已知的，在總體人群 (general population) 中的女嬰出生比例 0.485之間的差異？

當先驗概率分佈使用均一分佈 (uniform distribution: $\theta \sim \text{Beta}(a = 1, b = 1)$) 時，我們獲得的數據是 $n = 980, y = 437$。那麼根據共軛的性質可以計算出事後概率分佈是服從另一個 Beta 分佈: $p(\theta | y,n) = \text{Beta}(a + y, b + n - y) = \text{Beta}(438, 544)$

```{r 08-intro-to-Bayesfig1, fig.height=4.5, fig.width=6, fig.cap='Probability of a girl given placenta previa (BDA3, p.37).', fig.align='center', out.width='90%', cache=TRUE}
# 95% Credible Intervals  
L <- qbeta(0.025, 438, 544)
U <- qbeta(0.975, 438, 544)

post <- Vectorize(function(theta) dbeta(theta, 438, 544))

# Illustration
x1 <- seq(0.375, 0.525,length=10000)
y1 <- post(x1)
plot(x1, y1, type = "l", xlab=~theta, ylab="Density", 
     main = "Uniform prior -> Posterior is Beta(438, 544)",
     lwd = 2, frame.plot = FALSE)
abline(v = 0.488, col = "blue", lty = 3, lwd = 2)

polygon(c(L, x1[x1>=L & x1<= U], U), c(0, y1[x1>=L & x1<=U], 0), col="lightblue")
```


使用 `ggplot2` 繪圖的方法如下，參考原作者的[代碼](https://avehtari.github.io/BDA_R_demos/demos_ch2/demo2_1.html)

```{r 08-intro-to-Bayesfig2, fig.height=4.5, fig.width=6, fig.cap='Probability of a girl given placenta previa (BDA3, p.37), same figure in ggplot format.', fig.align='center', out.width='90%', cache=TRUE}
# seq creates evenly spaced values
df1 <- data.frame(theta = seq(0.375, 0.525, 0.001)) 
a <- 438
b <- 544
# dbeta computes the posterior density
df1$p <- dbeta(df1$theta, a, b)

## 計算95%可信區間
# seq creates evenly spaced values from 2.5% quantile
# to 97.5% quantile (i.e., 95% central interval)
# qbeta computes the value for a given quantile given parameters a and b
df2 <- data.frame(theta = seq(qbeta(0.025, a, b), qbeta(0.975, a, b), length.out = 100))
# compute the posterior density
df2$p <- dbeta(df2$theta, a, b)


ggplot(mapping = aes(theta, p)) +
  geom_line(data = df1) +
  # Add a layer of colorized 95% posterior interval
  geom_area(data = df2, aes(fill='1')) +
  # Add the proportion of girl babies in general population
  geom_vline(xintercept = 0.488, linetype='dotted') +
  # Decorate the plot a little
  labs(title='Uniform prior -> Posterior is Beta(438,544)', y = '') +
  scale_y_continuous(expand = c(0, 0.1), breaks = NULL) +
  scale_fill_manual(values = 'lightblue', labels = '95% posterior interval') +
  theme(legend.position = 'bottom', legend.title = element_blank())
```


可以看到這裡我們計算獲得的事後可信區間是 [0.415, 0.477]，它的上限也低於總體人群的女嬰出生概率= 0.485。所以，應該可以認為如果有孕期有前置胎盤，女嬰出生的概率比人群水平更低。

```{r}
L;U
```



## 貝葉斯理論下的事後二項分佈概率密度方程 notation for probability density functions in binomial data


- $R$ 用來表示服從一個二項分佈的隨機變量， $R\sim \text{Bin}(n, \theta)$。
- $r$ 表示觀察到 $r$ 次成功實驗，實驗次數爲 $n$。
- 先驗概率分佈： $\pi_\Theta(\theta)$
- 應用貝葉斯定理：

$$
\begin{aligned}
\pi_{\Theta|R}(\theta|r) &= \frac{f_R(r|\theta)\pi_\Theta(\theta)}{\int_0^1f_R(r|\theta)\pi_\Theta(\theta)\text{ d}\theta}\\
&= \frac{f_R(r|\theta)\pi_\Theta(\theta)}{f_R(r)}
\end{aligned}
$$


如果我們的先驗概率分佈：

$$\begin{equation}
\pi_\Theta(\theta)=\begin{cases}
1 \text{ if } \theta=0.2\\
0 \text{ otherwise}
\end{cases}
\end{equation}$$

意思就是，我們 100% 相信 $\theta$ 絕對就等於 0.2，不相信 $\theta$ 竟然還能取任何其他值（霸道自大又狂妄的我們）。

如果先驗概率分佈：

$$\begin{equation}
\pi_\Theta(\theta)=\begin{cases}
0.4 \text{ if } \theta=0.2\\
0.6 \text{ if } \theta=0.7
\end{cases}
\end{equation}$$

意思就是，我們有 60% 的把握相信 $\theta=0.7$，有 40% 的把握相信 $\theta=0.2$，稍微傾向於 $\theta=0.7$。

假設進行10次實驗，觀察到3次成功。當 $\theta=0.2$ 時，觀察數據的似然 (likelihood) 爲：

$$f_R(3|\theta=0.2)=\binom{10}{3}0.2^3(1-0.2)^7$$

當 $\theta=0.7$ 時，觀察數據的似然爲：

$$f_R(3|\theta=0.7)=\binom{10}{3}0.7^3(1-0.7)^7$$

應用貝葉斯定理計算事後概率分佈：


$$
\begin{equation}
\pi_{\Theta|R}(\theta|3)=\begin{cases}
\frac{\binom{10}{3}0.2^3(1-0.2)^7\times0.4}{\binom{10}{3}0.2^3(1-0.2)^7\times0.4+\binom{10}{3}0.7^3(1-0.7)^7\times0.6}=0.937 \text{ if } \theta=0.2\\
\frac{\binom{10}{3}0.7^3(1-0.7)^7\times0.6}{\binom{10}{3}0.7^3(1-0.7)^7\times0.6+\binom{10}{3}0.2^3(1-0.2)^7\times0.4}=0.063 \text{ if }\theta=0.7
\end{cases}
\end{equation}
$$

所以，我們從一開始認爲只有40%的把握相信 $\theta=0.2$，觀察數據告訴我們 10 次實驗，3次獲得了成功。所以我們現在有 93.7% 的把握相信 $\theta=0.2$。也就是說，觀察數據讓我們對參數 $\theta$ 的取值可能性發生了質的變化，從原先的傾向於 $\theta=0.7$ 到現在幾乎接近 100% 的認爲 $\theta=0.2$。也就是，觀察數據獲得的信息改變了我們的立場。

上面的例子很直觀，但是有下面幾個問題：

1. 如果我們無法對參數 $\theta$ 賦予先驗概率的點估計時，該怎麼辦？
2. 如果事後概率不是一個離散的分佈時，該如何才能表達事後概率？

## $\theta$ 的先驗概率

一種選擇是，我們用均一分佈 (uniform distribution)，即我們對數據一無所知，認爲所有的 $\theta$ 的可能性都一樣，概率密度方程爲 $1$。在這一情況下，先驗概率爲 1： $\pi_\Theta(\theta)=1$，其事後概率分佈爲：

$$
\begin{equation}
\pi_{\Theta|R}(\theta|r)=\frac{\binom{n}{r}\theta^r(1-\theta)^{n-r}}{\int_0^1\binom{n}{r}\theta^r(1-\theta)^{n-r} \text{ d}\theta}
(\#eq:uniformBayes)
\end{equation}
$$

看到即使在如此簡單的先驗概率下，我們還是要使用複雜的微積分進行計算。幸運的是，像 \@ref(eq:uniformBayes) 的分母這樣的積分公式其實是有跡可循的。這就是 beta ($\beta$) 分佈。

### beta 分佈 the beta distribution {#beta-distribution-intro}


```{r beta-distr, echo=FALSE, fig.height=7, fig.width=6, fig.cap='Beta distribution functions for various values of a, b', fig.align='center', out.width='90%', cache=TRUE}
par(mfrow=c(3,2))
pi <- Vectorize(function(theta) dbeta(theta, 1,1))
curve(pi, xlab=~ theta, ylab="Density", main="Beta prior: a=1, b=1", frame=FALSE, lwd=2)
pi <- Vectorize(function(theta) dbeta(theta, 0.3,1))
curve(pi, xlab=~ theta, ylab="Density", main="Beta prior: a=0.3, b=1",frame=FALSE, lwd=2)
pi <- Vectorize(function(theta) dbeta(theta, 0.3,0.3))
curve(pi, xlab=~ theta, ylab="Density", main="Beta prior: a=0.3, b=0.3",frame=FALSE, lwd=2)
pi <- Vectorize(function(theta) dbeta(theta, 2,2))
curve(pi, xlab=~ theta, ylab="Density", main="Beta prior: a=2, b=2",frame=FALSE, lwd=2)
pi <- Vectorize(function(theta) dbeta(theta, 8,2))
curve(pi, xlab=~ theta, ylab="Density", main="Beta prior: a=8, b=2",frame=FALSE, lwd=2)
pi <- Vectorize(function(theta) dbeta(theta, 2,8))
curve(pi, xlab=~ theta, ylab="Density", main="Beta prior: a=2, b=8",frame=FALSE, lwd=2)
```


你會發現，beta分佈的圖形特徵由它的兩個超參數 (hyperparameter) `a, b` 決定。當相對地 `a` 比較大的時候，beta分佈的概率多傾向於較靠近橫軸右邊，也就是1的位置（較高概率），相對地 `b` 比較大的時候，beta分佈的曲線多傾向於靠近橫軸左邊，也就是0的位置（較低概率）。如果 `a` 和 `b`同時都在變大的話，beta分佈的曲線就變得比較“瘦”。這樣決定概率分佈形狀的參數，又被叫做**形狀參數 (shape parameters)**




我們定義 $a>0$ 時[伽馬方程](https://zh.wikipedia.org/wiki/%CE%93%E5%87%BD%E6%95%B0)爲

$$\Gamma(a)=\int_0^\infty x^{a-1}e^{-ax}\text{ d}x$$

當 $a$ 取正整數時， $\Gamma(a)$ 是 $(a-1)!$。例如，當 $a=4, \Gamma(a)=3\times2\times1=6$。

對於 $\theta\in[0,1]$ 時，beta 方程 $Beta(a,b)$ 被定義爲：

$$
\begin{aligned}
\pi\Theta(\theta|a,b) &= \theta^{a-1}(1-\theta)^{b-1}\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\\
&= \frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)}
\end{aligned}
$$


其中 
$$B(a,b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)} = \int_0^1\theta^{a-1}(1-\theta)^{b-1}\text{d}\theta$$

**莫要混淆 B 函數和 Beta 方程。B 函數的性質和詳細定義可以參照[其維基百科頁面](https://zh.wikipedia.org/wiki/%CE%92%E5%87%BD%E6%95%B0)。**


利用 Beta 方程作爲先驗概率顯得十分便捷且靈活。圖 \@ref(fig:beta-distr) 展示的是 6 種不同的 $(a,b)$ 取值下的先驗概率分佈示意圖。其實我們可以看到，包括均一分佈在內的各種可能性都可以通過 Beta 分佈實現。其中 $(a,b)$ 被叫做超參數 (hyperparameter)。$(a,b)$ 取值越大，先驗概率分佈的方差越小。

關於 Beta 分佈的幾個性質：

1. 均值：$\text{mean}=\frac{a}{a+b}$；
2. 衆數：$\text{mode}=\frac{a-1}{a+b-2}$；
3. 方差：$\text{variance}=\frac{ab}{(a+b)^2(a+b+1)}$。

回到均一分佈的簡單例子 \@ref(eq:uniformBayes) 上：

$\pi_\Theta(\theta)=Beta(1,1)$ 是 $\theta\in[0,1]$ 上的均一分佈。所以事後概率 posterior 和下面的式子成正比：

$$\theta^r(1-\theta)^{n-r}$$

換句話說，事後概率分佈服從 $Beta(r+1,n-r+1)$ 分布，均值爲 $\frac{r+1}{n+2}$，方差爲 $\frac{(1+r)(n-r+1)}{(n+2)^2(n+3)}$。

由此可見，在貝葉斯統計思維下，先驗概率爲均一分佈的二項分佈數據，其事後概率分佈的均值和方差，和經典概率論下的極大似然估計 $r/n$ 不同，和它的漸進樣本方差 $r(n-r)/n^3$ 也不同。但是，當 $n$ 越來越大，獲得的觀察數據越多提供的信息越來越多以後，我們會發現事後概率分佈的均值和方差也會越來越趨近於經典概率論下的極大似然估計和它的方差。

於是這裏可以總結以下兩點：

1. 即使先驗概率對參數毫無用處（不能提供有效信息，或者我們對所觀察的數據一無所知），也可能會對事後概率分佈結果提供一些意外的信息。
2. 當樣本量增加，似然就主導了整個貝葉斯方程，在數學計算上，經典概率論和貝葉斯推理的估計結果將會十分接近。當然，其各自的意義還是截然不同的。

### 二項分佈數據事後概率分佈的一般化：共軛性 {#conjugate}

當 $r\sim \text{Binomial}(n,\theta)$ 時，如果先驗概率 $\pi_\Theta(\theta)=\text{Beta}(a,b)$。那麼參數 $\theta$ 的事後概率分佈的密度方程滿足：

$$\pi_{\Theta|r}(\theta|r)=\text{Beta}(a+r, b+n-r)$$

它的事後概率分佈均值爲：

$$E[\theta|r]=\frac{a+r}{a+b+n}$$

事後概率分佈的衆數爲：

$$\text{Mode}[\theta|r]=\frac{a+r-1}{n+a+b-2}$$

事後概率分佈方差爲：

$$\text{Var}[\theta|r]=\frac{(a+r)(b+n-r)}{(a+b+n)^2(a+b+n+1)}$$

因此，我們看到先驗概率服從 $\text{Beta}(a,b)$ 分佈，觀察數據爲二項分佈時，事後概率分佈還是服從 $\text{Beta}$ 分佈，僅僅只是超參數發生了轉變（更新）。這就是**共軛分佈**的實例。$\text{Beta}$ 分佈是二項分佈的共軛先驗概率分佈 (the $\text{Beta}(a,b)$ is the conjugate prior for the binomial likelihood)。

在經典概率論的框架下，參數 $\theta$ 的估計就是極大似然估計 (MLE)。在二項分佈的例子中， $\text{MLE}=\hat\theta=r/n$，當樣本量 $n\rightarrow\infty$ 時，事後概率分佈均值：

$$E[\theta|r]=\frac{a+r}{a+b+n}=\frac{\frac{r}{n}+\frac{a}{n}}{1+\frac{a+b}{n}}\approx\frac{r}{n}=\text{MLE}$$

事後概率分佈的衆數爲：

$$
\begin{aligned}
\text{Mode}[\theta|r] &=\frac{a+r-1}{n+a+b-2} \\
&= \frac{\frac{r}{n}+\frac{a-1}{n}}{1+\frac{a+b-2}{n}}\\
&\approx \frac{r}{n}
\end{aligned}
$$

事後概率分佈的方差爲：

$$\frac{(a+r)(b+n+r)}{(a+b+n)^2(a+b+n+1)}\approx0$$

當 $n$，樣本越來越大時，我們獲得更多的來自數據的信息，所以來自數據的信息逐漸主導 (dominate) 了整個貝葉斯推斷的過程，事後均值等衆多統計結果都越來越趨近於概率論統計思想下的極大似然估計等結論。

我們也可以注意到，當 $a\rightarrow0, b\rightarrow0$ 時，事後概率分佈的均值 $E[\theta|r] = \frac{a+r}{a+b+n} \rightarrow \frac{r}{n}$，方差也趨向於樣本漸進方差 (asymptotic sample variance)。但是當 $a\rightarrow 0, b\rightarrow0$ 時，先驗概率是沒有被定義的，可是此例下事後概率卻可以正常被定義。所以當先驗概率分佈無法被定義，或者被定義的不恰當時，事後概率分佈依然不受太大影響。所以特別是對於均值（或迴歸係數，regression coefficients）等參數，我們常常會使用均一分佈這樣的無信息先驗概率。


## 附贈--加量不加價

**當樣本數據服從二項分布時使用 $\text{Beta}(a,b)$ 作爲先驗概率分布，試推導事後概率分布。並且證明 Beta 分布是二項分布的共軛分布 (conjugate distribution)。**

- 當數據服從二項分布時，其似然方程 (或概率方程) 爲: <br> $$f(n,r|\theta) = \binom{n}{r}\theta^r(1-\theta)^{n-r}$$

- 用 $\text{Beta}(a,b)$ 做先驗概率分布 ($a,b$ 是 $\theta$ 的超參數)，那麼 <br>

$$
\begin{aligned}
\pi(\theta) &= \text{Beta}(\theta|a,b) \\
& =  \theta^{a-1}(1-\theta)^{b-1}\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \\ 
& = \frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)} \\
\text{Where } B(a,b) &=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)} = \int_0^1\theta^{a-1}(1-\theta)^{b-1}\text{d}\theta
\end{aligned}
$$

- 利用貝葉斯定理，後概率分布就可以推導爲 <br> 

$$
\begin{aligned}
\pi(\theta|n,r) & = \frac{f(n,r|\theta)\pi(\theta)}{\int f(n,r |\theta)\pi(\theta)\text{d}\theta}   \\
                & = \frac{\binom{n}{r}\theta^r(1-\theta)^{n-r}\pi(\theta)}{\int_0^1\binom{n}{r}\theta^r(1-\theta)^{n-r} \pi(\theta) \text{ d}\theta} \\
                & = \frac{\binom{n}{r}\theta^r(1-\theta)^{n-r}\frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)}}{\int_0^1\binom{n}{r}\theta^r(1-\theta)^{n-r} \frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)} \text{ d}\theta} \\
                & = \frac{\binom{n}{r}\theta^{r+a-1}(1-\theta)^{n+b-r-1}}{\int_0^1\binom{n}{r}\theta^{r+a-1}(1-\theta)^{n+b-r-1} \text{ d}\theta} \\
                & = \frac{\theta^{r+a-1}(1-\theta)^{n+b-r-1}}{B(r+a, n+b-r)} \\
                & \sim \text{Beta}(r+a, n+b-r)
\end{aligned}
$$
這個推導的結果告訴我們，先驗概率 $\pi(\theta) =  \frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)}\sim \text{Beta}(a,b)$，通過和二項分布的似然方程相乘，獲得後驗概率 $\pi(\theta|n,r) = \frac{\theta^{r+a-1}(1-\theta)^{n+b-r-1}}{B(r+a, n+b-r)}\sim\text{Beta}(r+a, n+b-r)$。這兩個概率都服從 $\text{Beta}$ 分布，也就是證明了 $\text{Beta}$ 分布，是二項分布的共軛分布。新產生的事後概率的性質有: <br>

1. $\theta$ 的均值 mean $\mu= \frac{a+r}{a+b+n}$;
2. $\theta$ 的方差 variance $\sigma^2 = \frac{(a+r)(n+b-r)}{(a+b+n)^2(a+b+n+1)}$;
