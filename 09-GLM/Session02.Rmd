
線性迴歸方法是十分強大的建模工具，可惜的是它只能適用與因變量爲連續型變量的情況。廣義線性迴歸模型 (或者叫一般化線性迴歸模型 generalised linear models, GLM) 是一大類將線性迴歸模型拓展到因變量可以使用二進制，計數，分組型變量的建模工具。

## 指數分佈家族

一個服從正態分佈的隨機變量 $Y$ 的概率密度方程 (probability density function, PDF) 可以寫作

$$
f(y) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y-\mu)^2}{2\sigma^2}}
$$

給 PDF 的左右兩邊同時取自然底數的對數，方程變形爲

$$
\begin{aligned}
\text{ln}\{f(y)\} & = -\frac{y^2}{2\sigma^2} + \frac{y\cdot\mu}{\sigma^2} - \frac{\mu^2}{2\sigma^2} -\frac{1}{2}\text{ln}(2\pi\sigma^2) \\
                & = \frac{y\cdot\mu - \frac{\mu^2}{2}}{\sigma^2} - [\frac{y^2}{2\sigma^2} + \frac{1}{2}\text{ln}(2\pi\sigma^2) ]
\end{aligned}
(\#eq:glm2-0)
$$

如果令

$$
\begin{aligned}
\theta & = \mu  \\
\psi   & = \sigma^2 \\
b(\theta) & = \frac{\mu^2}{2} \\
c(y, \theta) & = \frac{y^2}{2\sigma^2} + \frac{1}{2}\text{ln}(2\pi\sigma^2)
\end{aligned}
$$

那麼上面的式子 \@ref(eq:glm2-0) 可以被整理爲：

$$
\begin{equation}
\text{ln}\{f(y)\} = \frac{y\cdot\theta - b(\theta)}{\psi} - c(y, \theta)
\end{equation}
(\#eq:glm2-1)
$$

**此處有重要結論：** 凡是分佈的概率密度方程的對數方程能夠轉換整理成 \@ref(eq:glm2-1) 形式的分佈，都隸屬於指數分佈家族 (the Exponential Family of distributions)。

### 泊松分佈和二項分佈的指數分佈家族屬性

- 泊松分佈 Poisson Distribution

$$
\begin{aligned}
     f(y) & = \text{Pr}(Y = y) = \frac{\mu^y e^{-\mu}}{y!}, y = 0,1,2,\cdots \\
\text{ln}\{ f(y) \} & = y\cdot\text{ln}(\mu) - \mu - \text{ln}(y!) \\
\text{Let } &\color{red}{\boxed{\theta = \text{ln}(\mu), \psi = 1, b(\theta) = \mu, c(y,\psi) = \text{ln}(y!)}} \\
\Rightarrow \text{ln}\{f(y)\} & = \frac{y\cdot\theta - b(\theta)}{\psi} - c(y, \theta) \\
\end{aligned}
$$

所以，**泊松分佈屬於指數分佈家族成員**。

- 二項分佈 Binommial Distribution

$$
\begin{aligned}
f(y) & = \text{Pr}(Y = y) = \binom{n}{y}\pi^y(1-\pi)^{n-y}, y = 0,1,2,\cdots\\
\text{ln}\{ f(y) \} & = y\cdot \text{ln}(\pi) + (n - y)\text{ln}(1-\pi) + \text{ln}\{\binom{n}{y}\} \\
                    & = y\cdot \text{ln}(\frac{\pi}{1-\pi}) + n\text{ln}(1-\pi) +  \text{ln}\{\binom{n}{y}\} \\
\text{Let } &\color{red}{\boxed{\theta = \text{ln}(\frac{\pi}{1-\pi}), \psi = 1,}} \\
            &\color{red}{\boxed{b(\theta) = -n\text{ln}(1-\pi), c(y, \psi) = -\text{ln}\{\binom{n}{y}\}}}\\
\Rightarrow \text{ln}\{f(y)\} & = \frac{y\cdot\theta - b(\theta)}{\psi} - c(y, \theta) \\
\end{aligned}
$$

所以，**二項分佈也屬於指數分佈家族成員**。

指數分佈家族成員的數學表達式  \@ref(eq:glm2-1)  中，

- $\theta$ 被叫做標準 (或者叫自然) 參數 (**canonical or natural parameter**)，相關的函數被叫做標準鏈接函數 (canonical link function)，如上面所列舉的例子中：泊松分佈時用的對數函數 $\text{ln}(\mu)$，二項分佈時用的邏輯函數 (logit function) $\text{ln}(\frac{\pi}{1-\pi})$，鏈接函數可能還有別的選擇，(例如，二項分佈數據的另一種標準鏈接函數是概率函数 (probit function $\Phi^{-1}(P)$))，同時它對於條件推斷 conditional inference 至關重要，因爲它還提示我們應該用什麼樣的算法去估計我們苦苦尋找的人羣參數。
- $\phi$ 被命名爲**尺度參數 (scale or dispersion parameter)**，泊松分佈和二項分佈的尺度參數是 $1$。但是正態分佈的尺度參數是方差 $\sigma^2$，且常常是未知的，需要從樣本數據中估計。尺度參數是否需要從樣本中獲取其估計值，對於實際統計推斷或者假設檢驗的過程有重大影響。

廣義線性迴歸就是這個指數分佈家族數據共通的一種統計建模過程，所以，在這一“屋檐”下，它衍生出衆多種類的統計模型。

------------------

### Exercise. Exponential distribution

證明指數分佈本身也屬於指數分佈家族，定義其標準鏈接函數和標準參數。

**證明**

$$
\begin{aligned}
Y \sim \text{exp}(\lambda) & \rightarrow f(y) = \lambda \text{exp}(-y\lambda), y > 0\\
\Rightarrow \text{ln}\{ f(y) \} & = - y \lambda + \text{ln}(\lambda) \\
\text{Let } & \color{red}{\theta = -\lambda, b(\theta) = - \text{ln}(\lambda), \phi = 1, c(y, \phi) = 0} \\
\Rightarrow \text{ln}\{f(y)\} & = \frac{y\cdot\theta - b(\theta)}{\phi} - c(y, \theta) \\
\text{Because } E(Y) & = \frac{1}{\lambda}, \text{ the canonical link is } g(\lambda) = -\frac{1}{\lambda}\\
\end{aligned}
$$

------------------


## 廣義線性迴歸模型之定義 {#defineaGLM}

一個四肢健全的廣義線性模型包括三個部分：

1. 因變量分佈 (或者叫響應變量分佈，response distribution)：$Y_i, i = 1,\cdots,n$ 可以被認爲是互相獨立且服從指數家族分佈，設其期望值 (均值) $E(Y_i) = \mu_i$；
2. 線性預測方程 (linear predictor)：**預測變量及其各自的參數以線性迴歸形式進入模型**，其中第 $i$ 個觀測值的線性預測值爲：<br> $$\eta_i = \alpha + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}$$
3. 鏈接函數 (link function)：鏈接函數連接的是線性預測方程 $\eta_i$ 和其期待值 (均值) 之間 $\mu_i$ 的關係。<br> $$g(\mu_i) = \eta_i$$


簡單線性迴歸模型本身當然也數據廣義線性迴歸模型：

1. 因變量分佈是正態分佈；
2. 線性預測值也是線性迴歸形式；
3. 鏈接函數是它因變量本身 (the **identity** function)。


## 注意

1. 廣義線性迴歸的線性預測方程部分的意義，需要澄清的是它指的是 **參數 parameter** 之間呈線性關係，預測變量本身可以有二次方，三次方，多次方，因爲這些多項式線性迴歸本身仍然是**線性的**如： $$\eta_i = \alpha + \beta_1 x_i + \beta_2 x_i^2 + \cdots + \beta_p x_i^p$$ <br> 然而，這樣的形式 $$\eta_i = \alpha (1- e^{\beta_1 x_{i1}})$$ <br> 就不能說是一個線性預測方程。
2. 除了有很少的特例。廣義線性迴歸擬合後的參數估計，推斷，模型評價和比較時使用的原理都一樣，不同的只有各自的分佈和鏈接函數。
3. 通常選用的鏈接方程，要能夠使線性預測方程的取值範圍達到所有實數 $-\infty,+\infty$。
4. “模型的似然函數 the log likelihood of the model”，只是我們偷懶縮短了原文 “在給定數據的前提下，當所有參數均爲 $\text{MLE}$ 時模型的對數似然函數 (the log likelihood function of the model for the given data evaluated at the MLE's of the parameters)”，就是對數似然函數的極大值的意思 (i.e. the maximum of the log likelihood function)。
5. 從本節開始往後的章節中 “模型，model”，“廣義線性模型，generalized linear model”，和 "GLM" 將被視爲同義詞。

## 如何在 R 裏擬合 "GLM"

這裏討論用極大似然法擬合 "GLM" 模型的方法。前面一章節的複習也是在告訴我們，利用極大似然法簡單說就是找到模型參數，使得似然函數能夠取到極大值。對於線性迴歸來說， $\text{MLE}$ 可以用一個封閉式函數來計算；但是廣義線性迴歸模型則必須使用[迭代法計算 (iterative methods)](https://www.youtube.com/watch?v=JZIeX3eVyf4)。

在 R 裏面擬合廣義線性模型的命令及其格式是：

```{r GLM-0201, eval = FALSE}
glm(response variable ~ explanatory variables to form linear predictor, family=name of distribution(link=link function), data=dataset)
```

Tips: See `help(glm)` for other modeling options. See `help(family)` for other allowable link functions for each family.

下面的數據來自一個心理學臨牀實驗，比較的是和安慰劑組相比，注射嗎啡組，注射海洛因組對象的精神病檢測指數的前後變化。

```{r GLM-0202, cache = TRUE}
Mental <- read.table("../backupfiles/MENTAL.DAT", header =  FALSE, sep ="", col.names = c("treatment", "Before", "After"))
Mental$treatment[Mental$treatment == 1] <- "placebo"
Mental$treatment[Mental$treatment == 2] <- "morphine"
Mental$treatment[Mental$treatment == 3] <- "heroin"
Mental$treatment <- factor(Mental$treatment, levels = c("placebo", "morphine", "heroin"))
head(Mental)
```

我們來比較一下簡單線性迴歸的代碼輸出結果和廣義線性迴歸代碼輸出結果是否一致：

用 `lm` 命令，擬合因變量爲注射後精神病檢測指數，預測變量爲治療方式和注射錢精神病檢測指數，及兩者的交互作用項：

```{r GLM-0203, cache=TRUE}
Model1 <- lm(After ~ treatment*Before, data = Mental)
summary(Model1)
```

同樣的模型也可以用 `glm` 命令擬合：

```{r GLM-0204, cache=TRUE}
Model2 <- glm(After ~ treatment*Before, family = gaussian(link = "identity"), data = Mental)
summary(Model2)
```

可以看到，`glm` 命令的輸出結果略多，但是參數估計的部分是完全相同的。**但是如果你用的是坑爹的 STATA，那裏面的 `glm` 命令中的統計檢驗量和 $p$ 值用的則是正態分佈近似法。所以在 STATA 裏面簡單線性迴歸模型最好不要使用 `glm` 命令：**

```
 glm After i.treatt##c.Before, family(gaussian) link(identity)

Iteration 0:   log likelihood = -185.70711

Generalized linear models                         No. of obs      =         72
Optimization     : ML                             Residual df     =         66
                                                  Scale parameter =   11.10799
Deviance         =  733.1276068                   (1/df) Deviance =   11.10799
Pearson          =  733.1276068                   (1/df) Pearson  =   11.10799

Variance function: V(u) = 1                       [Gaussian]
Link function    : g(u) = u                       [Identity]

                                                  AIC             =   5.325197
Log likelihood   =  -185.707106                   BIC             =   450.8676

------------------------------------------------------------------------------
             |                 OIM
        After|      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
       treat |
          2  |  -1.211742   1.750342    -0.69   0.489    -4.642349    2.218865
          3  |  -1.461968   1.771855    -0.83   0.409    -4.934741    2.010805
             |
      Before |   .5939394   .1834682     3.24   0.001     .2343483    .9535305
             |
 treat#Before|
          2  |  -.0895258   .2483459    -0.36   0.718    -.5762749    .3972233
          3  |  -.3129855   .2503829    -1.25   0.211     -.803727    .1777561
             |
       _cons |    1.97803   1.294069     1.53   0.126    -.5582981    4.514359
------------------------------------------------------------------------------
```

回到 R 來， 當儲存了一個 `Model2` 向量在 R 裏之後，你可以用下面的各種命令獲取你想要的各種有用的信息。

```{r  GLM-0205, cache=TRUE, message=FALSE}
confint(Model2) # 95% CI for the coefficients
exp(coef(Model2)) # exponentiated coefficients
exp(confint(Model2)) # 95% CI for exponentiated coefficients
head(predict(Model2, type="response")) # predicted values
head(residuals(Model2, type="deviance")) # residuals
```

### `margins` 命令

一個在 STATA 裏面十分有用的用於**預測**的命令 `margins`，在 R 裏，下載了 `margins` 包以後就可以調用和 STATA 的 `margins` 類似的命令。

假如我們用擬合的模型預測當注射前精神病檢測值分別是 0，6，12 分時三組之間的注射後精神病檢測值差，可以這樣求：

```{r  GLM-0206, cache=TRUE}
summary(margins(Model2, at = list(Before=c(0,6,12))))
```


對比 STATA 裏的結果：

```
 margins, dydx(trt) at(pre = (0 6 12))

Conditional marginal effects                    Number of obs     =         72
Model VCE    : OIM

Expression   : Predicted mean post, predict()
dy/dx w.r.t. : 2.trt 3.trt

1._at        : pre             =           0

2._at        : pre             =           6

3._at        : pre             =          12

------------------------------------------------------------------------------
             |            Delta-method
             |      dy/dx   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
1.trt        |  (base outcome)
-------------+----------------------------------------------------------------
2.trt        |
         _at |
          1  |  -1.211742   1.750342    -0.69   0.489    -4.642349    2.218865
          2  |  -1.748897    .963025    -1.82   0.069    -3.636391    .1385977
          3  |  -2.286051   1.797717    -1.27   0.204    -5.809513     1.23741
-------------+----------------------------------------------------------------
3.trt        |
         _at |
          1  |  -1.461968   1.771855    -0.83   0.409    -4.934741    2.010805
          2  |  -3.339881   .9623512    -3.47   0.001    -5.226054   -1.453707
          3  |  -5.217794   1.796264    -2.90   0.004    -8.738406   -1.697181
------------------------------------------------------------------------------
Note: dy/dx for factor levels is the discrete change from the base level.
```

### `ggplot2::geom_smooth(method = "loess")` 命令

類似 STATA 作散點圖時的 `lowess` 命令，在 R 裏，你可以用 `ggplot2` 包裏自帶的 `geom_smooth(method = "loess")` 選項命令，給散點圖添加平滑曲線。把觀測數據中變量之間的關係視覺化，用於輔助判斷一個模型是否可以被擬合爲線性關係。全稱是 “locally weighted scatterplot smoothing”，縮寫成 "lowess/loess"。[LOWESS 的原理](https://en.wikipedia.org/wiki/Local_regression)簡略說是，通過把預測變量分成幾個部分，分別在各個小區間內擬合迴歸各自的迴歸曲線，如此便可以將**每個觀測值都以各自不同的加權值放入整個模型**中，然而正如我們在簡單線性模型中提到過的，這樣的曲線更加擬合觀測數據，而不能說明觀測值來自的人羣中，兩個變量之間的關係。此方法的靈活性在於，你可以選擇平滑的程度，該平滑程度用 `bandwith`(STATA) 或者 `span`(R) 表示，取值範圍是 $0 \sim 1$ 之間的任意值，越靠近 $1$，Lowess 曲線越接近簡單線性直線，越靠近 $0$，那麼每個觀測點本身的權重越大，擬合的 Lowess 曲線越接近觀測數據本身。下圖 \@ref(fig:loess-smoother1) 提示，選用的平滑程度 $= 0.8$ 時，精神病測量分數在 (安慰劑組中) 實驗前後的關係接近線性關係。當我們降低平滑程度，Lowess 曲線接近觀測數據本身，其實是太接近觀測數據本身，反而無法提供太多的信息。


```{r loess-smoother1, echo=TRUE, fig.width=7, fig.height=10, fig.cap="Lowess smoother, with bandwith/span set to 0.8, for the mental data", fig.align='center', out.width='100%', cache=TRUE}
ggplot(Mental, aes(Before, After)) + geom_point() +
  geom_smooth(method = "loess",  span = 0.8, se = FALSE) +
  facet_grid(treatment ~ .) + theme_bw()
```


```{r loess-smoother2, echo=TRUE, fig.width=7, fig.height=10, fig.cap="Lowess smoother, with bandwith/span set to 0.4, for the mental data", fig.align='center', out.width='100%', cache=TRUE, message=FALSE, warning=FALSE}
ggplot(Mental, aes(Before, After)) + geom_point() +
  geom_smooth(method = "loess",  span = 0.4, se = FALSE) +
  facet_grid(treatment ~ .) + theme_bw()
```
