
## 虛假的相關性

一起來看看一個關於結婚，離婚，和美國每個州的華夫餅餐廳的個數的數據：

```{r introBayes08-01, cache=TRUE}
data("WaffleDivorce")
d <- WaffleDivorce

head(d)
```

把數據標準化：

```{r introBayes08-02, cache=TRUE}
d$D <- standardize( d$Divorce )
d$M <- standardize( d$Marriage )
d$A <- standardize( d$MedianAgeMarriage )
```

先粗略繪製年齡中位數和離婚率標準化之後的散點圖：

```{r introBayes08-fig01, cache=TRUE,  fig.width=6, fig.height=5,  fig.cap="Divorce rate looks inversely associated with median age at marriage.", fig.align='center'}
plot(d$D, d$A)
```


結婚時年齡的中位數和離婚率之間的關係如果假定是線性的，那麼我們描述它的模型如下：

$$
\begin{aligned}
D_i & \sim \text{Normal}(\mu_i, \sigma) \\ 
\mu_i & = \alpha + \beta_A A_i \\
\alpha & \sim \text{Normal}(0, 0.2) \\
\beta_A & \sim \text{Normal}(0, 0.5) \\ 
\sigma & \sim \text{Exponential}(1)
\end{aligned}
$$

其中，

- $D_i$ 是第 $i$ 個州的標準化後的離婚率 (均值為零，標準差為1)
- $A_i$ 是第 $i$ 個州的標準化後的結婚年齡中位數


由於預測變量和結果變量都被標準化了，他們組成的線型回歸函數的截距 $\alpha$ 應該十分接近 0。至於斜率 $\beta_A$，它的涵義是，每增加一個標準差單位的結婚年齡中位數，隨之增加的標準差離婚率。那麼一個單位的結婚年齡標準差是多少呢？

```{r  introBayes08-03, cache=TRUE}
sd( d$MedianAgeMarriage )
```

上述先驗概率分佈 $\beta_A \sim \text{Normal}(0, 0.5)$ 其實暗示我們認為這個斜率大於 1 的概率很低，低於 2.275%：


```{r  introBayes08-04, cache=TRUE}
1 - pnorm(1, 0, 0.5)
```

實際使用二次方程近似法獲取其事後概率分佈的代碼如下：

```{r introBayes08-05, cache=TRUE}
m5.1 <- quap(
  alist(
    D ~ dnorm( mu, sigma ), 
    mu <- a + bA * A, 
    a ~ dnorm(0, 0.2), 
    bA ~ dnorm(0, 0.5), 
    sigma ~ dexp(1)
  ), data = d
)

precis(m5.1)
```

看看我們的先驗概率分佈都會給出哪些可能性：

```{r introBayes08-fig02, cache=TRUE,  fig.width=6, fig.height=5,  fig.cap="Plausible regression lines implied by the priors in m5.1. These are weakly informative priors that they allow some implusbly strong relationships but generally bound the lines to possible ranges of the variables.", fig.align='center'}
set.seed(10)
prior <- extract.prior( m5.1 )

mu <- link(m5.1, post = prior, data = list(A = c(-2, 2)))

plot(NULL, xlim = c(-2, 2), ylim = c(-2, 2),
      xlab = "Median age married (std)", 
     ylab = "Divorce rate (std)")
for( i in 1:50) lines(c(-2, 2), mu[i, ], col = col.alpha("black", 0.4))
```

繪製實際事後估計的回歸直線：

```{r introBayes08-fig03, cache=TRUE,  fig.width=6, fig.height=5,  fig.cap="Divorce rate is negatively associated with median age at marriage.", fig.align='center'}
# compute percentile interval of mean
A_seq <- seq(from = -3, to = 3.2, length.out = 30)
mu <- link(m5.1, data = list(A = A_seq))
mu.mean <- apply( mu, 2, mean )
mu.PI <- apply(mu, 2, PI)


# plot

plot( D ~ A, data = d, col = rangi2, 
       xlab = "Median age married (std, years)", 
     ylab = "Divorce rate (std, per/1000 adults)", 
     xaxt = "n", yaxt = "n")
at <- c(-2, -1, 0, 1, 2, 3)
labels <- at*sd(d$MedianAgeMarriage) + mean(d$MedianAgeMarriage)
labelsy <- at*sd(d$Divorce) + mean(d$Divorce)
lines( A_seq, mu.mean, lwd = 2)
shade( mu.PI, A_seq )
axis( side = 1, at = at, labels = round(labels, 1))
axis( side =2, at = at,  labels = round(labelsy, 2))
```

我們可以使用相似的方法計算並繪製結婚率和離婚率之間可能存在的線性關係：

```{r  introBayes08-06, cache=TRUE}
m5.2 <- quap(
  alist(
    D ~ dnorm( mu, sigma ), 
    mu <- a + bM * M, 
    a ~ dnorm( 0, 0.2 ), 
    bM ~ dnorm( 0, 0.5 ), 
    sigma ~ dexp(1)
  ), data = d
)
precis(m5.2)
```




```{r introBayes08-fig04, cache=TRUE,  fig.width=6, fig.height=5,  fig.cap="Divorce rate is negatively associated with median age at marriage.", fig.align='center'}
# compute percentile interval of mean
M_seq <- seq(from = -3, to = 3.2, length.out = 30)
mu <- link(m5.2, data = list(M = M_seq))
mu.mean <- apply( mu, 2, mean )
mu.PI <- apply(mu, 2, PI)


# plot

plot( D ~ M, data = d, col = rangi2, 
       xlab = "Marriage rate (std, per/1000 adults)", 
     ylab = "Divorce rate (std, per/1000 adults)", 
     xaxt = "n", yaxt = "n")
at <- c(-2, -1, 0, 1, 2, 3)
labels <- at*sd(d$Marriage) + mean(d$Marriage)
labelsy <- at*sd(d$Divorce) + mean(d$Divorce)
lines( A_seq, mu.mean, lwd = 2)
shade( mu.PI, A_seq )
axis( side = 1, at = at, labels = round(labels, 1))
axis( side =2, at = at,  labels = round(labelsy, 2))
```

似乎結婚率和離婚率存在著正關係。

## 繪製輔助我們理解的有向無環圖
 
```{r introBayes08-fig05, cache=TRUE,  fig.width=3.8, fig.height=3,  fig.cap="A possible DAG for the divorce rate data. (A = median of age at marriage; M = marriage rate; D = divorce rate)", fig.align='center'}
# library(dagitty)
dag5.1 <- dagitty( "dag{ A -> D; A -> M; M -> D }" )
coordinates(dag5.1) <- list( x=c(A=0,D=1,M=2) , y=c(A=0,D=1,M=0) )
drawdag( dag5.1 )
```

圖 \@ref(fig:introBayes08-fig05) 顯示了三者之間的關係：

1. A直接影響D
2. M直接影響D
3. A直接影響M

也就是說，A有兩個路徑可以影響到D：

- A $\rightarrow$ D
- A $\rightarrow$ M $\rightarrow$ D


```{r introBayes08-fig06, cache=TRUE,  fig.width=3.8, fig.height=3,  fig.cap="Another possible DAG for the divorce rate data, M does not influence D. (A = median of age at marriage; M = marriage rate; D = divorce rate)", fig.align='center'}
# library(dagitty)
dag5.1.1 <- dagitty( "dag{ A -> D; A -> M }" )
coordinates(dag5.1.1) <- list( x=c(A=0,D=1,M=2) , y=c(A=0,D=1,M=0) )
drawdag( dag5.1.1 )
```

在第二個 DAG 圖 \@ref(fig:introBayes08-fig06) 中，一個可以被驗證的關係是：

$$
D \perp\!\!\!\perp M |A
$$
可以通過 `dagitty` 的提示告訴我們相同的信息：

```{r　introBayes08-07, cache=TRUE}
DMA_dag2 <- dagitty('dag{ D <- A -> M }')
impliedConditionalIndependencies(DMA_dag2)
```

第一個 DAG 圖的關係如下：

```{r introBayes08-08, cache=TRUE}
DMA_dag1 <- dagitty('dag{ D <- A -> M -> D}')
impliedConditionalIndependencies(DMA_dag1)
```

你會看見你的計算機不給出任何結果，這是因為此時沒有條件獨立性的關係。


多重線型回歸，其實是幫我們描述這樣一個問題：

> 當已知的變量都已經控制了的時候，新增的一個變量，是否會對結果變量的信息還有任何額外的貢獻？
> Is there any additional value in knowing a variable, once I already know all of the other predictor variables?

## 多重線性回歸模型的表達

入果我們為離婚率數據設定結婚年齡中位數，和結婚率兩個變量作為預測變量的線性回歸模型，可以描述成；


$$
\begin{aligned}
D_i & \sim \text{Normal}(\mu_i, \sigma)   & \text{[Probability of data]} \\
\mu_i & = \alpha + \beta_M M_i + \beta_A A_i  & \text{[Linear model]} \\
\alpha & \sim \text{Normal}(0, 0.2)  & \text{[Prior for } \alpha] \\
\beta_M & \sim \text{Normal}(0, 0.5) & \text{[Prior for } \beta_M] \\
\beta_A & \sim \text{Normal}(0, 0.5) & \text{[Prior for } \beta_A] \\ 
\sigma  & \sim \text{Exponential}(1) & \text{[Prior for } \sigma]
\end{aligned}
$$

實際運行這個模型：

```{r introBayes08-09, cache=TRUE}
m5.3 <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bM * M + bA * A, 
    a ~ dnorm(0, 0.2), 
    bM ~ dnorm(0, 0.5),
    bA ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)
precis(m5.3)
```



我們來比較一下 `bM, bA` 在不同模型中的表現：

```{r introBayes08-fig07, cache=TRUE,  fig.width=5.5, fig.height=4,  fig.cap="The posterior distributions for parameters of bA and bM among all models.", fig.align='center'}
plot( coeftab(m5.1, m5.2, m5.3), par = c("bA", "bM"), 
      xlab = "Posterior with 89% credible intervals")
text(-0.8, 9, "bA", font = 2)
text(-0.3, 4, "bM", font = 2)
```


這個圖顯示的是回歸係數 `bA, bM` 在不同模型中的表現。

- m5.1是只放了一個結婚率 M
- m5.2是只放了一個結婚年齡中位數 A
- m5.3是同時放入結婚率M，和年齡中位數 A


可以觀察到，年齡中位數的回歸係屬並不因為是否模型中加入了結婚率 M 這個變量有多大的變化。但是反過來，結婚率 M 和離婚率的關係的回歸係屬，只有在沒有加入年齡中位數變量的 m5.2 模型中才顯現出來，也就是說：“當我們已經知道了結婚年齡的中位數，增加結婚率這個變量並不會對我們了解離婚率有幫助。”

上面廢話了這麼多，其實就是：$D \perp\!\!\!\perp M |A$。所以我們探討的兩個描述這個數據的 DAG 圖 \@ref(fig:introBayes08-fig05) 和圖 \@ref(fig:introBayes08-fig06) 中，前者不能提供上述信息，它被排除了。保留圖 \@ref(fig:introBayes08-fig06) 作為合理的關係圖。也就是說，事實上在 m5.2 中我們看到的結婚率(M)和離婚率(D) 之間的關係事實上是一個虛假的，帶有欺騙性的關係 (spurious)。


### 預測變量殘差圖 predictor residual plots 

預測殘差圖展示的是結果變量 (outcome) 和預測變量殘差 (residual predictor)。有助於理解模型本身。

在模型 m5.3 中，我們加入了兩個預測變量一個是結婚率 M，一個是結婚時年齡中位數 A。計算預測變量殘差，就是把其中一個預測變量做另一個預測變量的結果變量：

$$
\begin{aligned}
M_i & \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i & = \alpha + \beta A_i\\
\alpha & \sim \text{Normal}(0, 0.2)\\
\beta  & \sim \text{Normal}(0, 0.5) \\
\sigma & \sim \text{Exponential}(1) 
\end{aligned}
$$

```{r introBayes08-10, cache=TRUE}
m5.4 <- quap(
  alist(
    M ~ dnorm( mu, sigma ), 
    mu <- a + bAM * A, 
    a ~ dnorm(0, 0.2), 
    bAM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)
precis(m5.4)
```
之後，我們計算結婚率的殘差的方法就是取觀測到的結婚率和上述模型給出的預測結婚率：

```{r introBayes08-11, cache=TRUE}
mu <- link(m5.4)
mu_mean <- apply( mu, 2, mean )
mu_resid <- d$M - mu_mean
```

當一個州的結婚率殘差是正的，表示結婚率觀測值高於上述模型給出的已知該州結婚年齡中位數時的結婚率預測值。



```{r introBayes08-fig08, cache=TRUE,  fig.width=5.5, fig.height=4,  fig.cap="Plotting marriage rate residuals against the outcome (divorce rate). Residual variation in marriage rate shows little association with divorce rate.", fig.align='center'}
d$mu_res <- mu_resid
m5.4res <- quap(
  alist(
    D ~ dnorm( mu, sigma ), 
    mu <- a + bMres * mu_res, 
    a ~ dnorm( 0, 0.2 ), 
    bMres ~ dnorm( 0, 0.5 ), 
    sigma ~ dexp(1)
  ), data = d
)

# compute percentile interval of mean
M_seq <- seq(from = -1.8, to = 1.8, length.out = 30)
mu <- link(m5.4res, data = list(mu_res = M_seq))
mu.mean <- apply( mu, 2, mean )
mu.PI <- apply(mu, 2, PI)


# plot

plot( D ~ mu_res, data = d, col = rangi2, 
       xlab = "Marriage rate residuals", 
     ylab = "Divorce rate (std, per/1000 adults)")
lines( M_seq, mu.mean, lwd = 2)
shade( mu.PI, M_seq )
abline(v = 0, lty = 2)
```

我們使用相同的步驟，來繪製結婚時年齡中位數的預測殘差與結果變量之間的關係圖：


```{r introBayes08-fig09, cache=TRUE,  fig.width=5.5, fig.height=4,  fig.cap="Plotting age at marriage residuals against the outcome (divorce rate). Divorce rate on age at marriage residuals, showing remaining variation, and this variation is associated with divorce rate.", fig.align='center'}
# build the model A regress on M 
m5.41 <- quap(
  alist(
    A ~ dnorm( mu, sigma ), 
    mu <- a + bMA * M, 
    a ~ dnorm(0, 0.2), 
    bMA ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)

# calculate the predictor residual for A
mu <- link(m5.41)
mu_mean <- apply( mu, 2, mean )
mu_resid <- d$A - mu_mean

# build model D regress on A residuals
d$muA_res <- mu_resid
m5.4Ares <- quap(
  alist(
    D ~ dnorm( mu, sigma ), 
    mu <- a + bAres * muA_res, 
    a ~ dnorm( 0, 0.2 ), 
    bAres ~ dnorm( 0, 0.5 ), 
    sigma ~ dexp(1)
  ), data = d
)


# compute percentile interval of mean
M_seq <- seq(from = -1.8, to = 2.5, length.out = 30)
mu <- link(m5.4Ares, data = list(muA_res = M_seq))
mu.mean <- apply( mu, 2, mean )
mu.PI <- apply(mu, 2, PI)


# plot

plot( D ~ muA_res, data = d, col = rangi2, 
       xlab = "Age at marriage residuals", 
     ylab = "Divorce rate (std, per/1000 adults)")
lines( M_seq, mu.mean, lwd = 2)
shade( mu.PI, M_seq )
abline(v = 0, lty = 2)
```


### 事後分佈預測圖 posterior prediction plots


```{r introBayes08-12, cache=TRUE}
# call link without specifying new data
# so it uses original data 
mu <- link(m5.3)

# summarize samples across cases
mu_mean <- apply( mu, 2, mean )
mu_PI <- apply(mu, 2, PI)

# simulate observations
# again no new data, uses original data

D_sim <- sim(m5.3, n = 10000)
D_PI <- apply(D_sim, 2, PI)
```


繪製觀測值和預測值之間的散點圖：

```{r  introBayes08-fig10, cache=TRUE,  fig.width=5.5, fig.height=4.8,  fig.cap="Posterior predictive plot for the divorce model, m5.3. The horizontal axis is the observed divorce rate in each state. The vertical axis is the model's posterior predicted divorce rate, given each state's median age at marriage and marriage rate. The blue line segments are 89% compatibility intervals. The diagonal line shows where posterior predictions exactly match the sample.", fig.align='center'}
plot( mu_mean ~ d$D, col = rangi2, ylim = range(mu_PI), 
      xlab = "observed divorce rate", 
      ylab = "predicted divorce rate")

abline(a = 0, b = 1, lty = 2)
for (i  in 1 : nrow(d)) lines( rep(d$D[i], 2), mu_PI[, i], col = rangi2)
```

### 反現實圖 counterfactual plots


```{r introBayes08-13, cache=TRUE}
m5.3_A <- quap(
  alist(
    ## A -> D <- M
    D ~ dnorm(mu, sigma), 
    mu <- a + bM * M + bA * A, 
    a ~ dnorm(0, 0.2), 
    bM ~ dnorm(0, 0.5), 
    bA ~ dnorm(0, 0.5), 
    sigma ~ dexp(1), 
    ## A -> M 
    M ~ dnorm(mu_M, sigma_M),
    mu_M <- aM + bAM * A, 
    aM ~ dnorm(0, 0.2), 
    bAM ~ dnorm(0, 0.5), 
    sigma_M ~ dexp(1)
  ), data = d
)

precis(m5.3_A)
```

可以看見 `bAM` 是負的，-0.69, (-0.85， 0.54)。也就是說 A 和 M 之間是呈現強烈負相關的。我們來嘗試改變 A，看看會發生什麼。先定義一組A的數據：


```{r introBayes08-14, cache=TRUE}
A_seq <- seq(from = -2, to = 2, length.out = 30)
```

然後利用 `sim()` 函數來獲取 simulated 數據：

```{r introBayes08-15, cache=TRUE}
# prep data
sim_dat <- data.frame( A = A_seq)

# Simulate M and then D, using A_seq

s <- sim( m5.3_A, data = sim_dat, vars = c("M", "D"))
```


繪製 simulated 數據的圖：

```{r introBayes08-fig11, cache=TRUE,  fig.width=5.5, fig.height=4.8,  fig.cap="Visualize the predicted effect of manipulating age at marriage A on divorce rate D. (Total causal effect of A on D, A -> D, and A -> M -> D bot effects included.", fig.align='center'}
plot( sim_dat$A, colMeans(s$D), ylim = c(-2, 2), type = "l", 
      xlab = "Manipulated A", ylab = "counterfactual D")
shade(apply(s$D, 2, PI), sim_dat$A)
mtext("Total counterfactual effect of A on D")
```



```{r introBayes08-fig12, cache=TRUE,  fig.width=5.5, fig.height=4.8,  fig.cap="Visualize the predicted effect of manipulating age at marriage A on marriage rate M. (Simulated values of M show the estimated influence A -> M.", fig.align='center'}
plot( sim_dat$A, colMeans(s$M), ylim = c(-2, 2), type = "l", 
      xlab = "Manipulated A", ylab = "counterfactual M")
shade(apply(s$M, 2, PI), sim_dat$A)
mtext("Counterfactual A -> M")
```

我們之前已經知道當調整了 A 之後 M 對 D 的貢獻率幾乎可以忽略不計。所以你看到 \@ref(fig:introBayes08-fig11) 和 \@ref(fig:introBayes08-fig12) 相差不明顯。因為 A $\rightarrow$ M $\rightarrow$ D 中的第二個箭頭並無多大貢獻。

當然這樣的 simulation 也可以允許我們進行一些數據的總結運算。例如，把結婚年齡的中位數從 20 歲提高到 30 歲的話，帶來的因果效果 (causal effect) 可以這樣來計算；

```{r introBayes08-16, cache=TRUE}
mean(d$MedianAgeMarriage); sd(d$MedianAgeMarriage)
# new data frame, standardized to mean 26.1 and sd 1.24

sim2_dat <- data.frame( A = (c(20, 30 )- 26.1 ) / 1.24)
s2 <- sim(m5.3_A, data  = sim2_dat, vars = c("M", "D"))
mean(s2$D[ , 2] - s2$D[, 1])
```

標準化離婚率降低了 4.5 個標準差。這是相當大的變化。




值得注意的是，當我們人為的修改了某個預測變量 $X$，我們實際上打斷了其他所有變量對 $X$ 的影響。這相當於是我們把 DAG 圖修改成為一個無任何變量會影響 $X$ 的版本的 DAG：



```{r introBayes08-fig14, cache=TRUE, fig.width=2.8, fig.height=2,  fig.cap="Manipulating M means we break the arrow from A to M.", fig.align='center'}
# library(dagitty)
dag5.1.1 <- dagitty( "dag{ A -> D; M -> D }" )
coordinates(dag5.1.1) <- list( x=c(A=0,D=1,M=2) , y=c(A=0,D=1,M=0) )
drawdag( dag5.1.1 )
```

正如圖 \@ref(fig:introBayes08-fig14) 顯示的那樣，當我們人為修改控制了離婚率 M，那麼 A 結婚年齡中位數就不再對 M 有任何影響。



```{r introBayes08-fig13, cache=TRUE,  fig.width=5.5, fig.height=4.8,  fig.cap="The counterfactual effect of manipulating marriage rate M on divorce rate D. Since M -> D was estimated to be very small, there is no strong trend here. By manipulating M, we break the influence of A on M, and this removes the association between M and D.", fig.align='center'}
sim_dat <- data.frame( M = seq(from = -2 , to = 2, length.out = 30), A = 0)

s <- sim(m5.3_A, data = sim_dat, vars = "D")

plot(sim_dat$M, colMeans(s), ylim = c(-2, 2), type = "l", 
     xlab = "manipulated M", ylab = "counterfactual D")
shade(apply( s, 2, PI), sim_dat$M)
mtext( "Total counterfactual effect of M on D")
```

## 被掩蓋起來的關係

從上述結婚率離婚率的例子中我們學到了，多重線性回歸可以幫助我們找出虛假的關係。另外一個使用多重線性回歸的好處是，我們可以具體的把多個變量同時對一個結果變量的影響給呈現出來。當某兩個變量之間是相關的（correlated），但是其中的每一個和結果變量之間的關係可能是相反方向的時候，問題就很突出。我們來看一個評價奶品質的數據：


```{r introBayes08-17, cache=TRUE}
data(milk)
d <- milk
str(d)
```


讓我們重點來看 `kcal.per.g`（每克奶含有的卡路里），`mass`（平均母體體重, kg）和 `neocortex.perc`（大腦新皮質neocortex百分比）這三個變量。通常地認為，腦新皮質含量高的哺乳動物產的奶含有的能量卡路里會較高，主要是為了使之後代的大腦發育更加充分和快速。這裡使用的數據希望能夠進一步了解這樣的問題：“奶的能量含量，這裡是以千卡為單位測量的，和大腦新皮質百分比之間的關係有多大”。讓我們先把這三個變量標準化：

```{r introBayes08-18, cache=TRUE}
d$K <- standardize( d$kcal.per.g )
d$N <- standardize( d$neocortex.perc )
d$M <- standardize( log(d$mass) )
```

第一個能想到的模型是簡單的以新皮質百分比做預測變量，奶能量做結果變量的簡單線性回歸模型：

$$
\begin{aligned}
K_i & \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i & = \alpha + \beta_NN_i
\end{aligned}
$$

讓我們來試著使用二次方近似法獲取上述簡單線性回歸模型各個參數的事後樣本分佈：


```{r introBayes08-19, cache=TRUE, eval=FALSE}
m5.5_draft <- quap(
  alist(
    K ~ dnorm( mu, sigma ), 
    mu <- a + bN * N, 
    a ~ dnorm( 0, 1 ), 
    bN ~ dnorm( 0, 1 ), 
    sigma ~ dexp( 1 )
  ), data = d
)
```

你會看到你的計算機給出下面的警告：

```
Error in quap(alist(K ~ dnorm(mu, sigma), mu <- a + bN * N, a ~ dnorm(0,  : 
  initial value in 'vmmin' is not finite
The start values for the parameters were invalid. This could be caused by missing values (NA) in the data or by start values outside the parameter constraints. If there are no NA values in the data, try using explicit start values.
```

這主要是因為數據中的 N，是有缺失值的：


```{r introBayes08-20, cache=TRUE}
d$neocortex.perc
```

我們暫且先把這些缺失值無視掉：

```{r introBayes08-21, cache=TRUE}
dcc <- d[ complete.cases(d$K, d$N, d$M), ]
dcc
```

只剩下了 17 行的數據。我們把這個數據喂到之前 `m5.5_draft` 模型中去試試看：


```{r introBayes08-22, cache=TRUE}
m5.5_draft <- quap(
  alist(
    K ~ dnorm( mu, sigma ), 
    mu <- a + bN * N, 
    a ~ dnorm( 0, 1 ), 
    bN ~ dnorm( 0, 1 ), 
    sigma ~ dexp( 1 )
  ), data = dcc
)
```


繪製我們在 `m5.5_draft` 中設定的先驗概率分佈中的任意 50 條回歸直線看看我們的先驗概率分佈選擇是否合理：

```{r introBayes08-fig15, cache=TRUE,  fig.width=5.5, fig.height=4.8,  fig.cap="Prior predictive distribution for the first primate milk model. The vague first guess of prior choices. These priors are clearly silly.", fig.align='center'}
prior <- extract.prior( m5.5_draft )
xseq <- c(-2, 2)
mu <- link( m5.5_draft, post = prior, data = list(N = xseq))

plot(NULL, xlim = xseq, ylim = xseq, bty="n",
     xlab = "neocorrtex percent (std)", 
     ylab = "kilocal per g (std)", 
     main = "a ~ dnorm(0, 1) \nbN ~ dnorm(0, 1)")

for( i in 1:50){
  lines( xseq, mu[i, ], col = col.alpha("black", 0.3))
}
```

看圖 \@ref(fig:introBayes08-fig14) 給出的先驗分佈直線是不是有點荒謬。可見無信息的先驗概率分佈不一定是合理的先驗概率分佈。值得推薦的是，我們可以把截距控制在 0 附近，因為顯然當結果變量和預測變量都是標準化的數值之後，他們之間的關係在預測變量是零的事後，結果變量也應該在零附近才合理。另外，斜率的分佈也可以控制在使之盡可能不要出現一些極端情況的斜率：


```{r introBayes08-fig16, cache=TRUE,  fig.width=5.5, fig.height=4.8,  fig.cap="Prior predictive distribution for the first primate milk model. Slightly less silly priors that at least stay within the potential space of observations.", fig.align='center'}
m5.5 <- quap(
  alist(
    K ~ dnorm( mu, sigma ), 
    mu <- a + bN * N, 
    a ~ dnorm( 0, 0.2 ), 
    bN ~ dnorm( 0 , 0.5 ), 
    sigma ~ dexp(1)
  ), data = dcc
)


prior <- extract.prior( m5.5 )
xseq <- c(-2, 2)
mu <- link( m5.5, post = prior, data = list(N = xseq))

plot(NULL, xlim = xseq, ylim = xseq, bty="n",
     xlab = "neocorrtex percent (std)", 
     ylab = "kilocal per g (std)", 
     main = "a ~ dnorm(0, 0.2) \nbN ~ dnorm(0, 0.5)")

for( i in 1:50){
  lines( xseq, mu[i, ], col = col.alpha("black", 0.3))
}
```


實際 `m5.5` 給出的事後概率分佈估計是怎樣的呢：

```{r introBayes08-23, cache=TRUE}
precis(m5.5)
```

這個表格到底告訴我們什麼信息了呢？敏感的人應該看得出來，每個參數的事後概率分佈都很不精確，標準差相對均值都較大。
