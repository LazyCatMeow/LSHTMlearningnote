> The most newsworthy scientific studies are the least trustworthy. Maybe popular topics attract more and worse researchers, like flies drawn to the smell of honey?
> ~ Richard McElreath


Berkson's paradox, 又被叫做是選擇性扭曲現象（selection-distortion effect）。

```{r introBayes09-fig01, cache=TRUE, fig.width=6, fig.height=5,  fig.cap="Why the most newsworthy studies might be least trustworthy. 200 research proposals are ranked by combined trustworthiness and news worthiness. The top 10% are selected for funding. While there is no correlation before selection, the two criteria are strongly negatively correlated after selection. The correlation here is -0.77.", fig.align='center'}
set.seed(1914)
N <- 200 # num grant proposals
p <- 0.1 # proportion to select

# uncorrelated newsworthiness and trustworthiness
nw <- rnorm(N)
tw <- rnorm(N)

# select top 10 of combined scores
s <- nw + tw # total score
q <- quantile( s, 1-p ) # top 10% threshold

selected <- ifelse( s >= q, TRUE, FALSE) 
cor(tw[selected], nw[selected])
proposal <- data.frame(nw, tw, selected)
with(proposal, plot(nw[!selected], tw[!selected], col=c("black"),
                    xlim = c(-3,3.5), ylim = c(-3.5,3), 
                    bty="n", 
                    xlab = "newsworthiness", 
                    ylab = "trustworthiness"))
points(proposal$nw[proposal$selected], 
       proposal$tw[proposal$selected], 
       col = c("blue"), 
       pch = 16)
abline(lm(proposal$nw[proposal$selected] ~ proposal$tw[proposal$selected]), 
       lty = 2, lwd = 2, col = c("blue"))
text(1, -2.8, "rejected")
text(2, 2.5, "selected", col = c("blue"))
```

## 多重共線性問題 multicollinearity

多重共線性，通常當模型的預測變量之間有較強的相互關係的時候會出現。它造成的結果是你的模型給出的事後概率分佈會表現的似乎和任何一個預測變量之間都沒什麼關係，即便事實上其中的一個甚至幾個都可能和結果變量存在著相互依賴的關係。這樣的模型對於研究目的是使用模型來做預測的情形下沒有什麼本質的影響。

想像一下我們想使用一個人的腿長度來預測他/她的身高。你覺得模型中同時放入左右兩條腿的長度作為預測變量的話，事情會變成怎樣的呢？

下面的代碼是通過計算機模擬生成100個人的身高和腿長度。


```{r introBayes09-01, cache=TRUE}
N <- 100                        # number of individuals 
set.seed(909)
height <- rnorm(N, 10, 2)       # sim total height for each
leg_prop <- runif(N, 0.4, 0.5)  # leg as proportion of height 
leg_left <- leg_prop * height + # sim left leg as proportion + error
  rnorm( N, 0, 0.02 ) 
leg_right <- leg_prop * height + # sim right leg as proportion + error
  rnorm( N, 0, 0.02 )
                                  # combine into data frame
d <- data.frame(height, leg_left, leg_right)
head(d)
```

如果我們同時使用兩腿的長度作為預測身高的變量建立簡單線性回歸模型的話，我們會期待獲得怎樣的結果？從生成數據的過程我們已知平均地，腿長度佔身高的比例是45%。所以我們其實會期待腿長度的回歸係數應該在 $10/4.5 \approx 2.2$ 左右。但事實是怎樣呢？


```{r introBayes09-02, cache=TRUE}
m6.1 <- quap(
  alist(
    height ~ dnorm( mu, sigma ), 
    mu <- a + bl * leg_left + br * leg_right, 
    a ~ dnorm( 10, 100 ),
    bl ~ dnorm(2, 10), 
    br ~ dnorm(2, 10), 
    sigma ~ dexp( 1 )
  ), data = d
)
precis(m6.1)
```

左右腿的數據同時放到一個模型裡給出的結果似乎是令人困惑的。


```{r introBayes09-fig02, cache=TRUE, fig.width=6, fig.height=5,  fig.cap="If both legs have almost identical lengths, and height is so strongly associated with leg length, then why is this posterior distribution so weird?", fig.align='center'}
plot(precis(m6.1))
```

我們看模型 `m6.1` 給出的 `bl, br` 的事後聯合分佈 (joint posterior distribution)：

```{r introBayes09-fig03, cache=TRUE, fig.width=6, fig.height=5,  fig.cap="Posterior distribution of the association of each leg with hegiht, from model m6.1. Since both variables contain almost identical information, the posterior is a narrow ridge of negatively correlated values.", fig.align='center'}
post <- extract.samples(m6.1)
plot(bl ~ br, post, col = col.alpha(rangi2, 0.1),
     pch = 16)
```

如圖 \@ref(fig:introBayes09-fig03) 顯示的那樣，當 `bl` 很大時，`br` 就很小，反之亦然。


```{r introBayes09-fig04, cache=TRUE, fig.width=6, fig.height=5,  fig.cap="The posterior distribution of the sum of the two parameters is cnetered on the proper association of eight leg with height.", fig.align='center'}
sumblbr <- post$bl + post$br
dens(sumblbr, col = rangi2, lwd = 2, xlab = "sum of bl and br")
```


於是我們知道我們應該從模型中去掉其中一個腿的信息，從而獲得正確的模型和計算結果：


```{r  introBayes09-03, cache=TRUE}
m6.2 <- quap(
  alist(
    height ~ dnorm( mu, sigma ), 
    mu <- a + bl * leg_left, 
    a ~ dnorm( 10, 100 ), 
    bl ~ dnorm(2, 10), 
    sigma ~ dexp( 1 )
  ), data = d
)

precis(m6.2)
```


### 哺乳動物奶質量數據中的共線性

重新打開哺乳動物奶質量數據。我們看其中的含脂肪百分比和含乳糖百分比這兩個變量。把他們標準化：

```{r introBayes09-04, cache=TRUE}
data(milk)
d <- milk
d$K <- standardize( d$kcal.per.g )
d$F <- standardize( d$perc.fat )
d$L <- standardize( d$perc.lactose )
```

下面的模型使用標準化的脂肪百分比和乳糖百分比兩個變量作為預測變量來預測奶的能量密度：


```{r introBayes09-05, cache=TRUE}
# kcal.per.g regressed on perc.fat
m6.3 <- quap(
  alist(
    K ~ dnorm( mu, sigma ),
    mu <- a + bF * F, 
    a ~ dnorm( 0, 0.2 ), 
    bF ~ dnorm( 0, 0.5 ), 
    sigma ~ dexp(1)
  ), data = d
)

# kcal.per.g regressed on perc.lactose
m6.4 <- quap(
  alist(
    K ~ dnorm( mu, sigma ), 
    mu <- a + bL * L, 
    a ~ dnorm( 0, 0.2 ), 
    bL ~ dnorm( 0, 0.5 ), 
    sigma ~ dexp(1)
  ), data =  d
)


precis( m6.3 )
precis( m6.4 )
```

當單獨使用其中之一作為能量密度的預測變量時，我們發現他們各自的回歸係數似乎互相成鏡像數據，一個是正的，另一個是負的。而且二者的回歸係屬的事後概率分佈都很精確，我們認為這兩個單獨變量都是可以用來預測奶能量密度的極佳預測變量。因為脂肪百分比越高，能量密度越高，反之，乳糖含量比例越高，那麼能量密度則越低。我們來看把他們兩個同時加入模型中會發生什麼現象：

```{r introBayes09-06, cache=TRUE}
m6.5 <- quap(
  alist(
    K ~ dnorm( mu, sigma ), 
    mu <- a + bF * F + bL * L, 
    a ~ dnorm( 0, 0.2 ), 
    bF ~ dnorm( 0, 0.5 ), 
    bL ~ dnorm( 0, 0.5 ), 
    sigma ~ dexp( 1 )
  ), data = d
)
precis( m6.5 )
```

你看現在 `m6.5` 模型中同時加入了脂肪百分比，和乳糖百分比的兩個變量。都比單獨使用時給出的回歸係屬更接近 0。而且各自的事後概率分佈的標準差都比單獨使用時大了許多（幾乎兩倍）。這並非是來自計算機模擬的數據，而是真正現實中存在的奶製品測量之後的數據。脂肪百分比和乳糖百分比二者之間存在的很強的互相預測的關係。我們從他們的三點圖可以看出其中的奧妙：


```{r introBayes09-fig05, cache=TRUE, fig.width=6, fig.height=5,  fig.cap="A pairs plot of the total energy, percent fat, and percent lactose variables from the primate milk data. Percent fat and percent lactose are strongly negatively correlated with one another, providing mostly the same information. ", fig.align='center'}
pairs( ~ kcal.per.g + perc.fat + perc.lactose, data = d, 
       col = rangi2)
```


## 治療後偏倚 post-treatment bias

```{r introBayes09-07, cache=TRUE}
set.seed(71)
 # number of plants 
N <- 100

# simulate initial heights 
h0 <- rnorm(N, 10, 2)

# assign treatments and simulate fungus and growth
treatment <- rep(0:1, each = N/2)
fungus <- rbinom( N, size = 1, prob = 0.5 - treatment * 0.4)
h1 <- h0 + rnorm( N, 5 - 3*fungus )

# compose a clean data frame
d <- data.frame( h0 = h0, h1 = h1, treatment = treatment, fungus = fungus)
head(d)
precis(d)
```

### 設定模型

$$
\begin{aligned}
h_{1,i} & \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i & = h_{0,i} \times p
\end{aligned}
$$

其中，

- $h_{0,i}$ 是在時間 $t = 0$ 時的植物高度；
- $h_{1,i}$ 是在時間 $t = 1$ 時植物的高度；
- $p$ 是比例係數，也就是 $h_{1,i}$ 和 $h_{0,i}$ 之間的比值，$p = \frac{h_{1,i}}{h_{0,i}}$。如果 $p = 1$ 說明在時間 $t = 1$ 時植物並沒有比在時間 $t = 0$ 時有長高。

這裡我們對 $p$ 使用的先驗概率分佈，應該會集中在 1 的附近，因為無信息表示我們認為植物的高度不會隨時間發生變化。但是這個比例 $p$ 不能為負數。因為它是一個值和另一個值的比值。我們之前使用過相似特質的先驗概率分佈，也就是對數正（常）態分佈（Log-Normal distribution）：

$$
\beta \sim \text{Log-Normal}(0, 0.25)
$$

看看這個先驗概率分佈的密度曲線是什麼樣子：


```{r  introBayes09-fig06, cache=TRUE, fig.width=6, fig.height=5,  fig.cap="Distribution density funciton of Log-Normal(0,0.25)", fig.align='center'}
sim_p <- rlnorm(10000, 0, 0.25)
precis(sim_p)
dens( sim_p, xlim = c(0,3), adj = 0.1)
```


也就是說，我們給出的這個先驗概率分佈認為，植物在不同時間點之間的生長比例範圍在 0.67 和 1.49 之間，也就是要麼縮水33%，或者最多長高50%。


建立該模型：

```{r introBayes09-08, cache=TRUE}
m6.6 <- quap(
  alist(
    h1 ~ dnorm(mu, sigma), 
    mu <- h0 * p ,
    p ~ dlnorm( 0, 0.25 ),
    sigma ~ dexp(1)
  ), data  = d
)
precis(m6.6)
```

$p$ 的事後概率分佈均值是 1.43，也就是預估平均每單位時間植物會長高大約 40%。接下來如果加入另外兩個變量，治療組，和是否出現菌落。我們會把這兩個變量對植物施加的影響使用線性回歸模型的方式加到 $p$ 上去：


$$
\begin{aligned}
h_{1, i} & \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i  & = h_{0,i} \times p \\ 
p     & = \alpha + \beta_T T_i + \beta_F F_i \\
\alpha  & \sim \text{Log-Normal}(0, 0.25) \\
\beta_T & \sim \text{Normal}(0, 0.5) \\ 
\beta_F & \sim \text{Normal}(0, 0.5) \\ 
\sigma  & \sim \text{Exponential}(1)
\end{aligned}
$$

上述模型的R代碼如下：

```{r introBayes09-09, cache=TRUE}
m6.7 <- quap(
  alist(
    h1 ~ dnorm(mu, sigma), 
    mu <- h0 * p , 
    p <- a + bt * treatment + bf * fungus, 
    a ~ dlnorm( 0, 0.2 ),
    bt  ~ dnorm(0, 0.5), 
    bf  ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)
precis(m6.7)
```

這裡似乎在說，治療本身對植物生長速度並無效果，但是有菌落卻對生長比例造成了負影響。可是我們明明知道菌落是否存在，是取決於治療本身的，也就是菌落是治療對土壤造成的結果之一。上述模型似乎在告訴我們，當我們知道了治療造成的結果之一 -- 是否有菌落，那麼治療本身對植物生長比例的影響就消失了。正確的模型是，我們應該把菌落這個變量從模型中拿掉，從而尋找治療對植物生長率的效果：


```{r  introBayes09-10, cache=TRUE}
m6.8 <- quap(
  alist(
    h1 ~ dnorm(mu, sigma), 
    mu <- h0 * p, 
    p <- a + bt * treatment, 
    a ~ dlnorm(0, 0.25),
    bt ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)
precis(m6.8)
```

上述的分析過程和結果告訴我們，如果我們把由於治療本身造成的結果之一也錯誤地放進預測變量中的話，治療本身的效果會消失。

這些變量之間的關係還可以用下面的DAG圖來輔助理解：


```{r  introBayes09-fig07, cache=TRUE, fig.width=8, fig.height=1,  fig.cap="The DAG of the fungus and treatment effect on the grow of plant.", fig.align='center'}
plant_dag <- dagitty("dag{
      H_0 -> H_1 
      F -> H_1 
      T -> F
}")
coordinates( plant_dag ) <- list(x = c(H_0 = 0 , T = 2, F = 1.5, H_1 = 1), 
                                 y = c(H_0 = 0 , T = 0, F = 0, H_1 = 0))
drawdag(plant_dag)
```

如果我們錯誤地把 $F$ 也放入預測變量中去的話，就把實際治療變量的效果這條通路給堵住了。這在因果推斷中被叫做，由於控制了 F 變量，我們錯誤地在模型中引入了 D - separation。這裡的 D，指的是 directional（方向）。D-separation 在因果推斷中指的是，某個變量在DAG圖中和其他所有變量都獨立。在本例中， 由於控制了 $F$ 而導致從治療變量 $T$ 通往結果變量 $H_1$ 之間的的通路被阻斷了 ($T \rightarrow F \rightarrow H_1$)，使得 $H_1$ 和 $T$ 之間變得失去了依賴關係（相互獨立）。


事實上，錯誤地在預測變量中放入治療結果造成的結果不只是可能使我麼誤認為治療無效，也可能使我們誤認為原本無效的治療是有效的。看如下圖 \@ref(fig:introBayes09-fig08) 所提示的因果關係。它的涵義是，該治療土壤的方法確實導致了某些奇怪的菌落的生長，但是，我們種的那個植物並不會被菌落的生長所影響。但是假設有一個未知未測量的變量 "M"，它會同時影響植物和菌落的生長（例如空氣濕度）。這時如果我們建立一個簡單線型回歸模型來尋找治療 $T$ 和植物生長 $H_1$ 之間的關係的話，不小心加入了菌落這一變量會導致本來沒有關係的二者突然出現了治療效果一樣的聯繫。我們來試著模擬一下這個現象。


```{r  introBayes09-fig08, cache=TRUE, fig.width=8, fig.height=2,  fig.cap="The other DAG of the fungus and treatment effect on the grow of plant.", fig.align='center'}
# define our coordinates
dag_coords <-
  tibble(name = c("H0", "H1", "M", "F", "T"),
         x    = c(1, 2, 2.5, 3, 4),
         y    = c(2, 2, 1, 2, 2))

# save our DAG
dag <-
  dagify(F ~ M + T,
         H1 ~ H0 + M,
         coords = dag_coords)

# plot 
dag %>%
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(aes(color = name == "M"),
                 alpha = 1/2, size = 6.5, show.legend = F) +
  geom_point(x = 2.5, y = 1, 
             size = 6.5, shape = 1, stroke = 1, color = "orange") +
  geom_dag_text(color = "black") +
  geom_dag_edges() + 
  scale_color_manual(values = c("steelblue", "orange")) +
  theme_dag()
```

```{r introBayes09-11, cache=TRUE}
set.seed(71)
N <- 1000
h0 <- rnorm(N, 10, 2)
treatment <- rep(0:1, each = N/2)
M <- rbern(N)
fungus <- rbinom( N, size = 1, prob = 0.5 - treatment * 0.5 + 0.4 * M)
h1 <- h0 + rnorm( N, 5 + 3 * M)
d2 <- data.frame( h0 = h0, h1 = h1, treatment = treatment, fungus = fungus)
precis(d2)
# incorrectly included fugus 
m6.7 <- quap(
  alist(
    h1 ~ dnorm(mu, sigma), 
    mu <- h0 * p , 
    p <- a + bt * treatment + bf * fungus, 
    a ~ dlnorm( 0, 0.2 ),
    bt  ~ dnorm(0, 0.5), 
    bf  ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d2
)
precis(m6.7)

# the correct model to see the treatment effect 
m6.8 <- quap(
  alist(
    h1 ~ dnorm(mu, sigma), 
    mu <- h0 * p, 
    p <- a + bt * treatment, 
    a ~ dlnorm(0, 0.25),
    bt ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d2
)
precis(m6.8)
```

此時你發現加入 `fungus` 變量依然對正確的對斷造成了干擾。使得本來不應該出現的治療效果似乎突然成了有效的促進植物生長的治療。

## 對撞因子偏倚 collider bias

使用本章節開頭的申請研究經費的例子，我們認為研究的可靠性 (Trustworthiness, T)，和新穎程度 (Newsworthiness, N) 之間是無關聯性的（圖 \@ref(fig:introBayes09-fig01)）。但是，他們二者都會對是否該科研項目被選中 (Selected, S) 造成影響。這樣的關係可以使用下面的 DAG 來表達：

```{r  introBayes09-fig09, cache=TRUE, fig.width=8, fig.height=1,  fig.cap="The DAG of the grant selection problem: two unrelated variables (T and N) influence S, a collider example.", fig.align='center'}
grant_dag <- dagitty("dag{
      T -> S
      N -> S
}")
coordinates( grant_dag ) <- list(x = c(T = 0.5, S = 1, N = 1.5), 
                                 y = c(T = 0, S = 0, N = 0))
drawdag(grant_dag)
```


對撞因子偏倚的現象很有趣，當上述模型中加入對撞因子 S，就會在統計學上給出影響該對撞因子的變量之間的錯誤的關聯性，這裡就是本不該有關聯的 N 和 T 之間會出現統計學上的關聯性。因為，從邏輯上來說，當你知道了某個項目被選中了，也就是圖 \@ref(fig:introBayes09-fig01) 中藍色的部分，那麼本來不相關的兩個變量之間就存在了互相可以預測的掛係，即，如果此時你又對該科研項目的可信度或者是新穎度之一有所了解的話，你就可以大致猜測它的新穎度或者是可信度。也就是說，在這些被選中接受科研經費贊助的藍色項目中，如果你知道某項目的新穎程度很高很高，那麼你大概可以認為它給出的科研成果的可信度會比較低。同樣的，如果你知道某個科研項目並不是特別新穎的內容，但是它既然被選中了，這就說明該項目本身將會給出的科研成果會是十分令人信服的。
