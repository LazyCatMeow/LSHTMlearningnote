# (PART) 等級線性迴歸模型 analysis of hierarchical and other dependent data {-}


# 相互依賴數據及簡單的應對方案 {#Hierarchical}

> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
>
> ~ Sir Ronald Aylmer Fisher


```{block2, note-thankslinda, type='rmdnote'}
The Analysis of Hierarchical and Other Dependent Data lectures were orgainised and taught by Professor [Linda Sharples](https://www.lshtm.ac.uk/aboutus/people/sharples.linda), and Dr. [Edmund Njeru Njagi](https://www.lshtm.ac.uk/aboutus/people/njagi.edmund-njeru).
```



## 相互依賴的數據

線性回歸模型，廣義線性回歸模型，他們背後都有一個十分十分**十分重要的假設--數據的相互獨立性**。這個前提假設常常會在現實數據中得不到滿足，因爲數據與數據之間在背後很可能會有有所關聯，也許是已知的，也許是未知的因素讓某些數據顯得更加接近彼此。這個章節，主要的內容就是舉例說明分層數據在日常生活中的常見性，以及處理這個非獨立性質的必要性。

- 圖 \@ref(fig:Hier01-1) 展示的箱式圖顯示的是六個不同醫院對各自 12 名患者收縮期血壓測量的結果。如果把醫院看做一個單位，取院內患者的平均值，那麼六所醫院的血壓均值最大爲 135.7 mmHg，最小是 117.7 mmHg，六所醫院測量的血壓總體均值爲 125.6 mmHg。

```{r Hier01-1, echo=FALSE, fig.height=6, fig.width=7, fig.cap='Box and whiskers plot of measured SBP in patients from six hospitals', fig.align='center', out.width='90%', cache=TRUE}
Bp <- read_dta("backupfiles/bp.dta")

Bp$hosp <- as.factor(Bp$hosp)
with(Bp, boxplot(bp ~ hosp, xlab = "Hospital No.", ylab = "Systolic blood pressure (mmHg)"))
```


- 圖 \@ref(fig:Hier01-2) 展示的是對 17 名患者使用兩種不同的測量方法測量的最大呼吸速率 (peak-expiratory-flow rate, PEFR)。兩種方法又測量了兩次，途中展示的是其中一種測量方法前後兩次測量結果的散點圖。



```{r Hier01-2, cache=TRUE, echo=FALSE, fig.height=6, fig.width=9, fig.cap='Two recordings of PEFR taken with the Mini Wright meter', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}

pefr <- read_dta("backupfiles/pefr.dta")
# the data are in wide format


# transform data into long format
pefr_long <- pefr %>%
  gather(key, value, -id) %>%
  separate(key, into = c("measurement", "occasion"), sep = 2) %>%
  arrange(id, occasion) %>%
  spread(measurement, value)
## figure shows slightly closer agreement between the repeated measures of standard Wright,
## than between those of Mini Wright

ggplot(pefr_long, aes(x = id, y = wm, fill = occasion)) +
  geom_point(size = 4, shape = 21) +
  geom_hline(yintercept = mean(pefr_long$wm), colour = "red") +
  theme_bw() +
  scale_x_continuous(breaks = 1:17)+
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "Subject ID", y = "MW Measurements")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8),
        axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank()) + theme(legend.position = "bottom", legend.direction = "horizontal") + theme(legend.text = element_text(size = 19), 
  legend.title = element_text(size = 19))
```

- 圖 \@ref(fig:Hier01-3) 展示的來自全英 65 所學校的 4059 名學生入學前閱讀水平測試成績 (LRT) 和畢業時 GCSE 考試成績之間的散點圖關系。值得注意的是該圖其實無視了學校這個變量，把每個學生看成相互獨立的個體。但是當我們隨機選取四所學校，看它們各自的學生的成績表現 (圖 \@ref(fig:Hier01-4))。很顯然，之前忽視了學校這一層級的變量是不恰當的，因爲不同學校學生的入學前和畢業時成績之間的相關性很明顯存在不同的模式 (四所學校的回歸線各自的截距和斜率各不相同)。

```{r Hier01-3, cache=TRUE, echo=FALSE, fig.height=6, fig.width=9, fig.cap='GCSE by LRT in all 65 schools', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
gcse_selected <- read_dta("backupfiles/gcse_selected.dta")
ggthemr::ggthemr('fresh')

ggplot(gcse_selected, aes(x = lrt, y = gcse)) + geom_point(size = 2.5) + 
  geom_smooth(method = "lm", se = FALSE) +
  xlim(-40, 40) +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "LRT", y = "GCSE score")
```

```{r Hier01-4, cache=TRUE, echo=FALSE, fig.height=6, fig.width=9, fig.cap='GCSE by LRT in four randomly selected schools', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
ggthemr('fresh')

p <- ggplot(gcse_selected[gcse_selected$school %in% c(2, 7, 40, 53), ], aes(x = lrt, gcse)) + 
  geom_point(size = 2.5) +
    geom_smooth(method = "lm", se = FALSE) +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "LRT", y = "GCSE score")
p + facet_wrap( ~ school, ncol=2)+ 
  theme(strip.text = element_text(face = "bold", size = rel(1.5)))
```


- 另一個特別好的例子展示在圖 \@ref(fig:Hier01-5) 中，是關於同一個母親的不同孩子的出生體重的數據。一個母親可以有多個孩子，每個母親的孩子之間的出生體重很明顯無法看作相互獨立。圖中展示的是，3300 名生了兩個孩子的母親的孩子們出生體重的散點圖。同一個母親的小孩用線相連。顯然，同一個母親生的孩子，其出生體重比不同母親的孩子出生體重差距更小，更接近彼此，因爲他們來自同一個母親。可以想象，一個母親如果身材高大，那麼她的孩子們可能都傾向於有比較高的出生體重。所以同一個母親的孩子之間體重是有相關關系的 (within correlation)。


```{r Hier01-5, cache=TRUE, echo=FALSE, fig.height=6, fig.width=9, fig.cap='Birthweight of siblings by maternal identifier', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
siblings <- read_dta("backupfiles/siblings.dta")
subSiblings <- siblings[(siblings$momid > 80000)&(siblings$momid < 95000)&(siblings$idx %in% c(1,2)), ]
subSiblings$idx <- factor(subSiblings$idx)
subSiblings_w <- subset(subSiblings, select = c("momid", "idx", "birwt"))

subSiblings_w <- spread(subSiblings_w, key = idx, value = birwt)
ggthemr('fresh')


ggplot(subSiblings, aes(x = momid, y = birwt, fill = idx)) + 
 geom_point(size = 4, shape = 21) + 
#geom_segment(aes(x = momid, y = `1`, xend = momid, yend = `2`, colour = "segment"), data = subSiblings_w) + 
 geom_line(aes(group = momid), lty = 1) + 
   theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "Mother ID", y = "Birthweight (g)")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8),
        axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank()) + 
  theme(legend.position = "bottom", legend.direction = "horizontal") + theme(legend.text = element_text(size = 19),   legend.title = element_text(size = 19)) + theme(plot.subtitle = element_text(vjust = 1), 
  plot.caption = element_text(vjust = 1)) +labs(fill = "Child number") + 
  scale_fill_discrete(labels = c("1st Child", "2nd Child"))
```


- 最後一個用於本章節的實例是，一項研究亞洲兒童生長狀況的調查分別記錄了 198 個數據點，68 個兒童在 0 到 3 歲之間的四個年齡點的體重數據。圖 \@ref(fig:Hier01-6) 展示的就是這個典型的隨訪數據的個人生長曲線。且圖中每個人的生長軌跡提示，男孩子的生長過程可能相互之間體重差異顯得較女孩子來得大。如果，我們用每個兒童自己的數據，給每個兒童擬合各自的回歸線，數據顯然不足，但是如果我們決定忽略個體的生長的隨機效應 (不均一性)，又顯得十分不妥當。

```{r  Hier01-6, cache=TRUE, echo=FALSE, fig.height=6, fig.width=9, fig.cap='Growth profiles of boys and girls in the Asian growth data', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
growth <- read_dta("backupfiles/asian.dta")
growth <- growth %>%
  mutate(gender = factor(gender, labels= c("Boys", "Girls")))

ggthemr('fresh')

G <- ggplot(growth, aes(x = age, y = weight)) + 
 geom_line(aes(group = id), lty = 1) + 
   theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "Age (years)", y = "Weight (Kg)")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8),
        axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank()) + 
  theme(legend.position = "bottom", legend.direction = "horizontal") + theme(legend.text = element_text(size = 19),   legend.title = element_text(size = 19)) + theme(plot.subtitle = element_text(vjust = 1), 
  plot.caption = element_text(vjust = 1)) +labs(fill = "Child number") + 
  scale_fill_discrete(labels = c("1st Child", "2nd Child"))
G + facet_grid(. ~ gender) + 
  theme(strip.text = element_text(face = "bold", size = rel(1.5)))
```




## 依賴性的來源在哪裏


上述例子中的數據，均提示我們數據與數據之間獨立性的假設，常常會遇到尷尬的局面。因爲數據與數據之間本身就不可能完全獨立。

1. 同一個診所或者醫院的患者，他們之間可能有着某些相似的因素從而導致他們的血壓相比其他醫院的人彼此更加接近。這個原因可能是有同一家醫院的患者可能有類似的疾病。
2. 同一患者身上反復抽取樣本，也就是說一個對象貢獻了多個數據的時候，這些來自同一對象的數據當然具有相對不同對象數據更高的均質性。
3. 同一所學校的學生的成績或內部的相關性，很可能大於不同學校兩個學生之間成績的相關性。因爲同一學校的孩子可能共享某些共同的特徵，比如說相似的家庭經濟背景，或者是同樣的教學內容教學老師等環境因素。這樣，來自同一所學校的孩子的成績很可能就會更加相似。
4. 至於說家庭數據就更加典型了。來自同一家庭的兄弟姐妹，有着極強的相關性，因爲他們共享着遺傳因素，或者是相似的家庭教育/飲食/生活習慣等環境因素。
5. 同一個體身上的縱向 (時間) 隨訪數據很顯然會比不同患者有更強的內部相關性。

目前位置介紹的這些常見實例中，可以發現它們有一個共通點。就是這些數據其實內部是有分層結構的 (hierarchy)。這些數據中，都有一個最底層單元 (elementary units/level 1)，還有一個聚合單元 (aggregate units/level 2)，聚合單元常被命名爲層級 (cluster)。


```{r Hier01tab00, echo=FALSE, cache=TRUE, eval=TRUE}
dt <- read.csv("backupfiles/Hier01tab00.csv", header = T)
#names(dt) <- c("Model with", "sigma_u", "sigma_e", "sigma_u", "sigma_e")
kable(dt, "html",  align = "l", caption = "Hierarchy in the data (5 examples in Chapter 1)") %>%
  kable_styling(bootstrap_options = c("striped", "bordered"),full_width = F, position = "center") %>%
  add_header_above(c("Level" = 2))
```

正如表格 \@ref(tab:Hier01tab00) 總結的那樣，這些數據中存在這層級結構，這種數據被稱爲分層數據 (hierarchical)，或者叫嵌套式數據 (nested data)。根據你所在的知識領域，它可能還被命名爲多層結構數據 (multilevel and clustered data)。在一些研究中，你可能會遇見從實驗設計階段就存在分層結構的數據，比如使用分層抽樣 (multistage sampling) 的設計的實驗等。這樣的實驗設計最常在人口學，社會學的研究中看到。在大多數醫學研究中，每個數據點 (observation point, level 1)，所屬的層 (cluster) 本身可能是我們感興趣的研究點 (例如同屬於一個家庭，相同母親的後代)，又或者是同一個人/患者的隨着時間推移的隨訪健康狀態 (如生長曲線，體重變化，疾病康復情況)。

如果用前面用過的 圖 \@ref(fig:Hier01-6) 的生長曲線做例子，那麼每個被調查的兒童，就是該數據的第二級層，每個隨訪時刻測量的體重數據，則是觀察的數據點。這個數據還有一個特點是，觀察數據點是有前後的 (時間) 順序的，這是一個典型的**縱向研究數據 (longitudinal data)**。


## 數據有依賴性導致的結果

如果你手頭的數據，結構上是一種嵌套式結構數據，那麼任何無視了這一點作出的統計學推斷都是有瑕疵的。相互之間互不獨立這一特質，需要通過一種新的手段，把嵌套式的數據結構考慮進統計學模型裏來。

在一些情況下，數據的嵌套式結構可能可以被忽略掉，但是其結果是導致統計學的估計變得十分低效 (inefficient procedure)。你可能會聽說過一般化估計公式 (generalized estimating equations)，是其中一種備擇手段，因爲在這一公式中，你需要人爲地指定數據與數據之間可能的依賴關系是怎樣的。

其實，即使有人真的在分析過程中忽略了數據本身的嵌套式結構，他會發現最終在描述分析結果的時候，還是無法避免這一嚴重的問題。另外一些統計學家可能記得在穩健統計學法中，三明治標準誤估計法也是可以供選擇的一種處理相關數據的手段。

## 邊際模型和條件模型 marginal and conditional models

邊際模型和條件模型的概念其實不是分層模型特有的，卻在分析分層數據模型時十分有用。假如，對於某個結果變量 $Y$ 有它如下的回歸模型，其中我們把某個單一的共變量 $Z$ 從模型中分離出來，加以特別關注:

$$
g\{ \text{E}(Y|\textbf{X},Z) \} = \beta\textbf{X} +\gamma Z
$$

這是一個典型的條件模型，它描述了結果變量 $Y$ 的期望是以怎樣的**條件**和解釋變量 $\textbf{X},Z$ 之間建立關系的。每個解釋變量的回歸系數，其含義都是**以其他同一模型中的共變量不變的條件下**，和結果變量之間的關系。經過這樣的解讀，你會知道，其實本統計學教程目前爲止遇見過的所有的回歸模型都是條件模型。如果此時我們反過來思考，把上述模型中單獨分離出來的單一共變量 $Z$ 對於結果變量 $Y$ 均值的影響合並起來 (對共變量 $Z$ 積分即可)，此時我們得到的就是共變量 $\textbf{X}$ 和結果變量 $Y$ 之間，關於 $Z$ 的邊際模型 (Marginal model):

$$
\text{E}_Z\{ \text{E}(Y|\textbf{X}, Z) \} = \text{E}_Z\{ g^{-1}(\beta\textbf{X} + \gamma Z) \} \\
\text{Where } \text{E}(Z) = 0
$$


用**線性回歸**來舉例:

$$
\text{E}(Y| \textbf{X}, Z) = \beta\textbf{X} + \gamma Z
$$


那麼此時共變量 $\textbf{X}$ 的邊際模型回歸系數 $\beta$ 的含義，和條件模型時的回歸系數其實是相同的含義:

$$
\text{E}_Z\{\text{E}(Y|\textbf{X},Z)\} = \text{E}_Z(\beta\textbf{X} + \gamma Z) = \beta\textbf{X} + \gamma\text{E}(Z) = \beta\textbf{X}
$$


爲什麼這裏的邊際模型對於分層數據來說很重要呢？答案在於，嵌套式數據中，我們常常關心那第二個階層 (重復測量某個指標的患者，學生成績數據中的學校層級，等) 在它所在的那個階層中和結果變量之間的平均關系。(In models for hierarchical data we often use level effects to represent what is common among observations from one "cluster" or "group". We may then want marginal conclusions: we need to average over these effects).


```{r hier01tab01, echo=FALSE, cache=TRUE, eval=FALSE}
dt <- read.csv("backupfiles/hierexample.csv", header = T)
names(dt) <- c("Cluster (j)", "id (i)", "X", "Y", "X", "Y")
kable(dt, "html",  align = "c", caption = "Example data") %>%
  kable_styling(bootstrap_options = c("striped", "bordered"),full_width = F, position = "center") #%>%
  #add_header_above(c(" " = 1,"REML" = 2, "ML" = 2))
```

### 標記法 notation

- $Y_{ij}$ 標記第 $j$ 層的第 $i$ 個個體;
- $i = 1, \cdots, n_j$ 表示第 $j$ 層中共有 $n_j$ 個個體 (elements);
- $j = 1, \cdots, J$ 表示數據共有 $J$ 個第二階層 (clusters);
- $N = \sum_{j=1}^J n_j$ 表示總體樣本量等於各個階層樣本量之和;
- 特殊情況: 如果每個階層的個體數相同 $n$，$N=nJ$，這樣的數據被叫做均衡數據 (balanced data)。

### 合並每個階層 

過去常見的總結嵌套式數據的手段只是把每層數據取平均值，這樣的方法簡單粗暴但是偶爾是可以接受的，只要你能夠接受如此處理數據可能帶來的如下後果:

- 各層數據均值，其可靠程度 (方差) 隨着各層的樣本量不同而不同 (depends on the number of elementary units per cluster);
- 變量的含義發生改變。如果是使用層水平 (cluster level) 的數據，本來測量給個體的那些變量，就變成了**層的變量**，從此作出的任何統計學推斷，只能限制在層水平 (ecological fallacy, as correlations at the macro level cannot be used to make assertions at the micro level);
- 由於無視了層內個體數據，導致大量信息損失。

此處我們借用 [@Snijders1999] 書中第 28 頁的人造數據，如下表


<table class="table table-striped table-bordered" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>Artificial data</caption>
 <thead>
  <tr>
   <th style="text-align:center;"> Cluster $(j)$ </th>
   <th style="text-align:center;"> id $(i)$ </th>
   <th style="text-align:center;"> $X$ </th>
   <th style="text-align:center;"> $Y$ </th>
   <th style="text-align:center;"> $\bar{X}$ </th>
   <th style="text-align:center;"> $\bar{Y}$ </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> 1 </td>
   <td style="text-align:center;"> 1 </td>
   <td style="text-align:center;"> 1 </td>
   <td style="text-align:center;"> 5 </td>
   <td style="text-align:center;"> 2 </td>
   <td style="text-align:center;"> 6 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 1 </td>
   <td style="text-align:center;"> 2 </td>
   <td style="text-align:center;"> 3 </td>
   <td style="text-align:center;"> 7 </td>
   <td style="text-align:center;"> 2 </td>
   <td style="text-align:center;"> 6 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 2 </td>
   <td style="text-align:center;"> 1 </td>
   <td style="text-align:center;"> 2 </td>
   <td style="text-align:center;"> 4 </td>
   <td style="text-align:center;"> 3 </td>
   <td style="text-align:center;"> 5 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 2 </td>
   <td style="text-align:center;"> 2 </td>
   <td style="text-align:center;"> 4 </td>
   <td style="text-align:center;"> 6 </td>
   <td style="text-align:center;"> 3 </td>
   <td style="text-align:center;"> 5 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 3 </td>
   <td style="text-align:center;"> 1 </td>
   <td style="text-align:center;"> 3 </td>
   <td style="text-align:center;"> 3 </td>
   <td style="text-align:center;"> 4 </td>
   <td style="text-align:center;"> 4 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 3 </td>
   <td style="text-align:center;"> 2 </td>
   <td style="text-align:center;"> 5 </td>
   <td style="text-align:center;"> 5 </td>
   <td style="text-align:center;"> 4 </td>
   <td style="text-align:center;"> 4 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 4 </td>
   <td style="text-align:center;"> 1 </td>
   <td style="text-align:center;"> 4 </td>
   <td style="text-align:center;"> 2 </td>
   <td style="text-align:center;"> 5 </td>
   <td style="text-align:center;"> 3 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 4 </td>
   <td style="text-align:center;"> 2 </td>
   <td style="text-align:center;"> 6 </td>
   <td style="text-align:center;"> 4 </td>
   <td style="text-align:center;"> 5 </td>
   <td style="text-align:center;"> 3 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 5 </td>
   <td style="text-align:center;"> 1 </td>
   <td style="text-align:center;"> 5 </td>
   <td style="text-align:center;"> 1 </td>
   <td style="text-align:center;"> 6 </td>
   <td style="text-align:center;"> 2 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 5 </td>
   <td style="text-align:center;"> 2 </td>
   <td style="text-align:center;"> 7 </td>
   <td style="text-align:center;"> 3 </td>
   <td style="text-align:center;"> 6 </td>
   <td style="text-align:center;"> 2 </td>
  </tr>
</tbody>
</table>

這個表中的人造數據，其結構是一目了然的，它的第二層級數量是 5，每層的個體數量是 2。這是一個平衡數據。由於這是個我們人爲模擬的數據，圖 \@ref(fig:artificialdata00) 也顯示它沒有隨機誤差，所有數據都在各自的直線上。


```{r artificialdata00, cache=TRUE, echo=TRUE, fig.asp=.7, fig.width=6, fig.cap='Artificial data: scatter of clustered data', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}

dt <- read.csv("backupfiles/hierexample.csv", header = T)
names(dt) <- c("Cluster", "id", "X", "Y", "Xbar", "Ybar")
dt$Cluster <- as.factor(dt$Cluster)
ggthemr('fresh')

ggplot(dt, aes(x = X, y = Y, shape = Cluster, colour = Cluster)) + geom_point(size =5) +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "X", y = "Y")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8)) + 
theme(legend.position = "bottom", legend.direction = "horizontal") + theme(legend.text = element_text(size = 19), legend.title = element_text(size = 19))
```



- 如果我們無視其分層數據的嵌套式結構，把每個數據都看作是獨立的樣本，擬合一個**整體回歸 (total regression) 圖 \@ref(fig:artificialdata01)**:

$$
\hat Y_{ij} = 5.33 - 0.33 X_{ij}
$$


```{r artificialdata01, cache=TRUE, echo=TRUE, fig.asp=.7, fig.width=6, fig.cap='Artificial data: Total regression', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
ggthemr('fresh')

ggplot(dt, aes(x = X, y = Y)) + geom_point(size = 5, shape = 23) + 
  geom_smooth(method = "lm", se = FALSE, linetype = 2) +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "X", y = "Y")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8)) + 
theme(legend.position = "bottom", legend.direction = "horizontal") + theme(legend.text = element_text(size = 19), legend.title = element_text(size = 19))
```


- 如果我們只保留層級數據本身，求了變量 $X,Y$ 在每層的均值的話，就得到了**層間回歸 (between regression) 圖 \@ref(fig:artificialdata02)** -- 變量 $X,Y$ 之間的回歸直線的斜率變得更大了:

$$
\hat{\bar{Y}}_j = 8.0 - 1.0 \bar{X}_j
$$


```{r artificialdata02, cache=TRUE, echo=TRUE, fig.asp=.7, fig.width=6, fig.cap='Artificial data: scatter of clustered data', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
ggthemr('fresh')

ggplot(dt, aes(x = X, y = Y)) + geom_point(size =5, shape=23) + 
    geom_smooth(method = "lm", se = FALSE, linetype = 2) +
  geom_abline(intercept = 8, slope = -1) + 
  geom_point(aes(x = Xbar, y=Ybar, size = 5)) +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "X", y = "Y")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8)) + theme(legend.position="none")
```


### 生物學悖論 ecological fallacy

生物學悖論常見於我們認爲某分層數據中層級變量之間的關系，同樣適用與層級中的個體之間: 例如比較 A 國 和 B 國之間心血管疾病的發病率，發現 A 國國民食鹽平均攝入量高於 B 國，很多人可能就會下結論說食鹽攝入量高的個體，心血管疾病發病的危險度較高。然而，這樣的推論很多時候是錯誤的。

曾經在 [@Robinson1950] 論文中舉過的著名例子: 該研究調查美國每個州的移民比例，和該州相應的識字率之間的關系。研究者發現，移民比例較高的州，其識字率也較高 (相關系數 0.53)。由此就有人下結論說移民越多，那個州的教育水平會比較高。但是實際情況是，把每個個體的受教育水平和該個體本身是不是移民做了相關系數分析之後發現，這個關系其實是負相關 (-0.11)。所以說在州的水平作出的統計學推斷-移民多的州受教育水平高-是不正確的。之所以在州水平發現移民比例和受教育水平之間的正關系，是因爲移民傾向於居住在教育水平本來就比較高的本土出生美國人的州。

### 分解層級數據

如果是分析最初層級數據 (level 1) 的話，我們還需要考慮下列一些問題: 

- 當心數據被多次利用 

如果我們關心的變量其實是在第二層級的 (level 2/cluster level)，但是你卻把它當作是第一層級的數據，就會引起**數據很多**的錯覺，因爲同一層的個體他們的層屬變量都是一樣的，你擁有的數據其實並沒有你想的那麼多。

前文中用過的 GCSE 數據其實是一個很好的例子，下表中歸納了調查的學校類型 (男校，女校或者混合校)，以及按照每個學生個人所屬學校類型的總結，可以看出，當你嘗試使用個人 (elementary level) 水平的數據分析實際上是第二層級數據的特性時，你會被誤導。因爲個人數據告訴你， 34% 的學生在女校學習，然而正確的分析法應該是，學校中有 31% 的學校是女校。


```{r hier01tab02, echo=FALSE, cache=TRUE, eval=FALSE}
dt <- read.csv("backupfiles/hier01tab01.csv", header = T)
#names(dt) <- c("Model with", "sigma_u", "sigma_e", "sigma_u", "sigma_e")
kable(dt, "html",  align = "c", caption = "Aggregated and disaggregated") %>%
  kable_styling(bootstrap_options = c("striped", "bordered"),full_width = F, position = "center") %>%
  add_header_above(c("School type" = 1,"Cluster Level" = 2, "Elementary Level" = 2))
```

<table class="table table-striped table-bordered" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>Aggregated and disaggregated</caption>
 <thead>
<tr>
<th style="text-align:center; border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;" colspan="1"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px;">School type</div></th>
<th style="text-align:center; border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;" colspan="2"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px;">Cluster Level</div></th>
<th style="text-align:center; border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;" colspan="2"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px;">Elementary Level</div></th>
</tr>
  <tr>
   <th style="text-align:center;">  </th>
   <th style="text-align:center;"> N </th>
   <th style="text-align:center;"> % </th>
   <th style="text-align:center;"> N </th>
   <th style="text-align:center;"> % </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> mixed </td>
   <td style="text-align:center;"> 35 </td>
   <td style="text-align:center;"> 54 </td>
   <td style="text-align:center;"> 2169 </td>
   <td style="text-align:center;"> 53 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> boys only </td>
   <td style="text-align:center;"> 10 </td>
   <td style="text-align:center;"> 15 </td>
   <td style="text-align:center;"> 513 </td>
   <td style="text-align:center;"> 13 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> girls only </td>
   <td style="text-align:center;"> 20 </td>
   <td style="text-align:center;"> 31 </td>
   <td style="text-align:center;"> 1377 </td>
   <td style="text-align:center;"> 34 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> Total </td>
   <td style="text-align:center;"> 65 </td>
   <td style="text-align:center;"> 100 </td>
   <td style="text-align:center;"> 4059 </td>
   <td style="text-align:center;"> 100 </td>
  </tr>
</tbody>
</table>



- 分層數據分析法

有人會說，既然如此，那麼我們就把數據放在每層當中分析就好了 (stratified analyses)。還是用前文中用過的人造 5 層數據來說明這樣做的弊端。前面用了兩種方法 (total regression, between regression) 來總結這個 5 層的人造數據 \@ref(fig:artificialdata02)。最後一種分析此數據的方法是，把 5 層數據分開分別做回歸線如圖 \@ref(fig:artificialdata03)。等同於我們的對數據擬合五次下面的回歸方程: 

$$
\hat Y_{ij} - \bar{Y}_j = \beta(X_{ij} - \bar{X}_j)
$$

這種模型被叫做**層內回歸 (within regression)**。這 5 個線性回歸的斜率都是 1，是五條不同截距的平行直線。因爲我們自己編造的數據的緣故，現實數據不太可能恰好所有層內回歸的斜率都是完全相同的。這其實也是曾內回歸法的一個默認前提 -- 每層數據中解釋變量和結果變量之間的關系是相同的。




```{r artificialdata03, cache=TRUE, echo=FALSE, fig.asp=.7, fig.width=6, fig.cap='Artificial data: within cluster regressions', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
ggthemr('fresh')

ggplot(dt, aes(x = X, y = Y)) + geom_point(size =5, shape=20) +
  geom_line(aes(group = Cluster), lty = 2) + 
  #  geom_smooth(method = "lm", se = FALSE, linetype = 2) +
 #  geom_abline(intercept = 8, slope = -1) + 
 # geom_point(aes(x = Xbar, y=Ybar, size = 5)) +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "X", y = "Y")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8)) + theme(legend.position="none")
```



### 固定效應模型 fixed effect model

無論數據中的分層結構是否有現實意義 (如果說是五種不同的民族，那就有顯著的現實意義)，在回歸模型中都**有必要考慮這個分層結構對數據的變異的貢獻** (the contribution of the clusters to the data variation)。

線性回歸章節中我們使用的是五個啞變量來代表不同組別加以分析: 

$$
Y_{ij} = \alpha_1 I_{i, j = 1} + \alpha_2 I_{i, j=2} + \cdots + \alpha_5 I_{i, j=5} + \beta_1X_{ij} + \varepsilon_{ij}
$$

其中 $j$ 是所屬層級編號。該模型中的 $\varepsilon_{ij}$ 被認爲服從均值爲零，方差爲 $\sigma_{\varepsilon}^2$ 的正態分布。該模型也可以簡寫爲:

$$
Y_{ij} = \alpha_j + \beta_1X_{ij} + \epsilon_{ij}
$$
一樣一預案
這樣的模型在等級線性回歸模型中被認爲是**固定效應模型 fixed effect model**。它其實是默認給五個層級五個不同的截距，每層內部 $X,Y$ 之間的關系 (斜率) 則被認爲是完全相同的 (namely the within cluster models are the same)。

本課剛開始的例子中有個數據是來自 6 所不同醫院 72 名患者的收縮期血壓的數據。我們現在來分析這些人中血壓和年齡之間的關系。下面的散點圖重現了六所醫院的72名患者的血壓和年齡。



```{r Hier01-7, echo=FALSE, fig.height=6, fig.width=7, fig.cap='SBP versus age: different symbols identify the six hospitals', fig.align='center', out.width='90%', cache=TRUE}
Bp <- read_dta("backupfiles/bp.dta")

Bp$hosp <- as.factor(Bp$hosp)

ggthemr('fresh')

ggplot(Bp, aes(x = age, y = bp, shape = hosp)) + geom_point(size =5) + 
    #geom_smooth(method = "lm", se = FALSE, linetype = 2) +
  #geom_abline(intercept = 8, slope = -1) + 
  #geom_point(aes(x = Xbar, y=Ybar, size = 5)) +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "Age", y = "Systolic blood pressure (mmHg)")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8)) + theme(legend.position="bottom")
```

下面在 R 裏擬合這個固定效應模型:

```{r Hier01-8, echo=TRUE, cache=TRUE}
Bp <- read_dta("backupfiles/bp.dta")

Bp$hosp <- as.factor(Bp$hosp)
Bp <- Bp %>%
  mutate(c_age = age - mean(age))
# 通過指定截距爲零，獲取每個醫院的回歸線的截距
Model0 <- lm(bp ~ 0 +  c_age + hosp, data = Bp) 

summary(Model0)

# 先生成一個新的醫院變量 hops1 = 1。然後使用偏 F 檢驗法
# 檢驗控制了患者的年齡以後，這六所醫院的截距是否各自不相同。
Bp$hosp1 <- Bp$hosp[1]
mod2 <- lm(bp ~ 0 +  c_age + as.numeric(hosp1), data = Bp)
anova(Model0, mod2)
```

偏 F 檢驗法給出的結果 $F(5, 65) = 2.16, P = 0.07$，所以說，數據其實告訴我們，調整了年齡之後，這六所醫院患者中年齡和血壓之間關系的回歸線有不同的截距。

## 簡單線性迴歸複習

滾回線性回歸章節 \@ref(lm)。

## 練習題

### 數據

1. High-School-and-Beyond 數據 <br> 本數據來自1982年美國國家教育統計中心 (National Center for Education Statistics, NCES) 對美國公立學校和天主教會學校的一項普查。曾經在 Hierarchical Linear Model [@Raudenbush2002] 一書中作爲範例使用。其數據的變量名和各自含義如下：

```
minority           indicatory of student ethinicity (1 = minority, 0 = other)
female             pupil's gender
ses                standardized socio-economic status score
mathach            measure of mathematics achievement
size               school's total number of pupils
sector             school's sector: 1 = catholic, 0 = not catholic
schoolid           school identifier
```

2. PEFR 數據 <br> 數據本身是 17 名研究對象用兩種不同的測量方法測量兩次每個人的最大呼氣流速 (peak-expiratory-flow rate, PEFR)。最早在1986年的柳葉刀雜誌發表 [@Bland1986]。兩種測量法的名稱分別是 "Standard Wright" 和 "Mini Wright" peak flow meter。變量名和個字含義如下：


```
id                 participant identifier
wp1                standard wright measure at 1st occasion
wp2                standard wright measure at 2nd occasion
wm1                mini wright measure at 1st occasion
wm2                mini wright measure at 2nd occasion
```

### 問題

### 將 High-School-and-Beyond 數據導入 R 中，熟悉數據結構及內容，特別要注意觀察每個學校的學生特徵。


```{r hierex1-1, cache=TRUE}
hsb_selected <- read_dta("backupfiles/hsb_selected.dta")
length(unique(hsb_selected$schoolid)) ## number of school = 160
## create a subset data with only the first observation of each school
hsb <- hsb_selected[!duplicated(hsb_selected$schoolid), ]

## about 44 % of the schools are Catholic schools
with(hsb, tab1(sector, graph = FALSE, decimal = 2))

## among all the pupils, about 53% are females
with(hsb_selected, tab1(female, graph = FALSE, decimal = 2))

## among all the pupils, about 27.5% are from ethnic minorities
with(hsb_selected, tab1(minority, graph = FALSE, decimal = 2))
```

### 爲了簡便起見，接下來的分析只節選數據中前五所學校 188 名學生的數學成績，和 SES。分別計算每所學校的數學成績,及 SES 的平均值。


```{r hierex1-2, cache=TRUE}
hsb5 <- subset(hsb_selected, schoolid < 1320)
Mean_ses_math <- ddply(hsb5,~schoolid,summarise,mean_ses=mean(ses),mean_math=mean(mathach))
## the mean SES score ranges from -0.4255 to +0.5280
## the mean Maths score ranges from 7.636 to 16.255
Mean_ses_math
```

### 先無視掉學校這一分層變量，把所有學生看作是相互獨立的，擬合總體的 SES 和數學成績的線性迴歸 **(Total regression model)**。把該總體模型的預測值提取並存儲在數據庫中。

```{r mathses, cache=TRUE, echo=TRUE, fig.asp=.7, fig.width=8, fig.cap='Scatter plot of SES and math achievements among all pupils from first 5 schools, assuming that they are all independent', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
## plot the scatter of mathach and ses among these 5 schools

ggplot(hsb5, aes(x = ses, y = mathach)) + geom_point() +
  theme_bw() +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "SES", y = "Math achievement")  +
  xlim(-2.05, 2.05)+
  ylim(-10, 30) +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8))
```


```{r hierex1-3, cache = TRUE}
Total_reg <- lm(mathach ~ ses, data = hsb5)
## the total regression model gives an estimated regression coefficient for the SES
## of each pupil equal to 3.31 (SE=0.66)
summary(Total_reg)
hsb5$Pred_T <- Total_reg$fitted.values # save the fitted values to the dataset
```

### 用各個學校 SES 和數學成績的均值擬合一個學校間的線性迴歸模型 **(between regression model)**。

```{r hierex1-4, cache=TRUE}
Btw_reg <- lm(mean_math ~ mean_ses, data = Mean_ses_math)
## the regression model for the school level variables (between model) gives
## an estimated regression coefficient of 7.29 (SE=1.41)
summary(Btw_reg)
Mean_ses_math$Pred_B <- Btw_reg$fitted.values # save the fitted values to the dataset
```

### 分別對每個學校內的學生進行 SES 和數學成績擬合線性迴歸模型。

```{r hierex1-5, cache=TRUE}
Within_schl1 <- lm(mathach ~ ses, data = hsb5[hsb5$schoolid == 1224,])
Within_schl2 <- lm(mathach ~ ses, data = hsb5[hsb5$schoolid == 1288,])
Within_schl3 <- lm(mathach ~ ses, data = hsb5[hsb5$schoolid == 1296,])
Within_schl4 <- lm(mathach ~ ses, data = hsb5[hsb5$schoolid == 1308,])
Within_schl5 <- lm(mathach ~ ses, data = hsb5[hsb5$schoolid == 1317,])
# the within school regressions gives estimated slopes which have a mean of 1.65
# and which ranges between 0.126 and 3.255
summary(c(Within_schl1$coefficients[2], Within_schl2$coefficients[2],
      Within_schl3$coefficients[2], Within_schl4$coefficients[2],
      Within_schl5$coefficients[2]))

# the SEs ranging between 1.21 and 3.00
summary(c(summary(Within_schl1)$coefficients[4],
          summary(Within_schl2)$coefficients[4],
          summary(Within_schl3)$coefficients[4],
          summary(Within_schl4)$coefficients[4],
          summary(Within_schl5)$coefficients[4]))

hsb5$Pred_W <- c(Within_schl1$fitted.values, Within_schl2$fitted.values,
      Within_schl3$fitted.values, Within_schl4$fitted.values,
      Within_schl5$fitted.values) ## save the predicted value into the dataset
```

### 比較三種模型計算的數學成績的擬合值，他們一致？還是有所不同？爲什麼會有不同？

- 總體模型 (Total regression model) 實際上無視了學生的性別，種族等可能帶來的混雜效果；
- 學校間模型 (Between model) 估計的實際上是**SES均值**每增加一個單位，與之對應的**數學平均成績**的改變量，**這個模型絕對不可用與評估個人的 SES 與數學成績之間的關係**；
- 學校內模型 (Within model) 擬合的 SES 與數學成績之間的關係變得十分地不精確 (SEs are fairly large)，變化幅度也很大。


### 把三種模型的數學成績擬合值散點圖繪製在同一張圖內。


```{r mathses-3models, cache=TRUE, echo=TRUE, fig.height=6.5, fig.width=8, fig.cap='High-school-and-beyond data: Predicted values by Total, Between, and Within regression models', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}

Mean <- Mean_ses_math[, 1:3]
names(Mean) <- c("schoolid", "ses", "Pred_W")


ggplot(hsb5, aes(x = ses, y = Pred_W, group = schoolid)) +
  geom_line(linetype = 2, size = 1) +
  geom_abline(intercept = Total_reg$coefficients[1], slope = Total_reg$coefficients[2],
               colour = "dark blue") +
  geom_abline(intercept = Btw_reg$coefficients[1], slope = Btw_reg$coefficients[2],
               colour = "red") +
  geom_point(data = Mean, shape = 17, size = 4, colour = "Red") +
  theme_bw() +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "SES", y = "Fitted regression lines (Maths achievement)")  +
  xlim(-2.05, 2.05)+
  ylim(5, 20) +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8)) + 
  theme(plot.caption = element_text(size = 12,
  hjust = 0)) + labs(caption = "Black dash line: Within regression model;
Blue solid line: Total regression model;
Red solid line: Between regression model;
Red triangle: School mean values")
```

### 用這 5 個學校的數據擬合一個固定效應線性迴歸模型


```{r hierex1-8, cache=TRUE}
Fixed_reg <- lm(mathach ~ ses + factor(schoolid), data = hsb5)

## Fitting a fixed effect model to these data is equivalent to forcing
## a common slope onto the five within regression models. It gives an
## estimated slope of 1.789 (SE=0.76), close to their average of 1.64799.
## Note that controlling for female, minority, and sector but not for
## schoolid leads to roughly the same estimate (slope = 1.68, SE=0.75)

summary(Fixed_reg)
summary(lm(mathach ~ ses + female + minority + sector, data = hsb5))
```

### 讀入 PEFR 數據。

```{r hierex1-9, cache=TRUE}
pefr <- read_dta("backupfiles/pefr.dta")
# the data are in wide format
pefr

# transform data into long format
pefr_long <- pefr %>%
  gather(key, value, -id) %>%
  separate(key, into = c("measurement", "occasion"), sep = 2) %>%
  arrange(id, occasion) %>%
  spread(measurement, value)
pefr_long
```

```{r tworecordings, cache=TRUE, echo=TRUE, fig.height=6, fig.width=9, fig.cap='Two recordings of PEFR taken with the standard Wright meter', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}

## figure shows slightly closer agreement between the repeated measures of standard Wright,
## than between those of Mini Wright

ggplot(pefr_long, aes(x = id, y = wp, fill = occasion)) +
  geom_point(size = 4, shape = 21) +
  geom_hline(yintercept = mean(pefr_long$wp), colour = "red") +
  theme_bw() +
  scale_x_continuous(breaks = 1:17)+
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "Subject ID", y = "W Measurements")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8))+ 
  theme(legend.text = element_text(size = 19), 
  legend.title = element_text(size = 19))
```

### 求每個患者的 `wp` 兩次測量平均值

```{r hierex1-11, cache=TRUE}
# the means range from 171.5 to 644.5
pefr_long %>% 
  group_by(id) %>% 
  summarise(mean_wp = mean(wp))
```

### 在 R 裏先用 ANOVA 分析個人的 `wp` 變異。再用 `lme4::lmer` 擬合用 `id` 作隨機效應的混合效應模型。確認後者報告的 `Std.Dev for id effect` 其實可以用 ANOVA 結果的 $\sqrt{\frac{\text{MMS-MSE}}{n}}$ (n 是每個個體重複測量值的個數)。

```{r hierex1-12, cache=TRUE, message=FALSE}
with(pefr_long, anova(lm(wp~factor(id))))

#library(lme4)
( fit <- lmer(wp ~ (1|id), data=pefr_long) )

sqrt((27600 - 234)/2)
```

### 擬合結果變量爲 `wp`，解釋變量爲 `id` 的簡單線性迴歸模型。用數學表達式描述這個模型。


```{r hierex1-13, cache=TRUE, message=FALSE}
Reg <- lm(wp ~ factor(id), data = pefr_long)

# The fixed effect regression model leads to the same ANOVA
# table. To the same estimate of the residual SD = (15.307)
# However, it does not give an estimate of the "SD of id effect"
# Instead it gives estimates of mean PEFR for participant number 1
# = 492 and estimates of the difference in means from him/her
# for all the other 16 pariticipants
anova(Reg)
summary(Reg)
```

上面的模型用數學表達式來描述就是：

$$
\begin{aligned}
Y_{ij} & = \alpha_1 + \delta_i + \varepsilon_{ij} \\
\text{Where } \delta_j & = \alpha_j - \alpha_1 \\
\text{and } \delta_1   & = 0
\end{aligned}
$$

### 將 `wp` 中心化之後，重新擬合相同的模型，把截距去除掉。寫下這個模型的數學表達式。


```{r hierex1-14, cache=TRUE, message=FALSE}
Reg1 <- lm((wp - mean(wp)) ~ 0 + factor(id), data = pefr_long)

# it leads to the same ANOVA table again, same residual SD
anova(Reg1)
summary(Reg1)
```


上面的模型用數學表達式來描述就是：

$$
\begin{aligned}
Y_{ij} - \mu & = \gamma_j + \varepsilon_{ij} \\
      Y_{ij} & = \mu +  \gamma_j + \varepsilon_{ij} \\
\text{Where } \mu & \text{ is the overall mean} \\
\text{and } \sum_{j=1}^J\gamma_j & = 0\\
\end{aligned}
$$

### 計算這些迴歸係數 (其實是不同羣之間的隨機截距) 的均值和標準差。


```{r hierex1-15, cache=TRUE, message=FALSE}
# the individual level intercepts have mean zero and SD = 117.47, larger than the estimated
# Std.Dev for id effect.
Reg1$coefficients

mean(Reg1$coefficients)
sd(Reg1$coefficients)
```


# 隨機截距模型 random intercept model

最簡單的隨機效應模型 -- 隨機截距模型 random intercept model。

## 隨機截距模型的定義

有時，我們對每個分層各自的截距大小並不那麼感興趣，且如果只有固定效應的話，其實我們從某種程度上忽略掉了數據層與層之間變異的方差 (between cluster variation)。於是，在模型中考慮這些問題的解決方案就是 -- 我們讓各層的截距呈現隨機效應 (treat the variation in cluster intercepts not as fixed)，**把這些截距視爲來自與某種分布的隨機呈現 (randomly draws from some distribution)**。於是原先的只有固定效應部分的模型，就增加了隨機截距部分: 

$$
\begin{aligned}
Y_{ij} & = \mu + u_j + \varepsilon_{ij} \\
\text{where } u_j  & \stackrel{N.I.D}{\sim} N(0, \sigma_u^2) \\
              \varepsilon_{ij} & \stackrel{N.I.D}{\sim} N(0, \sigma_\varepsilon^2) \\
        u_j  & \text{ are independent from } \varepsilon_{ij} \\
\end{aligned}
(\#eq:hier2-1)
$$

這個混合效應模型中，

- $\mu$ 是總體均值; 
- $u_j$ 是一個服從均值 0, 方差 (the population between cluster variance) 爲 $\sigma_u^2$ 的正態分布的隨機變量; 
- $\varepsilon_{ij}$ 是隨機誤差，它也被認爲服從均值爲 0, 方差爲 $\sigma_\varepsilon^2$ 的正太分布，且這兩個隨機效應部分之間也是**相互獨立的**。
- 從該模型估算的結果變量 $Y_{ij}$ 的方差是 $\sigma_u^2 + \sigma_\varepsilon^2$。
- 隨機截距模型又被叫做是 **方差成分模型 (variance-component model)**，或者是**單向隨機效應方差模型 (one-way random effects ANOVA model)**。

這個模型和僅有固定效應的模型，有顯著的不同: 

$$
Y_{ij} = \mu + \gamma_j + \varepsilon_{ij}
$$

固定效應模型裏，

- $\mu$ 也是總體均值; 
- $\sum_{j=1}^J \gamma_j = 0$ 是**將各組不同截距之和強制爲零**的過程;

所以隨機截距模型打破了這個限制，使得隨機的截距 $\mu_j$ 成爲一個服從均值爲 0，方差爲 $\sigma_u^2$ 的 **隨機變量**。

隨機效應部分 $u_j$ 和隨機誤差 $\varepsilon_{ij}$ 之間相互獨立的前提，意味着兩個裏屬於不同層級的觀察之間是相互獨立的，但是反過來，同屬於一個層級的個體之間就變成了有相關性的了 (within cluster correlation): 

$$
\begin{aligned}
\because Y_{1j} & = \mu + u_j + \varepsilon_{1j} \\
         Y_{2j} & = \mu + u_j + \varepsilon_{2j}  \\
\therefore \text{Cov}(Y_{1j}, Y_{2j}) & =  \text{Cov}(u_j, u_j) + \text{Cov}(u_j, \varepsilon_{2j}) + \text{Cov}(\varepsilon_{1j}, u_j) + \text{Cov}(\varepsilon_{1j}, \varepsilon_{2j}) \\
                                      & = \text{Cov}(u_j, u_j) = V(u_j, u_j)\\
                                      & = \sigma_u^2
\end{aligned}
$$

由於 $\text{Var}(Y_{1j}) = \text{Var}(Y_{2j}) = \sigma_u^2 + \sigma_\varepsilon^2$，所以，同屬一層的兩個個體之間的**層內相關系數 (intra-class correlation)**: 

$$
\lambda = \frac{\text{Cov}(Y_{1j}, Y_{2j})}{\text{SD}(Y_{1j})\text{SD}(Y_{2j})} = \frac{\sigma_u^2}{\sigma_\varepsilon^2 + \sigma_u^2}
$$

從層內相關系數的公式也可看出，該相關系數可以同時被理解爲結果變量 $Y_{ij}$ 的方差中歸咎與層(cluster)結構的部分的百分比。

This is the within-cluster or intra-class correlation, that we will denote $\lambda$. Note that it is also the proportion of total variance that is accounted for by the cluster.

## 隨機截距模型的參數估計

如此，我們就知道在隨機截距模型裏，有三個需要被估計的參數 $\mu, \sigma_u^2, \sigma^2_\varepsilon$。我們可以利用熟悉的極大似然估計法估計這些參數 (Maximum Likelihood, ML)。當且進當嵌套式結構數據是**平衡數據 (balanced)**時 (即，每層中的個體數量相同)，這三個參數的 $\text{MLE}$ 分別是: 

$$
\begin{aligned}
\hat\mu & = \bar{Y} \\
\hat\sigma_\varepsilon^2 & = \text{Mean square error, MSE} \\ 
\hat\sigma_u^2 & = \frac{\text{Model Sum of Squares, MSS}}{Jn} - \frac{\hat\sigma^2_\varepsilon}{n}
\end{aligned}
(\#eq:hier02-2)
$$

只要模型指定正確無誤，前兩個極大似然估計是他們各自的無偏估計。但第三個，也就是層內方差的估計量確實際上是低估了的 (downward biased)。這裏常用的另一種對層內方差參數的估計法被叫做**矩估計量 (moment estimator, or ANOVA estimator)**: 

$$
\begin{aligned}
\widetilde{\sigma}_u^2 & = \frac{\text{MSS}}{(J-1)n}- \frac{\hat\sigma_\varepsilon^2}{n} \\ 
                       & = \frac{\text{MSS} - \text{MSE}(J-1)}{(J-1)n} \\
                       & = \frac{\text{MMS}(J-1) - \text{MSE}(J-1)}{(J-1)n} \\
                       & = \frac{\text{MMS} - \text{MSE}}{n}
\end{aligned}
$$

對於平衡數據 (balanced data)，這個矩估計量又被叫做**限制性極大似然 (Restricted Maximum Likelihood, REML)**。限制性極大似然法，是一個真極大似然過程 (genuine maximum likelihood procedure)，但是它每次進行估計的時候，會先"去除掉"固定效應部分，所以每次用於估計參數的數據其實是對數據的線性轉換後 $Y_{ij} - \mu = u_j + \varepsilon_{ij}$，它使用的數據是這個等式右半部分的轉換後數據。在 REML 過程中，先估計層內方差 $\sigma_u^2$ 再對固定效應部分的總體均值估計，所以是個兩步走的過程。另外除了這裏討論的 ML, REML這兩種對層內方差進行參數估計的方法之外，在計量經濟學 (econometrics) 中常用的是 (本課不深入探討) **廣義最小二乘法 (Generalized Least Squares, GLS)**。GLS 使用的是一種加權的最小二乘法 (OLS)，該加權法根據層與隨機誤差的方差成分 (variance components) 不同而給不同的層以不同的截距權重。當數據本身是平衡數據時，GLS給出的估計結果等同於 REML法。當數據不是平衡數據的時候，ML/REML 其實背後使用的原理也是 GLS。


## 如何在 R 中進行隨機截距模型的擬合

在 R 或 STATA 中擬合隨機截距模型，需要數據爲“長 (long)” 數據，下面的代碼可以在 R 裏面把 “寬 (wide)” 的數據調整成爲 **長** 數據:

```{r Hier02-1, cache=TRUE, echo=TRUE,  message=FALSE, warning=FALSE}

pefr <- read_dta("backupfiles/pefr.dta")
# the data are in wide format
head(pefr)


# transform data into long format
pefr_long <- pefr %>%
  gather(key, value, -id) %>%
  separate(key, into = c("measurement", "occasion"), sep = 2) %>%
  arrange(id, occasion) %>%
  spread(measurement, value)
pefr_long
```

在 R 裏面，有兩個包 (`lme4::lmer` 或 `nlme::lme`)  的各自兩種代碼以供選用:

```{r Hier02-2, cache=TRUE, echo=TRUE,  message=FALSE, warning=FALSE}
M0 <- lme(fixed = wm ~ 1, random  = ~ 1 | id, data = pefr_long, method = "REML")
summary(M0)

M1 <- lmer(wm ~ (1|id), data = pefr_long, REML = TRUE)
summary(M1)
```

不知道爲什麼在 R 裏有這兩種完全不同的方式來擬合混合效應模型。還好他們的結果基本完全一致。在這個極爲簡單的例子裏，我們可以利用模型擬合的結果中 `Random effects` 的部分來計算**層內相關系數 (intra-class correlation)**: 

$$
\hat\lambda = \frac{\hat\sigma_u^2}{(\hat\sigma_u^2 + \hat\sigma_\varepsilon^2)} = \frac{110.40^2}{110.40^2 + 19.91^2} = 0.97
$$

這是對 Mini Wright meter 測量方法可靠性的一個評價指標。其中 $\sigma_u^2$ 是患者最大呼吸速率 (PEFR) 測量值的方差，$\sigma_\varepsilon^2$ 是測量的隨機誤差，所以這裏的測量方法的可靠度是 97%，是可信度十分高的測量準確度。

## 隨機截距模型中的統計推斷

### 固定效應部分的推斷 {#fixed-inference}

當數據是平衡數據時，固定效應的 $\mu$ 的 $\text{MLE}$ 是總體的均值 (overall mean)。它的估計標準誤是: 

$$
\hat{\text{SE}}(\hat\mu) = \sqrt{\frac{n\hat\sigma_u^2 + \hat\sigma_\varepsilon^2}{Jn}}
$$

記得線性回歸中(固定效應模型中)，$\mu$ 的 $\text{MLE}$ 也還是總體的均值 (overall mean)。它的估計標準誤卻是: 

$$
\hat{\text{SE}}(\hat\mu^F) = \sqrt{\frac{\hat\sigma_\varepsilon^2}{Jn}}
$$

所以，僅有固定效應模型時的總體均值的標準誤總是要比混合效應模型下估計的總體均值標準誤要小

$$
\hat{\text{SE}}(\hat\mu^F) < \hat{\text{SE}}(\hat\mu)
$$

如果數據不是平衡數據，那麼隨機截距模型中 $\mu$ 的 $\text{MLE}$ 是每層均值的加權均值 (a weighted mean of the cluster specific means):

$$
\begin{aligned}
\hat\mu & = \frac{\sum_jw_j\bar{Y}_{\cdot j}}{\sum_j w_j} \\
\text{Where } w_j & = \frac{1}{\sigma_u^2 + \sigma_\varepsilon^2/n_j}
\end{aligned}
$$

從加權的方式來看，如果樣本量少的層級數據本身的誤差方差 $\sigma_\varepsilon^2$ 也較小，那麼層樣本量較小的層也會和層樣本量較大的層獲得相似的均值權重。

零假設是 $\mu = 0$ 的檢驗，就計算 $z$ 檢驗統計量就可以 (或者 $z^2$ 的 Wald 檢驗): 

$$
z = \frac{\hat\mu}{\hat{\text{SE}}(\hat\mu)}
$$
總體均值的 95% 信賴區間的計算式就是: 

$$
\hat\mu \pm z_{0.975}\hat{\text{SE}}(\hat\mu)
$$

### 隨機效應部分的推斷

總體均值的假設檢驗搞定了之後，我們肯定還想對隨機截距模型擬合的隨機效應方差作出是否有意義的假設檢驗。也就是我們希望能檢驗零假設 $\sigma_u^2 = 0$，和替代假設 $\sigma_u^2 > 0$。一般情況下大家肯定會想到對含有隨機效應的模型和只有固定效應的模型使用 LRT (似然比檢驗)，然後把檢驗統計量拿去和自由度爲 1 的卡方分布做比較。但是其實方差本身永遠都是大於等於零的，所以傳統的 LRT 在這個零假設時並不適用。

在零假設條件下 $\sigma_u^2 = 0$，也就是說層內相關在一半的數據中是正相關，另一半數據中是正好相反的負相關，以此相互抵消，方差爲零。所以其實這裏的 LRT 檢驗統計量應該服從的不是自由度爲 1 的卡方分布那麼簡單，而是一種混合卡方分布 (自由度 1 和 自由度爲 0 的混合卡方分布 $\chi_{0,1}^2$)。所以應該把模型比較之後計算獲得的 $p$ 值除以2，以獲得準確的對 $\sigma_u^2 = 0$ 檢驗的 $p$ 值。

```{r Hier02-3, cache=TRUE, echo=TRUE,  message=FALSE, warning=FALSE}
M0 <- lme(fixed = wm ~ 1, random  = ~ 1 | id, data = pefr_long, method = "REML")
M0_fixed<- lm(wm ~ 1, data = pefr_long)
anova(M0, M0_fixed)
```

回到本例中的混合效應模型和固定效應模型的比較來看，LRT本身的 P 值已經 $<0.0001$，所以除不除以二對推斷結果都沒有太大影響。也就是本例中的隨即截距模型是比固定效應的簡單線性回歸模型更加適合該數據的模型。

其他注意點: 

- 在坑爹的 STATA 裏面混合效應模型居然還會輸出隨機效應方差的 "標準誤"，該數字請你無視之。
- 當樣本擁有足夠多的樣本量 (其實是第二階層的層數)，極大似然法 (ML) 和限制性極大似然法 (REML) 給出的結果會相當接近。
- 當你比較兩個不是互爲嵌套 (nested) 的模型時，可以使用 AIC/BIC 指標。


## 練習題

### 數據

1. GHQ 數據 <br> 該數據包含 12 名學生前後兩次回答 General Health Questionnaire (GHQ) 問卷獲得的數據。該問卷用於測量學生的心理壓力，其變量名和含義如下：


```
id        Student identifier
GHQ1      General Health Questionnaire score- 1st occasion
GHQ2      General Health Questionnaire score- 2nd occasion
```

2. Siblings 數據 <br> 該數據是來自一項對 3978 名媽媽關於她們 8604 名孩子的出生體重及健康狀況的問卷調查。該數據的變量名和含義如下：


```
momid     Mother identifier
idx       Baby identifier
mage      Maternal age (years)
meduc     Maternal education
gestat    gestational age (weeks)
birwt     Birth weight (g)
smoke     Maternal smoking (0 = no, 1 = yes)
male      Baby boy (0 = no, 1 = yes)
year      Year of birth
married   Maternal marital status (0 = no, 1 = yes)
hsgrad    Maternal high school education (0 = no, 1 = yes)
black     Maternal race (1 = black, 0 = other)
```


### 讀入 GHQ 數據，探索其內容，該數據是否是平衡數據 (balanced)？計算每名學生的兩次問卷成績平均分。

```{r hierex2-1, cache=TRUE, message=FALSE}
ghq <- read_dta("backupfiles/ghq.dta")
ghq

ghq <- ghq %>%
  mutate(mean = (GHQ1 + GHQ2)/2)

# each student has 2 observations (i.e. n_j = n = 2)
# and therefore the data are balanced.
# the overall mean is 10.167 and its SD is 6.073
ghq %>% 
  summarise(OverallMean = mean(mean), SD = sd(mean))
```

### 把數據從寬 (wide) 改變成長 (long) 的形式


```{r hierex2-2, cache=TRUE, message=FALSE}

# transform data into long format
ghq_long <- ghq %>%
  gather(key, value, -id, -mean) %>%
  separate(key, into = c("measurement", "occasion"), sep = 3) %>%
  arrange(id, occasion) %>%
  spread(measurement, value)
ghq_long

# after reshaping there are 24 records. the summary statistics are
# overall mean sd and min max

ghq_long %>% 
  summarise(OverallMean = mean(GHQ), SD = sd(GHQ), Min = min(GHQ), Max = max(GHQ))

# between groups mean sd and min
summ(ghq_long[!duplicated(ghq_long$id), ]$mean, graph = FALSE)

ghq_long %>% 
  distinct(id, .keep_all= TRUE) %>% 
  summarise(Bet_mean = mean(mean), Bet_sd = sd(mean), Bet_min = min(mean), Bet_max = max(mean))

# within groups mean sd and min (came from the difference between
# the overall mean and the within difference) observations for
# each group = 2
ghq_long <- ghq_long %>%
  mutate(dif_GHQ = mean(GHQ) - (GHQ - mean))

ghq_long %>% 
  summarise(Wit_mean = mean(dif_GHQ), Wit_sd = sd(dif_GHQ), Wit_min = min(dif_GHQ), Wit_max = max(dif_GHQ))
```

GHQ 的分佈並不左右對稱。

```{r  histGHQ, cache=TRUE, echo=FALSE, fig.height=5.5, fig.width=7.5, fig.cap='Histogram of GHQ by occasion', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}

ggplot(ghq_long, aes(x = GHQ, ..density.. , fill = occasion)) +
geom_histogram(position = "identity",binwidth= 5.5) + facet_wrap(~occasion) +
theme_bw() +
#  scale_x_continuous(breaks = 1:17)+
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "GHQ", y = "Density")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8),
        axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank())
```


### 對數據按照 `id` 分層進行 ANOVA

```{r hierex2-3, cache=TRUE, message=FALSE}
with(ghq_long, anova(lm(GHQ~factor(id))))

#library(lme4)
( fit <- lmer(GHQ ~ (1|id), data=ghq_long) )
```

$\sigma_u, \sigma_e$ 的估計值分別是 5.92 (between)， 1.91 (within)。可以計算層間相關係數 (intra-class correlation) $\hat\lambda = \frac{\sigma^2_u}{\sigma^2_u + \sigma^2_e} = 0.905$。且 $\hat\sigma_u = \sqrt{\frac{73.8 - 3.7}{2}} = 5.92$，和前一次練習一樣地，這個隨機效應的方差，可以通過方差分析表格來直接手動計算 (當且僅當分層數據是**平衡狀態**的)。和前面計算的樣本數據比較，樣本層間標準差是高估了的 (sample between variance = 6.073 > 5.92)，相反樣本層內標準差 (within sd) 則是低估了的 (sample within sd = 1.383 < 1.91)。兩個層內標準差的關係是：

$$
\sqrt{1.383^2\times\frac{23}{12}} = 1.91
$$


### 用 R 裏的 `nlme` 包，使用限制性極大似然法 (restricted maximum likelihood, REML) 擬合截距混合效應模型，比較其結果和前文中隨機效應 ANOVA 的結果

```{r hierex2-4, cache=TRUE, message=FALSE}
summary(nlme::lme(fixed = GHQ ~ 1, random = ~ 1 | id, data = ghq_long, method = "REML"))
```

截距混合效應模型的參數估計和隨機效應 ANOVA 的參數估計是一樣的。

### 用極大似然法 (maximum likelihood, ML) `method = "ML"` 重新擬合前面的混合效應模型，比較結果有什麼不同。


```{r hierex2-5, cache=TRUE, message=FALSE}
#( fit <- lmer(GHQ ~ (1|id), data=ghq_long, REML = FALSE) ) # same but from `lme4` package

summary(lme(fixed = GHQ ~ 1, random = ~ 1 | id, data = ghq_long, method = "ML"))
```

用極大似然法估計的隨機殘差標準差 $\sigma_e$ 和 REML/ANOVA 法估計的相同，但是隨機效應標準差 $\sigma_u$ 略小 5.65 < 5.92。


### 用簡單線性迴歸擬合一個固定效應模型

```{r hierex2-6, cache=TRUE, message=FALSE}
Fixed_reg <- lm(GHQ-mean(GHQ) ~ 0 + factor(id), data = ghq_long)
summary(Fixed_reg)
```

可以看到輸出報告最底段部分 `Residual standard error: 1.91 on 12 degrees of freedom` 就是前文三種不同模型擬合的隨機殘差效應的標準差。在 STATA 裏被叫做 `Root MSE`。

### 計算這些隨機截距的均值和標準差

```{r hierex2-7, cache=TRUE, message=FALSE}
mean(Fixed_reg$coefficients)
sd(Fixed_reg$coefficients)
```

這裏僅僅用固定效應模型時，不同羣截距的均值雖然和用混合效應模型估計的一樣爲零，但是其估計的標準差要大於無論是 REML (5.92) 或者是 ML (5.65) 估計值的大小，其實這裏簡單線性迴歸給出的截距均值，就是本練習一開始讓你計算的樣本均值的標準差 (between group sd)。這是因爲**簡單線性迴歸 (固定效應模型) 忽視了這些不同組的均值的不確定性**。

### 忽略掉所有的分層和解釋變量擬合 `GHQ` 的簡單線性迴歸

```{r hierex2-8, cache=TRUE, message=FALSE}
Fixed_simple <- lm(GHQ ~ 1, data = ghq_long)
summary(Fixed_simple)
```

此時的模型估計的 `Residual standard error: 6.09 on 23 degrees of freedom` 其實就是一開始讓你計算的樣本整體的標準差 (overall sd)


### 用分層的穩健法 (三明治標準誤法) 計算簡單線性迴歸時，截距的標準誤差，和簡單線性迴歸時的結果作比較


```{r hierex2-9, cache=TRUE, message=FALSE}
# sandwich robust method with cluster id

robustReg <- clubSandwich::coef_test(Fixed_simple, vcov = "CR1", cluster = ghq_long$id)

rob.std.err <- robustReg$SE
naive.std.err<-summary(Fixed_simple)$coefficients[,2]
better.table <- cbind("Estimate" = coef(Fixed_simple),
                      "Naive SE" = naive.std.err,
                      "Pr(>|z|)" = 2 * pt(abs(coef(Fixed_simple)/naive.std.err), df=nrow(ghq_long)-2, lower.tail = FALSE),
                      "LL" = coef(Fixed_simple) - 1.96 * naive.std.err,
                      "UL" = coef(Fixed_simple) + 1.96 * naive.std.err,
                      "Robust SE" = rob.std.err,
                      "Pr(>|z|)" = 2 * pt(abs(coef(Fixed_simple)/rob.std.err), df=nrow(ghq_long)-2,
lower.tail = FALSE),
                      "LL" = coef(Fixed_simple) - qt(df=robustReg$df, 0.975) * rob.std.err,
                      "UL" = coef(Fixed_simple) + qt(df=robustReg$df, 0.975) * rob.std.err)
rownames(better.table)<-c("Constant")
better.table
```

### 讀入 `siblings` 數據。先總結嬰兒的出生體重，思考這個數據中嬰兒出生體重之間是否可能存在關聯性？它的來源是哪裏。用這個數據擬合兩個混合效應模型 (ML, REML)，不加入任何解釋變量。


```{r hierex2-10, cache=TRUE, message=FALSE}
siblings <- read_dta("backupfiles/siblings.dta")
Fixed_ml <- lme(fixed = birwt ~ 1, random = ~ 1 | momid, data = siblings, method = "ML")
summary(Fixed_ml)

Fixed_reml <- lme(fixed = birwt ~ 1, random = ~ 1 | momid, data = siblings, method = "REML")
summary(Fixed_reml)
```

由於該數據樣本量足夠大 (混合效應模型中等同於說數據的層數足夠多)，你可以看到其實 ML 法和 REML 法估計的參數結果十分地接近。


# 隨機截距模型中加入共變量 random intercept model with covariates

這一章我們來把隨機截距模型加以擴展，在固定效應部分增加想要調整的共變量。

## 多元線性回歸模型的延伸

如果有一個含有兩個預測變量的多元線性回歸模型: 

$$
\begin{equation}
Y_{ij} = \beta_0 + \beta_1 X_{1ij} + \beta_2 X_{2ij} + \epsilon_{ij}
\end{equation}
(\#eq:hier03-1)
$$

如果觀測數據內部具有嵌套式結構，也就是有些對象之間有相關性，有些對象之間沒有，那麼上面這個多元線性回歸模型的誤差項 $\epsilon_{ij}$ 其實是不能被認爲相互獨立的，因爲數據中處以同一層的個體之間互相有關聯性 (屬於同一所學校的學生之間，同一所醫院的病人之間)。但是於此同時，我們不妨把最後的誤差項分成兩個部分 

$$
\epsilon_{ij} = u_j + e_{ij}
$$

其中， 

- $u_j$，是在隨機截距模型中用到的隨機截距部分，$u_j \sim N(0, \sigma_u^2)$，它允許不同層的數據有自己的截距;
- $e_{ij}$，是剝離掉層內相關 (等同於層間相異，intra-class correlation = between-class heterogeneity) 之後，剩餘的隨機殘差; 

之後把式子 \@ref(eq:hier03-1) 重新整理，就遇到了我們似曾相識的隨機截距模型: 

$$
\begin{equation}
Y_{ij} = (\beta_0 + u_j) + \beta_1 X_{1ij} + \beta_2 X_{2ij} + e_{ij}
\end{equation}
(\#eq:Hier03-01)
$$

這就是一個混合效應線性回歸模型 (linear mixed model)。其中，

- 固定效應部分的參數有 fixed effect parameters: $\beta_0, \beta_1, \beta_2$; 
- 隨機效應部分的參數有 random effect parameters: $u_j, e_{ij}$。

但是和之前的隨機截距模型不同的是，這裏我們在固定效應部分增加了兩個共變量 $X_1, X_2$，所以從該模型作出的所有統計推斷，都是建立在以這兩個共變量爲條件的基礎之上的 (conditionally on $\mathbf{X} = \{ X_1, X_2\}$)。所以對於 $u_j, e_{ij}$，他們的前提條件就變成了: 

- $\text{E}(u_j|\mathbf{X} = \{ X_1, X_2\}) = 0$; 
- $\text{E}(e_{ij}|\mathbf{X} = \{ X_1, X_2, u_j\}) = 0$。

根據這兩個條件，我們可以繼續得到: 

- $\text{E}(e_{ij} | \mathbf{X} = \{ X_1, X_2\}) = 0$;
- $\text{E}(Y_{ij} | \mathbf{X} = \{ X_1, X_2\}) = \beta_0 + \beta_1X_{1ij} + \beta_2X_{2ij}$

也就是說，這個包含了 $u_j, e_{ij}$ 的多元線性回歸模型，其邊際模型 (marginal regression over $u_j, e_{ij}$) 還是一個線性回歸。

**注意**

- 模型的固定效應部分加入了多個共變量 $\mathbf{X} = \{ X_1, X_2\}$ 之後，模型所估計的層內相關系數 (intra-class correlation, $\lambda$) 也成了以這些共變量爲條件的層內相關系數。
- $u_j$ 這個層別隨機截距 (cluster-specific random intercept) 此時會囊括已知/未知的層水平的特徵 (class-level characteristics, i.e. unmeasured heterogeneity between clusters)。它會隨着你在模型中加入層水平的解釋變量而逐漸變小 (Its size will decrease as more explanatory variables for the **cluster difference** are included in the model)。

## `siblings` 數據中新生兒體重的實例

在數據 `silblings` 中，研究者收集了來自 3978 名母親，8604 名新生兒出生體重 (g) 的數據。此外，該數據中還收集了這些新生兒的胎齡 (week)，新生兒的性別，母親孕期的吸煙狀況，以及懷孕時母親的年齡。在這個數據裏，每個母親是該數據的第二階層 (level 2)，每個母親的相關信息，就是屬於第二階層的層水平數據。每個新生兒的體重和相關數據，就是第一階層 (level 1) 數據，一個母親可能生 1-3 個嬰兒，這些來自同一個母親的新生兒之間很顯然不能視之爲相互獨立。研究者關心一個固定效應部分不包含其他共變量的隨機截距模型 (the Null Model)，和固定效應部分增加了其他共變量的隨機截距模型 (the Full Model) 哪個更能解釋這個數據或者更好的擬合這個數據 (better fitting the data)。

下面就先把數據讀入 R，然後建立一個零模型 (the Null Model):

```{r Hier03-01, cache=TRUE}
siblings <- read_dta("backupfiles/siblings.dta")
M0 <- lme(fixed = birwt ~ 1, random  = ~ 1 | momid, data = siblings, method = "REML")
summary(M0)
M0_fixed <- lm(birwt ~ 1, data = siblings)
anova(M0, M0_fixed)
```

下一步，我們來對該數據擬合一個全模型 (the Full Model)，我們可以先對兩個連續型變量 (胎齡，gestational age 和母親懷孕時年齡，maternal age) 進行適當的轉換，比方說把胎齡標準化成 38 周，懷孕時年齡標準化成 30 歲: 

```{r Hier03-02, cache=TRUE, message=FALSE}
siblings <- siblings %>%
  mutate(c_gestat = gestat - 38, # centering gestational age to 38 weeks
         c_mage = mage - 30,  # centering maternal age to 30 years old
         male = factor(male, labels = c("female", "male")), 
         smoke = factor(smoke, labels = c("Nonsmoker", "Smoker")))
#M_full <- lme(fixed = birwt ~ c_gestat + male + smoke + c_mage, random  = ~ 1 | momid, data = siblings, method = "REML")
M_full <- lmer(birwt ~ c_gestat + male + smoke + c_mage + (1 | momid), data = siblings, REML = TRUE)
library(lmerTest)
summary(M_full)
```


從全模型的結果報告中可以看出，固定效應部分加入的所有解釋變量都是有意義的。他們的含義如下: 

- `c_gestat 85.42`: 當模型中的其他變量保持不變時 (當模型中其他的變量被調整時)，胎齡每增加一周，**無論是同一個媽媽還是不同媽媽 (either from the same or another mother, i.e. in any cluster)** 生下的新生兒的出生體重增加的期待值是 85.42 g。
- `male 133.95`: 新生兒的性別如果是男孩，**無論是同一個媽媽還是不同媽媽**生下的新生兒，他的出生體重會比女孩增加 133.95 g。

再看這兩個模型的隨機效應部分，無論是第二層級水平的層標準差 (cluster-level) 還是第一層級 (elementary-level) 的標準差都隨着固定效應部分加入新的解釋變量而變小。我們同樣可以用極大似然法 (ML) 擬合這兩個模型，其方差大小總結成下面的表格: 


```{r hier03-tab1, echo=FALSE, cache=TRUE, eval=FALSE}
library(knitr)
library(kableExtra)
dt <- read.csv("backupfiles/hier3tb1.csv", header = T)
names(dt) <- c("Random Effect", "Null Model", "Full Model", "Null Model", "Full Model")
kable(dt, "html",  align = "c", caption = "Summary of estimates of the variation of the random effects of the null and full model using REML or ML") %>%
  kable_styling(bootstrap_options = c("striped", "bordered"),full_width = F, position = "center") %>%
  add_header_above(c(" " = 1,"REML" = 2, "ML" = 2))
```


<table class="table table-striped table-bordered" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>表 60.1: Summary of estimates of the variation of the random effects of the null and full model using REML or ML</caption>
 <thead>
<tr>
<th style="border-bottom:hidden" colspan="1"></th>
<th style="text-align:center; border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;" colspan="2"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px;">REML</div></th>
<th style="text-align:center; border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;" colspan="2"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px;">ML</div></th>
</tr>
  <tr>
   <th style="text-align:center;"> Random Effect </th>
   <th style="text-align:center;"> Null Model </th>
   <th style="text-align:center;"> Full Model </th>
   <th style="text-align:center;"> Null Model </th>
   <th style="text-align:center;"> Full Model </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> $\hat\sigma_u$ </td>
   <td style="text-align:center;"> 368.3558 </td>
   <td style="text-align:center;"> 315.8853 </td>
   <td style="text-align:center;"> 368.2864 </td>
   <td style="text-align:center;"> 315.7320 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> $\hat\sigma_e$ </td>
   <td style="text-align:center;"> 377.6577 </td>
   <td style="text-align:center;"> 343.5296 </td>
   <td style="text-align:center;"> 377.6579 </td>
   <td style="text-align:center;"> 343.4581 </td>
  </tr>
</tbody>
</table>



表格的右半部分總結的是使用極大似然法 (會偏小估計隨機效應方差)，其實它們和 REML 法估計的結果相差不大。**值得強調的是，由於 REML 法每次估計的數據是去除掉固定效應部分以後的隨機誤差部分的數據，所以當兩個用 REML 法估計的混合效應模型其固定效應部分不一致的時候，這兩個模型實際擬合了不同的數據，是不能使用 LRT 來比較兩個模型哪個更好的。**

## 賦值予隨機效應成分

值得建議地，擬合了任何一個混合效應模型以後，需要盡量避免直接跳入結論陳述階段，而應當先對模型是否符合其假定的前提條件進行模型診斷。而且，對模型的擬合後截距及其層級隨機效應 (cluster random effect) 進行視覺化展現變得十分有用。

總體來說，有兩種方法可以用於估計並提取這些擬合值 -- ML 和 Empirical Bayes (EB)。

### 簡單預測 simple prediction

和簡單線性回歸模型一樣，我們可以計_算模型的預測值和觀測值之間的差，獲得一個包含了兩個隨機效應成分的量: 

$$
\begin{aligned}
Y_{ij} & = \beta_0 + \beta_1X_{1ij} + u_j + e_{ij} \\
\Rightarrow Y_{ij} & =  \beta_0 + \beta_1X_{1ij} + \epsilon_{ij} \\
\Rightarrow \hat\epsilon_{ij} & = Y_{ij} - (\hat\beta_0 + \beta_1X_{1ij})
\end{aligned}
$$

那麼最簡單的方法就是計算了這個隨機效應成分的混合體之後，對其取平均值，作爲 $u_j$ 的簡單估計: 

```{r Hier03-03, cache=TRUE, message=FALSE}
M_full <- lme(fixed = birwt ~ c_gestat + male + smoke + c_mage, random  = ~ 1 | momid, 
              data = siblings, method = "REML")

siblings$yhat <- M_full$fitted[,1]
siblings <- siblings %>%
  mutate(res = birwt- yhat)
Mean_siblings <- ddply(siblings, ~momid, summarise, uhat = mean(res))
Mean_siblings[Mean_siblings$momid == 14,]
siblings[siblings$momid == 14,c(1,5,6,15,16)]
```

找到編號 14 號的母親，她有三個孩子被研究者記錄到，他們中有的孩子使用該模型計算的擬合值 (fitted value = `yhat`) 並不準確。在調整了胎齡，嬰兒性別，母親的吸煙狀況，和母親懷孕時年齡後，該母親生的孩子，和該隊列的總體平均值 (overall mean) 相比較，其偏差達到了 105.12 g。

我們可以對每個母親的擬合偏差做總結歸納: 

```{r Hier03-04, cache=TRUE, message=FALSE}
Mean_siblings %>% 
  summarise(Mean_uhat = mean(uhat),
            sd_uhat = sd(uhat),
            min_uhat = min(uhat),
            max_uhat = max(uhat))
```

可見這 3978 名母親總體的擬合偏差的均值爲 -0.511，接近零。且它的標準差接近 400。這樣一種直接利用觀測值和擬合值之差做曾內平均的方法被叫做極大似然法 ML，這樣計算獲得的平均偏差被標記爲 $\hat u_j^{\text{ML}}$

### EB 預測值

EB 法 (經驗貝葉斯法) 也一樣要利用擬合模型後的 $\beta$ 來計算獲得層殘差 (cluster level residuals)。但是用 EB 法時我們還再使用層殘差的一個前提條件: $u_j \sim N(0, \sigma_u^2)$。在線性隨機截距模型中，EB 法計算的層級殘差和簡單法計算的層殘差之間有如下的簡單轉換關系: 

$$
\hat u_j^{\text{EB}} = \hat R_j\hat u_j^{\text{ML}}
$$

其中 $\hat R_j$ 被定義爲 ML 法計算層級殘差的可靠性 (reliability of $\hat u_j^{\text{ML}}$)，它是一個包含了層級方差和個人水平方差的方程: 

$$
\hat R_j = \frac{\hat\sigma_u^2}{\hat\sigma_u^2 + \sigma_e^2/n_j} = \hat w_j \hat \sigma_u^2
$$

其中 $\hat w_j$ 是之前在章節 \@ref(fixed-inference) 定義的權重。這個 $\hat R_j$ 又被叫做是**收縮因子 (shrinkage factor)**，因爲它取值是在 0 到 1 之間，所以它會把 ML 法計算獲得的層級誤差按照收縮銀子比例收縮變小。當 $\sigma_u$ 本身比較小，或者個體的隨機誤差大 $\sigma_e$，或者層內樣本量小 $n_j$ 時收縮因子的作用更大。

此時，預測誤差 $(\hat u_j^{\text{EB}} - u_j)$ 才是我們能夠從觀測數據以及模型中獲得的均值爲零方差又最小的殘差。所以 $\hat u_j^{\text{EB}}$ 又被稱爲 $\text{Best linear unbiased predictors, BLUP}$。

第二層級殘差的方差是: 

$$
R_j\hat \sigma_u^2
$$

## 混合效應模型的診斷

辛苦計算了 BLUP 之後，就可以拿它，和模型的標準化殘差來對模型作出一定的診斷。由於計算獲得的 BLUP 方差不齊，要先對其標準化之後再作正態圖: 

```{r Hier03-05, cache=TRUE, message=FALSE}
# the standardized 

n_child <- siblings %>% count(momid, sort = TRUE)
Mean_siblings <- merge(Mean_siblings, n_child, by = "momid")  

Mean_siblings <- Mean_siblings %>%
  mutate(# extract the random effect (EB) residuals at level 2
         uhat_eb = ranef(M_full)$`(Intercept)`, 
         # shrinkage factor 
         R = 315.7338^2/(315.7338^2 + (343.4572^2)/n), 
         # Empirical Bayes prediction of variance of uhat
         var_eb = R*(315.7338^2),
         # standardize the EB uhat
         uhat_st = uhat_eb/sqrt(var_eb)
  )

# 計算每個個體的標準化殘差

siblings$ehat <- residuals(M_full, level = 1, type = "normalized")
```



```{r level2-residuals-unst, cache=TRUE, echo=FALSE, fig.height=5.5, fig.width=11, fig.cap='Histogram and Q-Q plot of cluster (mother) level unstandardized residuals for the intercept', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
hist <- ggplot(Mean_siblings, aes(x = uhat_eb)) +
    geom_histogram(colour = "black", aes(y = ..density..), 
                   fill = "lightblue", size = 0.1, binwidth= 28)  + 
  labs(y = "Density", x = "EB estimates of un-standardised u_0") + 
  theme_bw()# + stat_function(fun=dnorm,
             #            color="darkgreen", size = 1,
              #           args=list(mean=mean(Mean_siblings$uhat_st), 
               #                   sd=sd(Mean_siblings$uhat_st)))


plot2 <- HLMdiag::ggplot_qqnorm(Mean_siblings$uhat_eb, line = "quantile") +
  theme_bw() +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "unstandardized EB u_hat (mother level)")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8),
        axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank())

ggarrange(hist, plot2,  
          ncol = 2, nrow = 1)
```


```{r level2-residuals-st, cache=TRUE, echo=FALSE, fig.height=5.5, fig.width=11, fig.cap='Histogram and Q-Q plot of cluster (mother) level standardized residuals for the intercept', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
hist <- ggplot(Mean_siblings, aes(x = uhat_st)) +
    geom_histogram(colour = "black", aes(y = ..density..), 
                   fill = "lightblue", size = 0.1, binwidth= 0.2)  + 
  labs(y = "Density", x = "EB estimates of standardised u_0") + 
  theme_bw()# + stat_function(fun=dnorm,
             #            color="darkgreen", size = 1,
              #           args=list(mean=mean(Mean_siblings$uhat_st), 
               #                   sd=sd(Mean_siblings$uhat_st)))


plot2 <- HLMdiag::ggplot_qqnorm(Mean_siblings$uhat_st, line = "quantile") +
  theme_bw() +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "standardized EB u_hat (mother level)")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8),
        axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank())

ggarrange(hist, plot2,  
          ncol = 2, nrow = 1)
```


```{r level1-residuals-baby, cache=TRUE, echo=FALSE, fig.height=5.5, fig.width=11, fig.cap='Histogram and Q-Q plot of individual (pupil) level standardized residuals for the intercept', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
hist <- ggplot(siblings, aes(x = ehat)) +
    geom_histogram(colour = "black", aes(y = ..density..), 
                   fill = "lightblue", size = 0.1, binwidth= 0.15)  + 
  labs(y = "Density", x = "standardised e") + 
  theme_bw()# + stat_function(fun=dnorm,
             #            color="darkgreen", size = 1,
              #           args=list(mean=mean(Mean_siblings$uhat_st), 
               #                   sd=sd(Mean_siblings$uhat_st)))


plot2 <- HLMdiag::ggplot_qqnorm(siblings$ehat, line = "quantile") +
  theme_bw() +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "standardized ehat (child level)")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8),
        axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank())

ggarrange(hist, plot2,  
          ncol = 2, nrow = 1)
```


這些正態圖，主要用於輔助尋找看哪裏有異常值 (outliers)。


## 第二層級 (cluster level/level 2) 的協方差

還是這個 `siblings` 數據中，關於母親的數據在該母親生的孩子中是保持不變的，比如有人種 (`black`)，母親受教育情況 (`hsgrad`)，和母親的婚姻狀況 (`married`)。因爲這些變量屬於解釋第二層級 (level 2) 的變量，加入這些變量在固定效應部分只能解釋層間的方差 (between clusters variance): 

```{r Hier03-06, cache=TRUE, message=FALSE}
siblings <- siblings %>%
  mutate(black = factor(black, labels = c("No", "Yes")), 
         hsgrad = factor(hsgrad, labels = c("No", "Yes")),
         married = factor(married, labels = c("No", "yes")))

M_full1 <- lmer(birwt ~ c_gestat + male + smoke + c_mage + black + married + hsgrad + (1 | momid), 
                data = siblings, REML = TRUE)
summary(M_full1)
```


加入了第二層級協變量之後， $\sigma^2_u = 96845.79$，相比沒加之前的小了一些 $(\sigma^2_{u} = 99784)$。但是 $\sigma^2_e$ 幾乎保持不變。 

## 層內層間效應估計

如有某個想加入模型的變量是屬於第一層級的，例如 `siblings` 數據中的胎齡，即使是同一個媽媽生的嬰兒，其出生時的胎齡也是各不相同。但是這樣在模型輸出的報告中，胎齡這一變量的估計量其實是其他變量保持不變時，**每增加一周胎兒對不論是同一個母親還是不同母親生的嬰兒的出生體重的影響**，怎樣才能把同一母親不同胎齡的影響 (within effect) 和不同母親不同胎齡的影響 (between effect) 給區分出來呢？

其實很簡單，我們來把胎齡這個變量做個分解: 

$$
Y_{ij} = \beta_0 + \beta_{1B} \bar{X}_{\cdot j} + \beta_{1W} (X_{ij} - \bar{X}_{\cdot j}) + u_j + e_{ij}
$$

把胎齡這個變量分解成 $\bar{X}_{\cdot j}$ (每個母親生的嬰兒的平均胎齡)，和 $X_{ij} - \bar{X}_{\cdot j}$ (每個母親內，每個胎兒的胎齡和平均胎齡之差) 兩個部分，就解決了區分層間效應 $(\beta_{1B})$ 和層內效應 $(\beta_{1W})$。 的方法。下面的模型在固定效應部分只使用了胎齡一個變量 (爲了這裏輸出報告簡潔明了): 

```{r Hier03-07, cache=TRUE, message=FALSE}
M_gestat <- lmer(birwt ~ c_gestat + (1 | momid), data = siblings, REML = TRUE)
summary(M_gestat)
```

當把胎齡作爲一個變量放進模型的固定效應部分時，不論是不是同一個母親生下的胎兒，只要胎齡每增加一周，出生體重就增加 83.7 g。下一個模型中，我們來把胎齡這個變量分解成層間變量和層內變量: 

```{r Hier03-08, cache=TRUE, message=FALSE}
Mean_gestat <- ddply(siblings, ~ momid, summarise, mean_gestat = mean(gestat))
# 把每個母親的胎兒胎齡均值 (level 2 mean) 賦予原有的數據中
avegest <- NULL
for (i in 1:3978){
  avegest <- c(avegest, rep(Mean_gestat$mean_gestat[i], with(siblings, table(momid))[i]))
}
siblings$avegest <- avegest
rm(avegest)
# 計算層內胎兒胎齡與其層均值的差異
siblings <- siblings %>%
  mutate(c_avegest = avegest - 38, 
         difgest = gestat - avegest)

siblings[siblings$momid == 14,c(1,2,5,6,18:20)]

# 下面用 c_avegest 和 difgest 代替 gestat 放入同樣模型的固定效應部分

M_gestat_sep <- lmer(birwt ~ c_avegest + difgest + (1 | momid), data = siblings, REML = TRUE)
summary(M_gestat_sep)
```

把胎齡分解了以後，從模型的輸出結果可以看出，層間效應 113 g (不同的母親)，要大於層內效應 70.9 g (同一母親不同胎兒)。

比較分解胎齡以後的模型 `M_gestat_sep` 和把胎齡作爲一個變量的模型 `M_gestat` 哪個更優，可以有兩種檢驗法: 

```{r Hier03-09, cache=TRUE, message=FALSE}
# 1. 用 ML 法重新擬合兩個模型後進行 LRT 檢驗比較 R 可以自動幫你
anova(M_gestat_sep, M_gestat)

# 2. 用 Wald 檢驗比較 Beta_1W 和 Beta_1B 是不是不同
linearHypothesis(M_gestat_sep, "c_avegest = difgest")
```

無論是哪種檢驗，都告訴我們把胎齡分解了的模型更好。了解更多層內層間回歸模型，參照 [@Mann2004]。


## 到底選擇固定還是混合模型？

目前爲止我們討論了嵌套式數據可以使用固定效應模型分析，也可以使用混合效應模型來擬合，那麼到底你該選擇哪個來解釋你的數據呢？ 選擇模型永遠是一個很難回答的問題。哪種模型更加恰當 (appropriate) 其實要取決於你的數據結構，分層的數據的話層的數量是不是足夠多？以及最重要的，你的**分析目的*。

1. 如果模型中想分析的層/羣組，可以被視爲唯一的實體 (uniqe entity，例如不同的種族)，而且我們希望從模型來獲得對不同種羣或者不同個體中每一個個體的估計，那麼固定效應模型是合適的。
2. 如果層/羣組其實是人羣中的樣本 (samples from a real population，如例題中的母親層級，人羣衆可以有許許多多的母親)，我們打算從這個模型的結果去推論整個人羣，那麼隨機效應模型才是最合適的。
3. 如果說層級本身的樣本量 (n of clusters) 太小，那麼強行使用混合效應模型的話會導致隨機效應的估計結果十分地低效，甚至沒有意義; 當然如果你的混合效應模型關心的是固定效應部分，那麼增加一些層級隨機效應應該也能達到提升統計估計效率的目的。
4. 如果我們關心的是層級協變量的效應，那麼隨機效應模型是唯一的選擇。





## 練習題目


### 把 High-school-and-Beyond 數據讀入 R 中。

```{r hierex3-1, cache=TRUE, message=FALSE}
hsb_selected <- read_dta("backupfiles/hsb_selected.dta")
length(unique(hsb_selected$schoolid)) ## number of school = 160
## create a subset data with only the first observation of each school
hsb <- hsb_selected[!duplicated(hsb_selected$schoolid), ]

## about 44 % of the schools are Catholic schools
with(hsb, tab1(sector, graph = FALSE, decimal = 2))

## among all the schools, average school size is 1098
with(hsb, summ(size, graph = FALSE, decimal = 2))


## among all the pupils, about 53% are females
with(hsb_selected, tab1(female, graph = FALSE, decimal = 2))

## among all the pupils, about 27.5% are from ethnic minorities
with(hsb_selected, tab1(minority, graph = FALSE, decimal = 2))
```

### 擬合兩個隨機截距模型 (ML, REML)，結果變量用 `mathach`，解釋變量用 `ses`。觀察結果是否不同。


```{r hierex3-2, cache=TRUE, message=FALSE}
Fixed_reml <- lmer(mathach ~ ses + (1 | schoolid), data = hsb_selected, REML = TRUE)
summary(Fixed_reml)

Fixed_ml <- lmer(mathach ~ ses + (1 | schoolid), data = hsb_selected, REML = FALSE)
summary(Fixed_ml)
```

其實由於樣本量 (層數) 足夠多，兩個隨機截距模型給出的參數估計十分接近。


### 觀察學校類型是否爲天主教學校 `sector` 的分佈，把它加入剛擬合的兩個隨機截距模型，它們估計的隨機效應標準差 $\hat\sigma_u$，和隨機誤差標準差 $\hat\sigma_e$，和之前有什麼不同？ “ML，REML” 的選用對結果有影響嗎？


```{r hierex3-3, cache=TRUE, message=FALSE}
Fixed_reml <- lmer(mathach ~ ses + factor(sector) + (1 | schoolid), data = hsb_selected, REML = TRUE)
summary(Fixed_reml)

Fixed_ml <- lmer(mathach ~ ses + factor(sector) +  (1 | schoolid), data = hsb_selected, REML = FALSE)
summary(Fixed_ml)
```

可以看出，`sector` 變量在學校層面上都是沒有變化的，所以加它進入混合效應的固定部分，只會對隨機效應標準差 (within level/cluster/group error) $\hat\sigma_u$ 的估計造成影響，隨機誤差標準差 $\hat\sigma_e$ 則幾乎不受影響。同樣的 “ML, REML” 兩種方法對結果的影響微乎其微。

### 現在把學校規模 `size` 這一變量加入混合效應模型的固定效應部分，記得先把該變量中心化，並除以 100，會有助於對結果的解釋 (比平均值每增加100名學生)。仔細觀察模型結果中隨機效應標準差和隨機誤差標準殘差的變化。


```{r hierex3-4, cache=TRUE, message=FALSE}
hsb_selected <- hsb_selected %>%
  mutate(c_size = (size - with(hsb, mean(size)))/100)

Fixed_reml <- lmer(mathach ~ ses + factor(sector) + c_size + (1 | schoolid), data = hsb_selected, REML = TRUE)
summary(Fixed_reml)

Fixed_ml <- lmer(mathach ~ ses + factor(sector) + c_size + (1 | schoolid), data = hsb_selected, REML = FALSE)
summary(Fixed_ml)
```

增加了 `size` 進入混合效應模型的固定效應部分，對兩種參數估計方法輸出的結果 $(\hat\sigma_u, \hat\sigma_e)$ 並沒有太大的影響。


### 在模型的固定效應部分增加 `size*sector` 的交互作用項。觀察輸出結果中該交互作用項是否有意義。用什麼檢驗方法判斷這個交互作用項能否幫助模型更加擬合數據？


```{r hierex3-5, cache=TRUE, message=FALSE}
Fixed_reml <- lmer(mathach ~ ses + factor(sector)*c_size + (1 | schoolid), data = hsb_selected, REML = TRUE)
summary(Fixed_reml)

Fixed_ml <- lmer(mathach ~ ses + factor(sector)*c_size + (1 | schoolid), data = hsb_selected, REML = FALSE)
summary(Fixed_ml)
```

在兩個估計方法的報告中，交互作用項均不具有統計學意義。

### 把上面八個模型估計的隨機效應標準差，和隨機誤差標準差總結成表格，它們之間有什麼規律嗎？


```{r hierex3-6, echo=FALSE, cache=TRUE, eval=FALSE}
library(knitr)
library(kableExtra)
dt <- read.csv("backupfiles/hierPractical3tb1.csv", header = T)
names(dt) <- c("Model with", "sigma_u", "sigma_e", "sigma_u", "sigma_e")
kable(dt, "html",  align = "c", caption = "Random effect sd and random residual sd from previous 8 mixed effect models") %>%
  kable_styling(bootstrap_options = c("striped", "bordered"),full_width = F, position = "center") %>%
  add_header_above(c(" " = 1,"REML" = 2, "ML" = 2))
```

<table class="table table-striped table-bordered" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>Random effect sd and random residual sd from previous 8 mixed effect models</caption>
 <thead>
<tr>
<th style="border-bottom:hidden" colspan="1"></th>
<th style="text-align:center; border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;" colspan="2"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px;">REML</div></th>
<th style="text-align:center; border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;" colspan="2"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px;">ML</div></th>
</tr>
  <tr>
   <th style="text-align:center;"> Model with </th>
   <th style="text-align:center;"> $\sigma_u$ </th>
   <th style="text-align:center;"> $\sigma_e$ </th>
   <th style="text-align:center;"> $\sigma_u$ </th>
   <th style="text-align:center;"> $\sigma_e$ </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> ses </td>
   <td style="text-align:center;"> 2.184 </td>
   <td style="text-align:center;"> 6.085 </td>
   <td style="text-align:center;"> 2.175 </td>
   <td style="text-align:center;"> 6.085 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> ses &amp; sector </td>
   <td style="text-align:center;"> 1.920 </td>
   <td style="text-align:center;"> 6.086 </td>
   <td style="text-align:center;"> 1.903 </td>
   <td style="text-align:center;"> 6.086 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> ses, size &amp; sector </td>
   <td style="text-align:center;"> 1.907 </td>
   <td style="text-align:center;"> 6.086 </td>
   <td style="text-align:center;"> 1.883 </td>
   <td style="text-align:center;"> 6.085 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> ses, size &amp; sector  <br> &amp; size*sector </td>
   <td style="text-align:center;"> 1.887 </td>
   <td style="text-align:center;"> 6.086 </td>
   <td style="text-align:center;"> 1.854 </td>
   <td style="text-align:center;"> 6.086 </td>
  </tr>
</tbody>
</table>

$\hat\sigma_e$ 幾乎在所有模型的估計中都保持不變。因爲我們在固定效應中增加的共變量在學校層面 (level 2) 都是一樣的。也就是說對於同一學校的學生，新增的共變量是一模一樣沒有變化的，所以在個人水平 (level 1) 的隨機效應幾乎不會發生變化。且注意到 “ML” 極大似然法估計的隨機效應標準差比 “REML” 限制性極大似然估計法給出的結果略小 1% 左右。

### 在混合效應模型的固定效應部分增加學生性別 `female`，和學生是否是少數族裔 `minority` 兩個變量。再觀察 $\hat\sigma_u, \hat\sigma_e$ 是否發生變化？


```{r hierex3-7, echo=TRUE, cache=TRUE}
Fixed_reml <- lmer(mathach ~ ses + factor(sector) + c_size + factor(female) + factor(minority) + (1 | schoolid), data = hsb_selected, REML = TRUE)
summary(Fixed_reml)

Fixed_ml <- lmer(mathach ~ ses + factor(sector) + c_size + factor(female) + factor(minority) + (1 | schoolid), data = hsb_selected, REML = FALSE)
summary(Fixed_ml)
```

混合效應模型的固定效應部分增加了學生性別，以及是否是少數族裔以後，“ML/REML” 估計的 $\hat\sigma_u, \hat\sigma_e$ 均發生了顯著變化。因爲它們在個人水平都不一樣 (level 1, within group random residuals)。

### 檢查學生性別和族裔是否和學校是否是天主教會學校有關係，先作分類型數據的分佈表格，然後把它們各自與 `sector` 的交互作用項加入混合效應模型中的固定效應部分，記錄下此時的 $\hat\sigma_u, \hat\sigma_e$


```{r hierex3-8, echo=TRUE, cache=TRUE}
# Only minority is associated with sector. There are more pupils from
# ethnic minorities attending catholic schools
with(hsb_selected, tabpct( sector, minority, graph = FALSE))
with(hsb_selected, tabpct( sector, female, graph = FALSE))


## there was no significant interaction between female sex and sector so
## this is deleted from the final model
Fixed_reml <- lmer(mathach ~ ses + factor(sector)*factor(female)  + c_size + factor(minority) + (1 | schoolid), data = hsb_selected, REML = TRUE)
summary(Fixed_reml)

## There is an interaction between minority and sector
Fixed_reml <- lmer(mathach ~ ses + factor(sector)*factor(minority)  + c_size + factor(female) + (1 | schoolid), data = hsb_selected, REML = TRUE)
summary(Fixed_reml)
```

數據顯示，少數族裔更多地選擇天主教會學校學習。學生性別則與是否是天主教會學校之間沒有顯著的關係。少數族裔和教會學校之間的交互作用同時也被發現具有統計學意義。

### 對上面最後一個模型進行殘差分析和模型的診斷。



```{r hierex3-9, echo=TRUE, cache=TRUE}
#fit <- lmer(mathach ~ ses + factor(sector)*factor(minority) + c_size + 
#              factor(female) + (1| schoolid), data=hsb_selected)
#summary(fit)
Fixed_reml <- lme(fixed = mathach ~ ses + factor(sector)*factor(minority)  + c_size + factor(female),  random = ~ 1 | schoolid, data = hsb_selected, method = "REML")
summary(Fixed_reml)

# number of students in each school
n_pupil <- hsb_selected %>% count(schoolid, sort = TRUE)
hsb <- merge(hsb, n_pupil, by = "schoolid")  


hsb <- hsb %>%
  mutate(# extract the random effect (EB) residuals (at school level)
         uhat_eb = ranef(Fixed_reml)$`(Intercept)`, 
         # number of students in each school
         # npupil = count(hsb_selected$schoolid)[2]$freq, 
         # shrinkage factor = sigma_u^2/(sigma_u^2+sigma_e^2/n_j)
         R = 1.474^2/(1.474^2 + (5.981^2)/n),
         # Empirical Bayes prediction of variance of uhat
         var_eb = R*1.474^2, 
         # standardize the uhat
         uhat_st = uhat_eb/sqrt(var_eb))

# extract the standardized random residuals (at pupil level)
hsb_selected$ehat <- residuals(Fixed_reml, level = 1, type = "normalized")
```

```{r level2-residuals, cache=TRUE, echo=TRUE, fig.height=5.5, fig.width=11, fig.cap='Histogram and Q-Q plot of cluster (school) level standardized residuals for the intercept', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
par(mfrow=c(1,2))
hist(hsb$uhat_st, freq = FALSE, breaks = 12, col='lightblue')
qqnorm(hsb$uhat_st, ylab = "Standardized level 2 (school) residuals"); qqline(hsb$uhat_st, col=2)
```
```{r level1-residuals, cache=TRUE, echo=TRUE, fig.height=5.5, fig.width=11, fig.cap='Histogram and Q-Q plot of individual (pupil) level standardized residuals for the intercept', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
par(mfrow=c(1,2))
hist(hsb_selected$ehat, freq = FALSE, breaks = 38, col='lightblue')
qqnorm(hsb_selected$ehat,  ylab = "Standardized level 1 (pupil) residuals"); qqline(hsb_selected$ehat, col=2)
```

### 通過剛剛所求的隨機效應方差的殘差，確認哪個學校存在相對極端的值。

```{r hierex3-10, echo=TRUE, cache=TRUE}
summ(hsb$uhat_st, graph = FALSE)
hsb[with(hsb, which(uhat_st > 2.5)),  c(7, 5, 6, 12)]
hsb[with(hsb, which(uhat_st < -2.5)), c(7, 5, 6, 12)]
```

所以，此處可以看出，隨機效應殘差下提示的隨機效應標準差可能比較極端的有上面這三所規模較小的學校。其中一所是天主教會學校，另外兩所是非天主教會學校。

### 計算學校水平的 SES 平均值，以及每個學生自己和所在學校均值之間的差值大小。分別擬合兩個不同的混合效應模型，一個只用 `SES`，另一個換做使用新計算的組均值和組內均差。

```{r hierex3-11, echo=TRUE, cache=TRUE}
Mean_ses_math <- ddply(hsb_selected,~schoolid,summarise,mean_ses=mean(ses),mean_math=mean(mathach))


hsb_selected$dif_ses <- NA
for (i in Mean_ses_math$schoolid) {
hsb_selected$dif_ses[which(hsb_selected$schoolid == i)] <-  hsb_selected$ses[which(hsb_selected$schoolid == i)] - 
  Mean_ses_math$mean_ses[which(Mean_ses_math$schoolid == i)]
}

hsb_selected <- hsb_selected %>%
  mutate(mean_ses = ses - dif_ses)

## total simple model with ses only 
Simple_reml <- lmer(mathach ~ ses + (1 | schoolid), data = hsb_selected, REML = TRUE)
summary(Simple_reml)

## fit the extended model within and between effect separated
Extend_reml <- lmer(mathach ~ dif_ses + mean_ses + (1 | schoolid), data = hsb_selected, REML = TRUE)
## the between schools effect (5.87) seems much larger than the within school effect (2.19) 
summary(Extend_reml)

## We find strong evidence to support that the second model gives a better fit to the data
mod2<- update(Extend_reml, . ~ . - dif_ses - mean_ses)
anova(Extend_reml, mod2)
```


# 隨機回歸系數模型  random coefficient model 

這一章節我們把隨機截距模型進一步擴展，在隨機效應部分增加隨機斜率成分 (random slope)。這樣的模型又稱隨機系數模型 (random coefficient model) 或 隨機斜率模型 (random slope model)。

## GCSE scores 實例

第一章介紹過的 65 所中學學生在入學前的閱讀水平成績和畢業時的考試成績的 GCSE 數據，用來作爲本章介紹概念的實例。我們先對其中學校編號爲 1 的學生做兩個成績的線性回歸: 

```{r hie04-1, cache=TRUE, warning=FALSE, message=FALSE}
gcse_selected <- read_dta("backupfiles/gcse_selected.dta")
M_sch1 <- lm(gcse ~ lrt, data = gcse_selected[gcse_selected$school == 1, ])

summary(M_sch1)
```

```{r hier04-fig1, cache=TRUE, echo=FALSE, fig.height=5, fig.width=7, fig.cap='GCSE versus LRT in school 1', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
ggthemr("fresh")

ggplot(gcse_selected[gcse_selected$school == 1, ], aes(x = lrt, y = gcse)) + geom_point() + 
  geom_abline(intercept = M_sch1$coefficients[1], slope = M_sch1$coefficients[2]) + 
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "GCSE score",x = "London Reading Test")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8),
        axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank())
```

當我們重復同樣的實驗，給 65 所學校 (48號學校除外，它只有兩個學生) 一一繪制回歸直線的時候，你得到的一簇直線是這樣紙的: 

```{r hier04-fig2, cache=TRUE, echo=FALSE, fig.height=5, fig.width=7, fig.cap='Predicted regression lines of GCSE versus LRT scores: separate estimates from each school', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
ggthemr("fresh")

# extract the school numbers as a numeric vector
School_n <- as.numeric(with(gcse_selected, table(school)))
# fit 65 linear regressions save them into a list
my_lms <- lapply(1:65, function(School_n) lm(gcse ~ lrt, data = gcse_selected[gcse_selected$school == School_n, ]))

# extract just coefficients
Coefs <- sapply(my_lms, coef) # this is a matrix
Coefs <- as.data.frame(t(Coefs)) # convert it into a dataframe vertically 

ggplot( gcse_selected[gcse_selected$school != 48, ], aes(x = lrt, y = gcse, group = factor(school))) + geom_blank() + 
  geom_smooth(se = FALSE, method = "lm") + 
  #geom_abline(data = Coefs[-48,],   aes(intercept = `(Intercept)`, slope = lrt))  +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  xlim(-40, 40) + 
  ylim(-20, 30) +
  #scale_x_continuous(breaks = seq(-45, 45, by = 10))+
 # scale_y_continuous(breaks = -20:20) +
  labs(y = "Fitted regression lines",x = "London Reading Test")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8),
        axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank())
```


實際上這麼多學校學生的成績前後回歸線，其截距和斜率各不相同 (圖\@ref(fig:hier04-fig2))。這些斜率和截距的總結歸納如下: 

```{r hie04-2, cache=TRUE, warning=FALSE, message=FALSE}
summ(Coefs[-48,])
```

```{r hier04-fig3, cache=TRUE, echo=FALSE, fig.height=5, fig.width=7, fig.cap='School specific slopes and intercepts', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
ggthemr("fresh")

ggplot(Coefs[-48, ], aes(x = `(Intercept)`, y = lrt)) + geom_point() + 
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  geom_vline(xintercept = -0.33) + 
  geom_hline(yintercept = 0.54) + 
  xlim(-10, 10) + 
  ylim(0, 1) +
  #scale_x_continuous(breaks = seq(-45, 45, by = 10))+
 # scale_y_continuous(breaks = -20:20) +
  labs(y = "Slopes",x = "Intercept")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8),
        axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank())

```


圖 \@ref(fig:hier04-fig3) 展示的是這些回歸直線各自的截距 (x 軸) 和斜率 (y 軸) 的散點圖。縱橫添加的兩條直線分別是截距和斜率的均值的位置。很明顯，截距和斜率之間本身是呈現正相關的 (相關系數 0.36): **如果一個學校學生入學時成績一般，但是畢業時 GCSE 成績較高，說明那所學校本身對學生成績的提升作用明顯**。

經過擬合64個線性回歸模型，獲得 $64\times3$ 個不同的回歸線的參數 (截距，斜率，和殘差方差)。所以我們可以提出的關於 "學校" 這個個體，它們各自的入學前後成績作出的回歸線獲得的三個參數，在它的 **"人羣 (可以是英國國內的中學，全歐洲的中學，或者是全世界的中學)"** 中是隨機分布在一些 "均值" 附近的。

## 隨機回歸系數的實質

在隨機截距模型中，截距可以隨機分布在某個均值周圍，但是每條回歸直線我們默認其解釋變量和結果變量之間的關系是一樣的 (相同斜率的一簇直線)。現在，我們來把這個模型擴展，放寬它對斜率的限制，允許不同的層與層之間不僅僅可以有不同的截距，還可以有不同的斜率: 

$$
\begin{equation}
Y_{ij} = (\beta_0 + u_{0j}) + (\beta_1 + u_{1j})X_{1ij} + e_{ij}
\end{equation}
(\#eq:hier04-1)
$$

其中: 

- $u_{0j}:$ 是隨機截距成分 (第 $j$ 層數據和總體均值 $\beta_0$ 之間的差異)
- $u_{1j}:$ 是隨機斜率成分 (第 $j$ 層數據和總體寫率 $\beta_1$ 之間的差異)
- $\text{E}(u_{0j}|X_{1ij}) = 0$
- $\text{E}(u_{1j}|X_{1ij}) = 0$
- $\text{E}(e_{ij}|X_{1ij},u_{0j}, u_{1j}) = 0$
- $u_0, u_1 \perp X_{1ij}$ (兩個隨機部分和解釋變量之間獨立不相關)
- $u_0, u_1 \perp e_{ij}$ (兩個隨機部分和總體的隨機誤差獨立不相關)

另外，$\mathbf{u}_j = \{u_0, u_1\}$ 服從分布: 

$$
\mathbf{u}_j | X_{1ij} \sim N(\mathbf{0}, \mathbf{\sum}_{\mathbf{u}})
$$

其中的 $\mathbf{\sum}_{\mathbf{u}}$ 是一個 $2\times2$ 的方差協方差矩陣: 

$$
\begin{aligned}
\text{Where } \mathbf{u}_j & = (u_{0j}, u_{1j})^T \\ 
              \mathbf{0}   & = (0, 0)^T \\ 
              \mathbf{\sum}_{\mathbf{u}} & =\left( \begin{array}{cc}
              \sigma^2_{u_{00}} & \sigma_{u_{01}} \\
              \sigma_{u_{01}}   & \sigma^2_{u_{11}} \\
              \end{array} \right)
\end{aligned}
$$

$e_{ij}$ 則服從下列分布: 

$$
e_{ij} | X_{1ij}, u_{0j}, u_{1j} \sim N(0, \sigma^2_e)
$$

## 繼續 GCSE scores 實例

繼續用 GCSE 數據，去除掉 48 號學校以後，擬合一個固定效應模型 (相同斜率，但是不同的固定截距): 

```{r hie04-3, cache=TRUE, warning=FALSE, message=FALSE}
FIX_inter <- lm(gcse ~ 0 + lrt + factor(school), data = gcse_selected[gcse_selected$school != 48, ])
```

```{r hier04-silence, cache=TRUE, eval=FALSE}
summary(FIX_inter)
```

```
Call:
lm(formula = gcse ~ 0 + lrt + factor(school), data = gcse_selected[gcse_selected$school != 
    48, ])

Residuals:
   Min     1Q Median     3Q    Max 
-28.32  -4.77   0.22   5.08  24.41 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
lrt                0.5595     0.0125   44.63  < 2e-16 ***
factor(school)1    4.0823     0.8806    4.64  3.7e-06 ***
factor(school)2    5.6202     1.0154    5.53  3.3e-08 ***
   ...................                  ............
   ................... <Output ommited> ............
   ...................                  ............
factor(school)62  -0.5566     0.8929   -0.62  0.53306    
factor(school)63   6.4827     1.3734    4.72  2.4e-06 ***
factor(school)64   1.0089     0.9808    1.03  0.30368    
factor(school)65  -1.7701     0.8415   -2.10  0.03547 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 7.52 on 3992 degrees of freedom
Multiple R-squared:  0.442,	Adjusted R-squared:  0.433 
F-statistic: 48.7 on 65 and 3992 DF,  p-value: <2e-16
```

該固定效應模型 (簡單線性回歸模型) 估計的共同斜率是 0.56 (se = 0.01)，和 64 個不同的固定斜率。這些固定斜率的範圍是 -9,63 到 7.91，均值是 0.03，標準差是 3.38。估計的殘差標準差是 `Residual standard error: 7.52`。

如果用相同的數據，我們允許截距發生隨機變動的話 (隨機截距模型): 

```{r hie04-4, cache=TRUE, warning=FALSE, message=FALSE}
MIX_inter <- lmer(gcse ~ lrt + (1 | school), data = gcse_selected[gcse_selected$school != 48, ], REML = TRUE)
summary(MIX_inter)
```

 
隨機截距模型估計的共同斜率還是不變 (0.56, se = 0.01)，總體平均截距是 0.03 (無統計學意義)。截距的 (正態) 分布的標準差是 3.07。殘差標準差，和剛才簡單現行回歸計算的殘差標準差是一樣的 (=7.52)。幾乎所有我們關心的參數估計，都接近簡單線性回歸的結果，但是隨機截距模型使用的參數個數只有 4 個，固定效應模型則用到了 66 個 (很顯然隨機截距模型更加高效)。

接下來，我們進一步擬合本章的重點模型 -- 隨機參數模型:

```{r hie04-5, cache=TRUE, warning=FALSE, message=FALSE}
MIX_coef <- lmer(gcse ~ lrt + (lrt | school), data = gcse_selected[gcse_selected$school != 48, ], REML = TRUE)
summary(MIX_coef)
```

這個模型，不僅允許了隨機的截距，還允許每個直線的斜率成爲隨機的斜率。此時的總體平均截距被估計爲 -0.11 (依然沒有統計學意義)，標準差略微變小 (3.07 變成了 3.04)。總體平均斜率是 0.56，現在也被允許有變動，其標準差是 0.12。此時這些許許多多的估計回歸方程中，斜率和截距的相關系數是 0.49，這十分接近我們在一開始的簡單回歸64次計算獲得的斜率和截距的相關系數 (0.36)。此隨機系數模型的殘差標準差變成了 7.44，略微小於之前的 7.52。這三個模型的結果總結如下表: 



```{r Hier04tab01, echo=FALSE, cache=TRUE, eval=FALSE}
dt <- read.csv("backupfiles/hier04tab1.csv", header = T)
#names(dt) <- c("Model with", "sigma_u", "sigma_e", "sigma_u", "sigma_e")
kable(dt, "html",  align = "l", caption = "表 61.1: Comparison of fixed, random intercept, and random coefficient models: school data") %>%
  kable_styling(bootstrap_options = c("striped", "bordered"),full_width = F, position = "center") %>%
  add_header_above(c(" " = 1, "Fixed effect" = 2, "Random intercept" = 2, "Random coeff."  =2)) %>%
    add_header_above(c(" " = 1, "Model" = 6))
```



<table class="table table-striped table-bordered" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>表 61.1: Comparison of fixed, random intercept, and random coefficient models: school data</caption>
 <thead>
<tr>
<th style="border-bottom:hidden" colspan="1"></th>
<th style="text-align:center; border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;" colspan="6"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px;">Model</div></th>
</tr>
<tr>
<th style="border-bottom:hidden" colspan="1"></th>
<th style="text-align:center; border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;" colspan="2"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px;">Fixed effect</div></th>
<th style="text-align:center; border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;" colspan="2"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px;">Random intercept</div></th>
<th style="text-align:center; border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;" colspan="2"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px;">Random coeff.</div></th>
</tr>
  <tr>
   <th style="text-align:left;"> Parameter </th>
   <th style="text-align:left;"> est </th>
   <th style="text-align:left;"> se </th>
   <th style="text-align:left;"> est </th>
   <th style="text-align:left;"> se </th>
   <th style="text-align:left;"> est </th>
   <th style="text-align:left;"> se </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> $Fixed\; part$ </td>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;">  </td>
  </tr>
  <tr>
   <td style="text-align:left;"> $\beta_0$ </td>
   <td style="text-align:left;"> -0.03 </td>
   <td style="text-align:left;"> - </td>
   <td style="text-align:left;"> 0.031 </td>
   <td style="text-align:left;"> 0.405 </td>
   <td style="text-align:left;"> -0.109 </td>
   <td style="text-align:left;"> 0.403 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> $\beta_1$ </td>
   <td style="text-align:left;"> 0.56 </td>
   <td style="text-align:left;"> 0.013 </td>
   <td style="text-align:left;"> 0.563 </td>
   <td style="text-align:left;"> 0.013 </td>
   <td style="text-align:left;"> 0.557 </td>
   <td style="text-align:left;"> 0.020 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> $Random\; part$ </td>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;">  </td>
  </tr>
  <tr>
   <td style="text-align:left;"> $\sigma_{u_{00}}$ </td>
   <td style="text-align:left;"> 3.38 </td>
   <td style="text-align:left;"> - </td>
   <td style="text-align:left;"> 3.07 </td>
   <td style="text-align:left;"> 0.312 </td>
   <td style="text-align:left;"> 3.041 </td>
   <td style="text-align:left;"> 0.311 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> $\sigma_{u_{11}}$ </td>
   <td style="text-align:left;"> - </td>
   <td style="text-align:left;"> - </td>
   <td style="text-align:left;"> - </td>
   <td style="text-align:left;"> - </td>
   <td style="text-align:left;"> 0.122 </td>
   <td style="text-align:left;"> 0.019 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> $\text{Corr}(0,1)$ </td>
   <td style="text-align:left;"> - </td>
   <td style="text-align:left;"> - </td>
   <td style="text-align:left;"> - </td>
   <td style="text-align:left;"> - </td>
   <td style="text-align:left;"> 0.494 </td>
   <td style="text-align:left;"> 0.149 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> $\sigma_e$ </td>
   <td style="text-align:left;"> 7.522 </td>
   <td style="text-align:left;"> - </td>
   <td style="text-align:left;"> 7.524 </td>
   <td style="text-align:left;"> 0.084 </td>
   <td style="text-align:left;"> 7.442 </td>
   <td style="text-align:left;"> 0.084 </td>
  </tr>
</tbody>
</table>


## 使用模型結果推斷

接下來，我們討論如何比較隨機系數模型，隨機截距模型，也就是如何選擇一個更優的模型。如果只是比較相同數據下，隨機系數模型和隨機截距模型的優劣，那麼只需要同時檢驗 $u_{1j} = 0; \text{Cov}(u_{0j}, u_{1j}) = 0$。

就用剛剛擬合好的 `MIX_inter` 和 `MIX_coef` 來比較: 

```{r hie04-6, cache=TRUE, message=FALSE}
MIX_coef <- lme(fixed = gcse ~ lrt, random =  ~ lrt | school, data = gcse_selected[gcse_selected$school != 48, ], method = "REML")
MIX_inter <- lme(fixed = gcse ~ lrt, random = ~ 1 | school, data = gcse_selected[gcse_selected$school != 48, ], method = "REML")

anova(MIX_inter, MIX_coef)
```

值得注意的是，這裏計算的 似然比的檢驗統計量服從的是一個 自由度爲 2 的卡方分布和一個 自由度爲 1 的卡方分布的混合分布。所以報告中給出的 p 值是一個保守估計，正確的 p 值可以這樣計算: 

```{r hie04-7, cache=TRUE}
likelihood <- as.numeric(-2*(logLik(MIX_inter) - logLik(MIX_coef)))
0.5*(1-pchisq(as.numeric(likelihood), df = 1)) + 0.5*(1-pchisq(as.numeric(likelihood), df = 2))
```

另一個重要的問題是該如何去真正理解這裏隨機系數模型給出的結果呢？

該模型的結果說，"人羣"中的總體均值是 -0.11，總體斜率是 0.56 (se = 0.02, 95%CI: 0.52, 0.60)。這裏的"人羣"指的是全英國/或者全世界這樣的學校。學校水平的截距和斜率服從以這兩個數字爲均值，標準差分別是 3.04 和 0.12 的正態分布。且截距和斜率之間的相關系數接近 0.50。第一層級 (學生的) 個體隨機誤差的標準差爲 7.44。這些結果可以拿來估計"學校人羣"的 95% 截距/斜率: $-0.11 \pm 1.96 \times3.04$ 和 $0.56 \pm 1.96\times0.12$，所以人羣的截距的 95% 信賴區間是: $-6.07, 5.85$，斜率的 95% 信賴區間是: $0.33, 0.80$。與此形成對比的是，我們開頭給 64 所學校建立的 64 個模型的 截距和斜率拿來估計的 95% 截距信賴區間是 $-0.18 \pm 1.96\times3.29: -6.63, 6.27$，95% 斜率信賴區間是 $0.54 \pm 1.96 \times 0.18: 0.19, 0.89$。所以，隨機系數模型對截距和斜率的人羣估計及推斷更加精準。



## 隨機效應的方差 {#random-var}


在解釋混合效應模型的隨機效應部分的時候，有幾點需要注意。首先，隨機截距的方差，和隨機斜率的方差，是具有不同單位的。**隨機截距的方差的單位是結果變量 $Y$ 的單位的平方**。**隨機斜率的方差，是結果變量和解釋變量的單位的商的平方**。

另一個要注意的點是，$Y_{ij}$ 在 $X_{1ij}$ 的條件下的殘差的標準差，不是恆定不變的 (隨着 $X_1$ 的變化而變化): 

$$
\begin{aligned}
Y_{ij} & = (\beta_0 + u_{0j}) + (\beta_1 + u_{1j}) X_{1ij} + e_{ij}  \\ 
       & = (\beta_0 + \beta_1X_{1ij}) + (u_{0j} + u_{1j}X_{1ij} + e_{ij}) \\ 
       & = (\beta_0 + \beta_1X_{1ij}) + \epsilon_{ij}
\end{aligned}
$$

所以從上面的式子可看出，隨機參數模型的**總體殘差 (total residuals)**，$\epsilon_{ij} = u_{0j} + u_{1j}X_{1ij} + e_{ij}$，是隨着你想給斜率隨機性的那個解釋變量的變化而變化的。也正因爲如此，總體殘差的方差，也是隨着解釋變量變化而變化的 (和解釋變量成二次方程關系，如果繪制總體慘差的方差和解釋變量之間的關系會是一個拋物線): 

$$
\begin{aligned}
\text{Var}(Y_{ij} | X_{1ij}) & = \text{Var}( u_{0j} + u_{1j}X_{1ij} + e_{ij}) \\ 
                             & = \sigma^2_{u_{00}} + X_{1ij}^2\sigma^2_{u_{11}} + 2X_{1ij}\sigma_{u_{01}} + \sigma^2_e
\end{aligned}
(\#eq:hier04-3)
$$


## 模型效果評估 

擬合模型的評估中，另一個重要的事是分析第一階層殘差和第二階層殘差是不是符合其前提條件 (正態分布)。記得第二階層殘差獲取之後需要被標準化。

```{r hie04-8, cache=TRUE} 
MIX_coef <- lmer(gcse ~ lrt + (lrt | school), data = gcse_selected[gcse_selected$school != 48, ], REML = TRUE)
School_res0 <- HLMdiag::HLMresid(MIX_coef, level = "school", type = "EB", standardize = FALSE)
summ(School_res0)

School_res1 <- HLMdiag::HLMresid(MIX_coef, level = "school", type = "EB", standardize = TRUE)

summ(School_res1)
```


```{r hier4-level2-residuals, cache=TRUE, echo=FALSE, fig.height=5, fig.width=11, fig.cap='Q-Q plots of school level intercept and slope residuals (unstandardized)', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
ggthemr("fresh")

plot1 <- HLMdiag::ggplot_qqnorm(School_res0[,1], line = "quantile") +
  theme_bw() +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "BLUP r.e level 2 (school) intercept residuals")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8),
        axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank())


plot2 <- HLMdiag::ggplot_qqnorm(School_res0[,2], line = "quantile") +
  theme_bw() +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "BLUP r.e level 2 (school) slope residuals")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8),
        axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank())

ggarrange(plot1, plot2,  
          ncol = 2, nrow = 1)
```



```{r 4-level2sta-residuals, cache=TRUE, echo=FALSE, fig.height=5, fig.width=11, fig.cap='Q-Q plots of school level intercept and slope residuals (standardized)', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
ggthemr("fresh")

plot1 <- HLMdiag::ggplot_qqnorm(School_res1[,1], line = "quantile") +
  theme_bw() +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "BLUP r.e level 2 (school) intercept residuals")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8),
        axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank())


plot2 <- HLMdiag::ggplot_qqnorm(School_res1[,2], line = "quantile") +
  theme_bw() +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "BLUP r.e level 2 (school) slope residuals")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8),
        axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank())

ggarrange(plot1, plot2,  
          ncol = 2, nrow = 1)
```



```{r 4-level1sta-residuals, cache=TRUE, echo=FALSE, fig.height=5, fig.width=11, fig.cap='Histogram and Q-Qf plots of elementary level (pupil) standardized residuals', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
ggthemr("fresh")
gcse <- gcse_selected[gcse_selected$school != 48, ]
MIX_coef <- lme(fixed = gcse ~ lrt, random = ~ lrt | school, data = gcse, method = "REML")

gcse$ehat <-  residuals(MIX_coef, level = 1, type = "normalized")
summ(gcse$ehat, graph = FALSE)

hist <- ggplot(gcse, aes(x = ehat)) +
    geom_histogram(colour = "black", 
                   fill = "lightblue", size = 0.1, binwidth= 0.3)  + 
  theme_bw() + stat_function(fun = function(x) dnorm(x, mean = mean(gcse$ehat), sd = sd(gcse$ehat)) * 4057 * 0.3, color="darkgreen", size = 1) + 
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) 


plot2 <- HLMdiag::ggplot_qqnorm(gcse$ehat, line = "quantile") +
  theme_bw() +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "EB level 1 (pupil) standardized residuals")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8),
        axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank())

ggarrange(hist, plot2,  
          ncol = 2, nrow = 1)
```




## 練習題

1. GCSE data: 數據來自65所中學的學生畢業成績 "the Graduate Certificate of Secondary Education (GCSE) score"，和這些學生在剛剛入學時接受閱讀能力水平測試 (LRT score) 的成績。其變量和各自含義爲：

```
school          school identifier
student         student identifier
gcse            GCSE score (multiplied by 10)
lrt             LRT score (multiplied by 10)
girl            Student female gender (1 = yes, 0 = no)
schgend         type of school (1: mixed gender; 2: boys only; 3: girls only)
```

###　將數據導入軟件裏，



```{r hierex4-02, echo=TRUE, cache=TRUE}
gcse_selected <- read_dta("backupfiles/gcse_selected.dta")

length(unique(gcse_selected$school)) ## number of school = 65

gcse_selected <- gcse_selected %>%
  mutate(schgend = factor(schgend, labels  = c("mixed geder", "boys only", "girls only")))


## create a subset data with only the first observation of each school
gcse <- gcse_selected[!duplicated(gcse_selected$school), ]

# 一共有 65 所學校，54% 是混合校，15% 是男校，31% 是女校
with(gcse, tab1(schgend, graph = FALSE))
  

# 計算每所學校兩種成績的平均分，計算一個包含每所學校的平均女生人數的變量
Mean_gcse_lrt <- ddply(gcse_selected,~school,summarise,mean_gcse=mean(gcse),mean_lrt=mean(lrt), mean_girl=mean(girl)) 

# 整體來說，GCSE 分數的分布比起入學前 LRT 分數的分布更加寬泛，標準差更大。
# 意味着入學時學生閱讀成績的差異，比起畢業時成績的差異要小。
# 或者反過來說，畢業時成績差異，比起入學時閱讀成績的差異要大。
summ(Mean_gcse_lrt[,2:4])
```


### 先忽略學校編號爲 48 的學校，擬合一個只有固定效應 (簡單線性回歸模型)，結果變量是 GCSE，解釋變量是 LRT 和學校。

```{r hierex4-01, echo=TRUE, cache=TRUE}
Fix <- lm(gcse ~ lrt + factor(school), data = gcse_selected[gcse_selected$school !=48, ])
anova(Fix)
```
```{r 10-Hierarchical-models-1, eval=FALSE}
summary(Fix) # 輸出結果太長，中間被省略掉
```

LRT 的回歸系數 (直線斜率 = 0.56, se = 0.01)，殘差的標準差 $\hat\sigma_e =$ 7.52。

```
Call:
lm(formula = gcse ~ lrt + factor(school), data = gcse_selected[gcse_selected$school != 
    48, ])

Residuals:
   Min     1Q Median     3Q    Max 
-28.32  -4.77   0.22   5.08  24.41 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)        4.08232    0.88060    4.64  3.7e-06 ***
lrt                0.55948    0.01253   44.63  < 2e-16 ***
factor(school)2    1.53785    1.34332    1.14  0.25235    
                         ...
                         ...<OMITTED OUTPUT>
                         ...
                         ...
factor(school)65  -5.85245    1.21850   -4.80  1.6e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 7.52 on 3992 degrees of freedom
Multiple R-squared:  0.442,	Adjusted R-squared:  0.433 
F-statistic: 49.4 on 64 and 3992 DF,  p-value: <2e-16
```


### 僅有固定效應模型的學校變量變更爲學校類型 (男校女校或混合校)，從這個新模型的結果來看，你是否認爲學校類型，和學校編號本身相比能夠解釋相同的學校層面的方差？ `lrt` 的估計回歸參數發生了怎樣的變化？

```{r  hierex4-03, echo=TRUE, cache=TRUE}
Fix1 <- lm(gcse ~ lrt + schgend, data = gcse_selected[gcse_selected$school !=48, ])
anova(Fix1)
summary(Fix1)
```


新的模型 `Fix1` 參數明顯減少很多，殘差標準差估計 $\hat\sigma_u =$ 7.99。LRT 的回歸系數估計僅發生了不太明顯的變化 0.59 (0.01)


### 使用限制性極大似然法擬合一個隨機截距模型。記錄此時的限制性對數似然的大小 (log-likelihood)。用 `lmerTest::rand` 命令對隨機效應部分的方差是否爲零做檢驗，指明該檢驗的零假設是什麼，並解釋其結果的含義。


```{r hierex4-04, echo=TRUE, cache=TRUE, message=FALSE}
library(lmerTest)
Fixed_reml <- lmer(gcse ~ lrt +  (1 | school), data = gcse_selected[gcse_selected$school !=48, ], REML = TRUE)
summary(Fixed_reml)
ranova(Fixed_reml) ## random effect test
```

隨機截距模型的輸出結果可以看出，這裏的混合模型估計的 LRT 的回歸系數跟僅有固定效應的簡單線性回歸模型估計的值完全一樣 (0.56, se=0.01)。隨機效應部分 $\hat\sigma_e = 7.524, \hat\sigma_u = 3.07$，此時的限制性似然 (restricted log-likelihood) 是 -14022。最晚部分的隨機效應檢驗的零假設是 $\sigma_u = 0$，且值得注意的是，由於方差本身不可能小於零，故本次檢驗只用到自由度爲 1 的卡方分布的右半側(單側)。也就是說，其替代假設有且只有 $\sigma_u > 0$ 的單側假設。這裏的檢驗結果提示高度有意義 (highly significant)。


### 在前一題的隨機截距模型中加入 `schgend` 變量，作爲解釋隨機截距的一個自變量，觀察輸出結果，解釋其是否有意義。記錄這個模型的限制性似然。

```{r hierex4-05, echo=TRUE, cache=TRUE, message=FALSE}
Fixed_reml1 <- lmer(gcse ~ lrt + schgend +  (1 | school), data = gcse_selected[gcse_selected$school !=48, ], REML = TRUE) 
#Fixed_reml1 <- lme(fixed = gcse ~ lrt + schgend , random =  ~ 1 | school, data = gcse_selected[gcse_selected$school !=48, ], method = "REML") 

summary(Fixed_reml1)

## 檢驗新增的學校種類 schgend 是否對應該加入模型。

mod2<- update(Fixed_reml1, . ~ . - schgend)
anova(Fixed_reml1, mod2)

## 求 Fixed_reml1 的似然

logLik(Fixed_reml1)
```

增加了學校類型在固定效應部分時，隨機效應的標準差從錢一個模型的 3.07 降低到這裏的 2.92。這個變量本身，從最後的模型比較也能看出，對模型的貢獻是有意義的 (p=0.018)。當然從隨機截距模型的輸出結果可以看出，學校類型的這一變量中，可能只有"女校"這一細分部分提供了足夠的效應。這裏的隨機截距模型的REML似然是 (restricted log-likelihood = -14016)

### 擬合隨機截距隨機斜率模型，固定效應部分的 `lrt` 也加入進隨機效應部分。

```{r hierex4-06, echo=TRUE, cache=TRUE}
Fixed_reml2 <- lmer(gcse ~ lrt + schgend +  (lrt | school), data = gcse_selected[gcse_selected$school !=48, ], REML = TRUE)
summary(Fixed_reml2)
logLik(Fixed_reml2)
```

當截距 (不同學校之間， gcse 的起點)，斜率 (不同學校之間 lrt 和 gcse 之間的關系的斜率) 均可以有隨機性以後，`lrt` 的斜率雖然仍然保持不變 $=0.56$，但是它的隨機效應標準差變成了 $=0.12$，隨機截距的標準差也保持不變 $=2.88$，這二者之間的相關系數是 $=0.58$。第一階層隨機殘差標準也有了微妙的變化 $7.52 \rightarrow 7.44$，此模型的限制性對數似然 (restricted log-likelihood) 是 `-13994.393 (df=8)`。

### 通過上面幾個模型計算獲得的似然，嘗試檢驗隨機斜率標準差，以及該標準差和隨機截距標準差的協相關是否有意義。

```{r hierex4-07, echo=TRUE, cache=TRUE}
ranova(Fixed_reml2)

# 手算的方法是這樣的
likelihood <- as.numeric(-2*(logLik(Fixed_reml1) - logLik(Fixed_reml2)))
0.5*(1-pchisq(as.numeric(likelihood), df = 1)) + 0.5*(1-pchisq(as.numeric(likelihood), df = 2))
```

似然比檢驗的統計量是 43.8，不用檢驗也知道肯定是有意義的。手算也是可以達到相同的效果。值得注意的是，R計算給出的基於自由度爲 2 的卡方分布，其實是偏保守的。注意看手算部分，其實用到了自由度爲 1 自由度爲 2 兩個卡方分布換算獲得的 p 值。

### 模型中的 `schgend` 改成 `mean_girl` 會給出怎樣的結果呢？


```{r hierex4-08, echo=TRUE, cache=TRUE}
## 把女生平均值放回整體數據中去
Mean_girl <- NULL
for (i in 1:65) {
  Mean_girl <- c(Mean_girl, rep(Mean_gcse_lrt$mean_girl[i], with(gcse_selected, table(school))[i]))
  }
gcse_selected$mean_girl <- Mean_girl
  rm(Mean_girl)


Fixed_reml3 <- lmer(gcse ~ lrt + mean_girl +  (lrt | school), data = gcse_selected[gcse_selected$school !=48, ], REML = TRUE)
summary(Fixed_reml3)
```

由於 `mean_girl` 其實是和 `schgend` 非常相似的表示學校層面的男女生性別比例的變量，所以這個模型的結果其實和前一個給出的隨機效應標準差的估計都很接近。

### 現在我們把注意力改爲關心學校編號爲 48 的學校的情況。用且禁用它一所學校的數據，擬合一個簡單線性回歸，結果變量是 `gcse`，解釋變量是 `lrt`。

```{r hierex4-09, echo=TRUE, cache=TRUE}
gcse_selected[gcse_selected$school ==48, ]
school48lm <- lm(gcse ~ lrt, data = gcse_selected[gcse_selected$school ==48, ])
summary(school48lm)
```

由於 48 號學校只有兩個數據點，所以強行進行簡單線性回歸的結果，就是擬合了一條通過這兩個點的直線，截距是-32.7，斜率是 -6.9，且沒有任何估計的誤差。

### 這次不排除 48 號學校，擬合所有學校的數據進入 `Fixed_reml2` 模型中去，結果有發生顯著的變化嗎？

```{r hierex4-10, echo=TRUE, cache=TRUE}
Fixed_reml2 <- lmer(gcse ~ lrt + schgend +  (lrt | school), data = gcse_selected, REML = TRUE)
summary(Fixed_reml2)
```

可以看到，即使我們加入這個數據量極少的一個學校的數據，對結果也沒有太大的影響。

### 計算這個模型的第二階級(level 2, `school` level)的殘差。


```{r hierex4-11, echo=TRUE, cache=TRUE}
School_res <- HLMdiag::HLMresid(Fixed_reml2, level = "school")
summ(School_res)
School_res[48, ] 
```

隨機截距的殘差估計範圍在 -6.25 和 5.83 之間，隨機斜率殘差估計範圍在 -0.19 和 0.33 之間。其中 48 號學校的擬合後截距和斜率分別是 -0.74 和 -0.02。48 號學校在這個模型中估計的截距和斜率，與我們單獨對它一所學校擬合模型時的結果大相徑庭。這是因爲在總體的混合效應模型中，該學校的數據被拉近與總體的平均水平。


```{r 4-level2-residuals, cache=TRUE, echo=FALSE, fig.height=5, fig.width=11, fig.cap='Q-Q plots of school level intercept and slope (unstandardized) residuals', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
plot1 <- HLMdiag::ggplot_qqnorm(School_res[,1], line = "quantile") +
  theme_bw() +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "BLUP r.e level 2 (school) intercept residuals")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8),
        axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank())


plot2 <- HLMdiag::ggplot_qqnorm(School_res[,2], line = "quantile") +
  theme_bw() +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "BLUP r.e level 2 (school) slope residuals")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8),
        axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank())

ggarrange(plot1, plot2,  
          ncol = 2, nrow = 1)
```


圖 \@ref(fig:4-level2-residuals) 顯示標準化前的隨機效應部分的殘差表現尚可接受。


### 計算這個模型的第一階級(level 1, student)殘差，分析其分布，查看第48所學校的殘差表現如何。

```{r hierex4-12, echo=TRUE, cache=TRUE, warning=FALSE}
Fixed_reml2 <- lme(fixed = gcse ~ lrt + schgend, random = ~  lrt | school, data = gcse_selected, method="REML") # for extracting standardized level 2 error

gcse_selected$ehat <- residuals(Fixed_reml2, level = 1, type = "normalized")
with(gcse_selected, summ(ehat, graph = FALSE))
#  48 號學校的標準化殘差並不顯得異常
gcse_selected$ehat[gcse_selected$school == 48]
```

```{r 4-level1-residuals, cache=TRUE, echo=FALSE, fig.height=5, fig.width=11, fig.cap='Histogram and Q-Q plots of elementary level (pupil) standardized residuals', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}

hist <- ggplot(gcse_selected, aes(x = ehat)) +
    geom_histogram(colour = "black", aes(y = ..density..), 
                   fill = "lightblue", size = 0.1, binwidth= 0.3)  + 
  theme_bw() + stat_function(fun=dnorm,
                         color="darkgreen", size = 1,
                         args=list(mean=mean(gcse_selected$ehat), 
                                  sd=sd(gcse_selected$ehat)))


plot2 <- HLMdiag::ggplot_qqnorm(gcse_selected$ehat, line = "quantile") +
  theme_bw() +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "EB level 1 (pupil) standardized residuals")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8),
        axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank())

ggarrange(hist, plot2,  
          ncol = 2, nrow = 1)
```



# 縱向研究數據 longitudinal data 1
 
本章我們來把目前爲止了解的混合效應 (截距/斜率) 模型應用到一種特殊形態的數據 -- 縱向研究數據 -- 中去。




縱向數據，是一種前瞻性收集的來的數據，它隨着時間的推移，在不同的時間點對相同的觀察對象進行數據的採集。每個研究對象被收集數據時的時間點，可以是相同的，也可以是不同的。在很多臨牀實驗中，患者被觀察隨訪，並且常常在同樣的時間點收集數據，所以在臨牀實驗的特殊形態下，每個患者收集數據的時間點可以做到統一，這樣的縱向研究數據是屬於**固定測量時刻的類型 (fixed occasions)**。但是在流行病學等觀察性研究中獲得的數據，就沒有這麼幸運，他們通常測量收集數據的時間點就不太可能保持一致，收集時間點不一致的縱向數據屬於**不固定測量時刻的類型 (variable occasions)**。

縱向數據英文名是 longitudinal data，它的常見別的名稱是 重復測量數據 (repeated measures data)，計量經濟學中叫做面板型數據 (panel data)，或者是時間序列橫斷面研究數據 (cross sectional time series data)。所以在縱向數據這種特殊形態的的嵌套式數據結構中，第二層級結構就是一個個的個體，第一層級結構，就是每個個體在不同的時間點獲得的測量值。除了和前面幾章討論過的嵌套式數據結構相似可以應用混合效應模型，縱向數據還有一些自己獨特的性質需要加以考量: 

- 層內數據的相關性結構是有測量時間的先後順序的;
- 之前討論的嵌套式結構數據在層內的觀察值則沒有嚴格的時間或者大小的排序 (例如同一所學校的不同學生);
- 換句話說，層內相關系數 (intra-class correlation) 很難被認爲是相似或者相同的。



## 固定測量時刻 fixed occasions

對於臨牀試驗中固定時刻隨訪收集到的病人數據，理想狀態下應該是一種平衡數據 (balanced data)。也就是在不同時間 $t_i , i = 1, \cdots, n$ 我們成功收集到所有患者的所有數據，所以每層 (名患者) 擁有的時間序列數據的樣本量是相同的 $n_j = n, \forall j$。

如同分析其他類型的數據一樣，分析縱向數據也要從描述數據開始。如果是平衡數據，描述性分析就很容易，當有缺失值時，分析就變得有些棘手。例如，我們可以計算每個時間點的平均值作爲所有患者的 "平均特質 average profiles"。或者也可以用每個人的時間序列數據對時間做簡單線性回歸模型，從而獲取每個個體的截距和斜率。


### 缺失值 Missing data

當縱向數據中存在一些缺失值，即使你在計算一些簡單的歸納性分析，也要**特別特別特別**地小心。如果不是所有人都有全部測量時間點的數據的話，總體的平均特徵數據分析了也沒有太大的卵用，因爲缺失值導致這樣計算獲得的並不是真實的平均值 (也因爲不同的患者，貢獻了不同時間點的數據，沒辦法平均)。

如果存在缺失值，那麼當且僅當這些缺失值和觀測值 $Y$ 之間沒有關系時，才能認爲這些簡單計算和簡單模型的建立是不帶有偏倚的。如果說，有些缺失值確實是根據觀測數據有選擇性地缺失 (the mechanism driving the selection depends on measured data)，隨機效應模型的建立可以自動化校正這樣的缺失，從而保證估計無偏。

根據觀測數據選擇性地出現缺失值的機制被叫做隨機缺失 (Missing at random, MAR)。


#### 隨機截距模型 random intercept model

**復合對稱模型 compound symmetry model**， 是常見的一種用於重復測量數據的模型，它是基於隨機截距模型的一種擴展模型。

當模型中沒有解釋變量時，

$$
\begin{equation}
Y_{ij} = \mu_i + u_{0j} + e_{ij}
\end{equation}
(\#eq:hier05-1)
$$

其中， 

- $i$ 是測量時刻; 
- $j$ 是實驗的個體; 
- $\mu_i$ 是測量時刻 $i$ 時的平均截距 -- 這是一個固定效應。

爲了擬合這個模型，我們需要先生成一系列的啞變量用來表示不同的測量時刻: 

$$
Y_{ij} = \sum_{h=1}^n\beta_{0h} I_{i = h,j} + u_{0j} + e_{ij}
$$

其中，

- $I_{i = h,j}$ 是用於表示第 $j$ 名患者的 $i$ 次觀測值，在第 $h$ 次測量時是否被測量到的啞變量。
- 該模型暗示同一個患者收集到的不同時刻的觀察數據是可以互換的，有相同的協方差 
$$
\begin{aligned}
\text{Cov}(Y_{1j} , Y_{2j}) & = \text{Cov}(u_{0j} + e_{1j}, u_{0j} + e_{2j}) \\ 
                            & = \sigma^2_{u_{00}}
\end{aligned}
$$
- 該模型還有另一個暗示是，不同患者之間任意時間點的兩個觀察數據之間是相互獨立的 
$$
\begin{aligned}
\text{Cov}(Y_{1j}, Y_{2j*}) & = \text{Cov}(u_{0j} + e_{1j}, u_{0j*} + e_{2j*}) \\ 
                            & = 0
\end{aligned}
$$

所以當沒有缺失值時，數據是固定測量時刻 (fixed occation) 的數據也是是平衡數據，那麼每一個患者 (第二層級數據) 的觀察值可以寫作是一個向量 $\{ \mathbf{Y}_{ij} \}$，每名患者的觀察值向量的長度都是相同的 $n$。所以，它們的 $n\times n$ 協方差矩陣就是:  

$$
\Omega_y = \left( \begin{array}{cccc} 
 \sigma^2_{u_{00}} + \sigma^2_e & \sigma^2_{u_{00}}  & \cdots & \sigma^2_{u_{00}} \\
 \sigma_{u_{00}}   & \sigma^2_{u_{00}} + \sigma^2_e    & \cdots & \sigma^2_{u_{00}} \\
 \vdots            & \vdots                            & \vdots & \vdots \\
 \sigma^2_{u_{00}} & \sigma^2_{u_{00}}                &  \cdots & \sigma^2_{u_{00}} + \sigma^2_e\\
\end{array} \right)
$$

也正是由於觀測值的協方差矩陣是如此地對稱，該模型被命名爲復合對稱模型 compound symmetric model。

**Adult height measures 數據**

有(閒人)花了數十年時間追蹤隨訪了近2000名女性在 26 歲，36歲，43歲，53歲時的身高。忽略掉可能存在的測量誤差，研究者想知道是否隨着年齡增加，女性的身高會縮水。這些女性在這些年齡時的身高數據總結如下: 

```{r hier05-01, cache=TRUE}
height <- read_dta("backupfiles/height.dta")
summ(height[, 2:5])
```


原則上每個女性在所有的時間應該都有身高測量值才對，我們暫且認爲擁有缺失測量值的時間點是完全隨機的。先計算樣本中數據完整部分的女性身高在四個時間點時的方差協方差矩陣: 

```{r hier05-02, cache=TRUE}
var(height[, 2:5], use = "complete.obs")
```

要給這個數據擬合混合對稱模型 (compound symmetry model)，需要先把數據從寬變長，之後爲每個測量身高的時間點生成一個啞變量，然後擬合無截距式的隨機截距模型: 


```{r hier05-03, cache=TRUE}
# 把數據格式從寬變長
hei_long <- height %>%
  gather(key, value, -id, -bw, -mht) %>%
    separate(key, into = c("Height", "H_Age"), sep = 2) %>%
      arrange(id, H_Age, bw, mht) %>%
        spread(Height, value)

# 生成四個年齡時間點數據的啞變量
hei_long <- hei_long %>%
  mutate(Age_1 = ifelse(H_Age == 26, 1, 0), 
         Age_2 = ifelse(H_Age == 36, 1, 0),
         Age_3 = ifelse(H_Age == 43, 1, 0),
         Age_4 = ifelse(H_Age == 53, 1, 0))
M_hei <- lmer(ht ~ 0 + Age_1 + Age_2 + Age_3 + Age_4 + (1 | id), data = hei_long, REML = TRUE)
summary(M_hei)

# 檢驗三個年齡點的身高均值是否相同用下面的方法: 
linearHypothesis(M_hei, c("Age_1 - Age_2 = 0", 
                          "Age_1 - Age_3 = 0", 
                          "Age_1 - Age_4 = 0"))
```

所以，用這個模型 (符合對稱模型 compound symmetry model)，其實我是在告訴 R 軟件說，我認爲，這個數據中的女性四次測量的身高之間的方差協方差矩陣是這樣紙的 (因爲 $5.992^2 = 35.91; 1.409^2 = 1.99$): 

$$
\Omega_y = \left( \begin{array}{cccc} 
 37.90 & 35.91  & 35.91 & 35.91 \\
 35.91 & 37.90  & 35.91 & 35.91 \\
 35.91 & 35.91  & 37.90 & 35.91 \\
 35.91 & 35.91  & 35.91 & 37.90\\
\end{array} \right)
$$


分析這個模型第二層階級殘差，和第一層階級殘差可以計算並做圖 \@ref(fig:5-level2-res) \@ref(fig:5-level1-res) 如下: 

```{r hier05-04, cache=TRUE}
# refit the model with lme
M_hei <- lme(fixed = ht ~ 0 + Age_1 + Age_2 + Age_3 + Age_4, random = ~ 1 | id, 
             data = hei_long, method = "REML", na.action=na.omit)
# individual level standardized residuals
ehat_st <- residuals(M_hei, type = "normalized", level = 1)

# extract the EB uhat (level 2 EB residual)
uhat_eb <- ranef(M_hei)$`(Intercept)`

# standardized level 2 residuals
### count number of measures for each women
Nmeas <- 4
### shrinkage factor 
R = 5.992^2/(5.992^2 + 1.409^2/Nmeas)
### use shrinkage factor calculate variance of uhat_eb
var_eb <- R * 5.992^2
### standardize uhat
uhat_st <- uhat_eb/sqrt(var_eb)
```



```{r 5-level2-res, cache=TRUE, echo=FALSE, fig.height=5, fig.width=11, fig.cap='Standardized cluster level residuals (intercept) from the compound symmetry model', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
ggthemr("fresh")

uhat_st <- as.data.frame(uhat_st)

hist <- ggplot(uhat_st, aes(uhat_st)) + 
    geom_histogram(colour = "black", aes(y = ..density..), 
                   size = 0.1, binwidth= 0.2)  + 
             stat_function(fun=dnorm, size = 1,
                         args=list(mean=mean(uhat_st$uhat_st), 
                                  sd=sd(uhat_st$uhat_st))) + 
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "Level 2 cluster standardized residuals (EB)")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8))



plot2 <- HLMdiag::ggplot_qqnorm(uhat_st$uhat_st, line = "quantile") +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "Level 2 cluster standardized residuals (EB)")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8))

ggarrange(hist, plot2,  
          ncol = 2, nrow = 1)
```

```{r 5-level1-res, cache=TRUE, echo=FALSE, fig.height=5, fig.width=11, fig.cap='Standardized elementary level residuals from the compound symmetry model', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
ggthemr("fresh")

ehat_st <- as.data.frame(ehat_st)

hist <- ggplot(ehat_st, aes(ehat_st)) + 
    geom_histogram(colour = "black", aes(y = ..density..), 
                   size = 0.1, binwidth= 0.2)  + 
             stat_function(fun=dnorm, size = 1,
                         args=list(mean=mean(ehat_st$ehat_st), 
                                  sd=sd(ehat_st$ehat_st))) + 
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "Level 1 standardized residuals")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8))



plot2 <- HLMdiag::ggplot_qqnorm(ehat_st$ehat_st, line = "quantile") +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "Level 1 standardized residuals (EB)")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8))

ggarrange(hist, plot2,  
          ncol = 2, nrow = 1)
```


混合對稱模型的前提假設實在是太強了 (它假定個體內的方差保持不變，且個體間的協方差也保持不變)。你我都清楚，當考慮了時間以後，同一個體在時間上比較接近的點測量之間會更相似，也更相關。


#### 隨機參數模型 random intercept and slope model

實際上有多種方法可以放鬆混合對稱模型對方差和協方差的約束性前提，其中之一是在隨機截距模型中允許有隨機斜率成分。


使用隨機參數模型擬合縱向數據時的簡單模型如下: 

$$
Y_{ij} = (\beta_0 + u_{0j}) + (\beta_1 + u_{1j})t_i +e_{ij}
$$

前一章討論過 (滾回 \@ref(random-var))，這裏隨機參數模型的解釋變量是時間 $t_i$，導致的結果之一是觀測值的方差其實是隨着時間變化而變化的 (拋物線關系):

$$
\begin{aligned}
\text{Var}(Y_{ij}) & = \text{Cov}(u_{0j} + u_{ij}t_i + e_{ij}, u_{0j} + u_{ij}t_i + e_{ij})  \\ 
                   & = \sigma^2_{u_{00}} + \sigma^2_{u_{11}}t_i^2 + 2t_i\sigma_{u_{01}} + \sigma^2_e
\end{aligned}
$$

同時，同一患者不同時間測量的觀測值之間的協方差是: 

$$
\begin{aligned}
\text{Cov}(Y_{1j}, Y_{2j}) & = \text{Cov}(u_{0j} + u_{1j}t_1 + e_{1j}, u_{0j} + u_{2j}t_2 + e_{2j}) \\ 
& = \sigma^2_{u_{00}} + \sigma^2_{u_{11}}t_1t_2 + \sigma_{u_{01}}(t_1 + t_2)
\end{aligned}
$$

不同患者任意測量時刻之間的協方差是: 

$$
\begin{aligned}
\text{Cov}(Y_{1j}, Y_{2j*}) & = \text{Cov}(u_{0j} + u_{1j}t_1 + e_{1j}, u_{0j*} + u_{2j*}t_2 + e_{2j*}) \\ 
& = 0
\end{aligned}
$$


**Adult height measures 數據**

利用上面的理論，來對身高數據擬合另一個混合效應模型:

```{r hier05-06, cache=TRUE}
# 對年齡中心化到以 26 歲爲起點
hei_long <- hei_long %>%
  mutate(age = as.numeric(H_Age) - 26)
M_hei_ran <- lme(fixed = ht ~ age, random = ~ age | id, data = hei_long, method = "REML", na.action = na.omit)
#M_hei_ran <- lmer(ht ~ age + (age | id), data = hei_long, REML = TRUE)
summary(M_hei_ran)
```


這個混合效應模型同時包含了隨機截距和隨機斜率兩個部分。你可以用 LRT 比較它和一個只有隨機截距的模型哪個更好，但是我們沒有辦法比較它和混合對稱模型哪個更優於擬合這個數據 (因爲他們的固定效應部分不同，在 REML 方法下實際二者擬合的數據是不同的)。這個隨機系數模型和前一個混合對稱模型都給出了身高隨着年齡增加而減少的相同結論。不同的是，隨機系數模型把同一對象內不同時間觀測值之間的等協方差的約束條件給放開了，因爲用腳趾頭想也知道**同一個人不同時間測量的數據之間的協方差會隨着時間跨度不同而發生改變**。

根據隨機系數模型給出的報告，計算模型估計的觀測值 (身高的4個時間點) 的方差協方差矩陣: 

$$
\begin{aligned}
\hat{\text{Cov}}(Y_{1j}, Y_{2j}) & = \sigma^2_{u_{00}} + \sigma^2_{u_{11}}t_1t_2 +\sigma_{u_{01}} (t_1 + t_2) \\
 & = 6.1588^2 + 0.0599^2t_1t_2 + (-0.28)\times6.1588\times0.0599 (t_1 + t_2)\\ 
 & = 37.93 + 0.004\times t_2 \times t_2 - 0.104 \times(t_1 + t_2) \\
\hat{\text{Var}} (Y_1j) & = \sigma^2_{u_{00}} + \sigma^2_{u_{11}}t_1^2 - 2\sigma_{u_{01}}t_1 + \sigma_e^2 \\ 
& = 37.93 + 0.004 \times t_1^2 - 0.104\times2\times t_1 + 1.59
\end{aligned}
$$

所以，當 $t_1 = 0, t_2 = 10, t_3 = 17, t_4 = 27$ 時，

$$
\mathbf{\hat{\Sigma}_u} =  \left( \begin{array}{cccc} 
 39.52 & 36.90  & 36.17 & 35.14 \\
 36.90 & 37.81  & 35.75 & 35.07 \\
 36.17 & 35.75  & 37.03 & 35.03 \\
 35.14 & 35.07  & 35.03 & 36.54 \\
\end{array} \right)
$$


```{r 5-level2-ress, cache=TRUE, echo=FALSE, fig.height=5, fig.width=11, fig.cap='UN-Standardized cluster level residuals (intercept and slope) from the random intercept and slope model', fig.align='center', out.width='80%', message=FALSE, warning=FALSE, eval=TRUE}
# refit the model with lme
M_hei_ran <- lme(fixed = ht ~ age, random = ~ age | id, data = hei_long, method = "REML", na.action = na.omit)
# individual level standardized residuals
ehat_st <- residuals(M_hei_ran, type = "normalized", level = 1)

# extract the EB uhat (level 2 EB residual) intercept and slope
uhat_eb <- ranef(M_hei_ran)


# standardized level 2 residuals
### count number of measures for each women
Nmeas <- 4
### shrinkage factor 
R1 = 6.15884 ^2/(6.15884 ^2 + 1.25921^2/Nmeas)
R2 = 0.05993^2/(0.05993^2 + 1.25921^2/Nmeas)
### use shrinkage factor calculate variance of uhat_eb
var_eb_inter <- R1 * 6.15884 ^2
var_eb_slope <- R2 * 0.05993
### standardize uhat
uhat_eb$inte_st <- uhat_eb$`(Intercept)`/sqrt(var_eb_inter)
uhat_eb$slop_st <- uhat_eb$age/sqrt(var_eb_slope)


ggthemr("fresh")

plot_u0hat <- HLMdiag::ggplot_qqnorm(uhat_eb$`(Intercept)`, line = "quantile") +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "Level 2 cluster  residuals (EB) of intercept")  +
  theme(axis.title = element_text(size = 14), axis.text = element_text(size = 8))

plot_u1hat <- HLMdiag::ggplot_qqnorm(uhat_eb$slop_st, line = "quantile") +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "Level 2 cluster  residuals (EB) of slope")  +
  theme(axis.title = element_text(size = 14), axis.text = element_text(size = 8))


ggarrange(plot_u0hat, plot_u1hat,  
          ncol = 2, nrow = 1)
```


```{r 5-level1-res0, cache=TRUE, echo=FALSE, fig.height=5, fig.width=11, fig.cap='Standardized elementary level residuals from the random intercept and slope model', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
ggthemr("fresh")
ehat_st <- residuals(M_hei_ran, type = "normalized", level = 1)

ehat_st <- as.data.frame(ehat_st)

hist <- ggplot(ehat_st, aes(ehat_st)) + 
    geom_histogram(colour = "black", aes(y = ..density..), 
                   size = 0.1, binwidth= 0.2)  + 
             stat_function(fun=dnorm, size = 1,
                         args=list(mean=mean(ehat_st$ehat_st), 
                                  sd=sd(ehat_st$ehat_st))) + 
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "Level 1 standardized residuals")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8))



plot2 <- HLMdiag::ggplot_qqnorm(ehat_st$ehat_st, line = "quantile") +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "Level 1 standardized residuals (EB)")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8))

ggarrange(hist, plot2,  
          ncol = 2, nrow = 1)
```


## 不固定測量時刻 variable occasions

當重復收集的數據不是平衡數據時，意味着不同的人數據的收集時間點不一樣，我們就無法像前面那樣用協方差矩陣的方式來描述不同人不同時間點之間測量值可能存在的相關性，也沒有辦法給每個時間點所有人的數據做平均值作爲全部人的平均特質。

但是我們可以把不固定測量時刻的不平衡數據看作是受缺失值數據影響的平衡數據 (unbalanced data can be thought of as balanced data affected by missingness)。所以需要特別小心謹慎，因爲用線性混合效應模型擬合這樣的數據，其實是在含蓄地假設那些應該出現但是沒有出現的測量值的缺失是隨機的。


**Asian growth data 實例**

在本部分開頭的章節介紹過，這是一個收集了亞洲兒童在 6 周，8 個月，12 個月，和 27 個月大時的體重數據。



```{r  Hier05-07, cache=TRUE, echo=FALSE, fig.height=6, fig.width=9, fig.cap='Growth profiles of boys and girls in the Asian growth data', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
growth <- read_dta("backupfiles/asian.dta")
growth <- growth %>%
  mutate(gender = factor(gender, labels= c("Boys", "Girls")))

ggthemr('fresh')

G <- ggplot(growth, aes(x = age, y = weight)) + 
 geom_line(aes(group = id), lty = 1) + 
   theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "Age (years)", y = "Weight (Kg)")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8),
        axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank())
G + facet_grid(. ~ gender) + 
  theme(strip.text = element_text(face = "bold", size = rel(1.5)))
```

如圖 \@ref(fig:Hier05-07) 所示，觀察男孩女孩的體重隨着時間的變化，似乎暗示男孩子體重增加的速度較高，且男孩中體重增加的差異 (方差) 似乎也較女孩子的體重增加曲線來得大。另外，體重和年齡的關系並不是線性的，而且，這些數據中有缺失值。

**隨機截距模型**

第一個想到的合適模型應該包括一個隨機截距，一個固定效應的線性和拋物線性的年齡項，還有最後一個啞變量用以區分男孩和女孩: 

$$
Y_{ij} = (\beta_0 + u_{0j}) + \beta_1t_{ij} + \beta_2 t_{ij}^2 + \beta_3 \text{girl}_j + e_{ij}
$$

在 R 裏擬合這個模型: 

```{r Hier05-08, cache=TRUE}
growth <- growth %>%
  mutate(age2 = age^2)

M_growth <- lme(fixed = weight ~ age + age2 + gender, random = ~ 1 | id, data = growth, method = "REML", na.action = na.omit)
summary(M_growth)

## 由於樣本量較小，這裏如果使用極大似然法估計 ML，結果就和 REML 估計的隨機效應的方差部分不太相同
M_growthml <- lme(fixed = weight ~ age + age2 + gender, random = ~ 1 | id, data = growth, method = "ML", na.action = na.omit)
summary(M_growthml)
```

**隨機截距和斜率模型**

此時我們再來用相同的數據擬合混合效應模型，現在允許線性年齡的斜率有隨機變化: 

```{r Hier05-09, cache=TRUE}
M_growth_mix <- lme(fixed = weight ~ age + age2 + gender, random = ~ age | id, data = growth, method = "REML", na.action = na.omit)
summary(M_growth_mix)
```

這裏可以看到隨機殘差 (residuals) 的標準差 (`StdDev`) 部分在後者(混合系數模型)中明顯變小了 $(0.74\rightarrow 0.54)$。另外，第二層級殘差和第一層級殘差 (未標準化) 如圖 \@ref(fig:hier05-10) 和 \@ref(fig:hier05-11):



```{r hier05-10, cache=TRUE, echo=FALSE, fig.height=5, fig.width=11, fig.cap='UN-Standardized cluster level residuals (intercept and slope) from the random intercept and slope model', fig.align='center', out.width='80%', message=FALSE, warning=FALSE, eval=TRUE}

uhat_eb <- ranef(M_growth_mix)


ggthemr("fresh")

plot_u0hat <- HLMdiag::ggplot_qqnorm(uhat_eb$`(Intercept)`, line = "quantile") +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "Level 2 cluster  residuals (EB) of intercept")  +
  theme(axis.title = element_text(size = 14), axis.text = element_text(size = 8))

plot_u1hat <- HLMdiag::ggplot_qqnorm(uhat_eb$age, line = "quantile") +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "Level 2 cluster  residuals (EB) of slope")  +
  theme(axis.title = element_text(size = 14), axis.text = element_text(size = 8))


ggarrange(plot_u0hat, plot_u1hat,  
          ncol = 2, nrow = 1)
```



```{r hier05-11, cache=TRUE, echo=FALSE, fig.height=5, fig.width=11, fig.cap='Standardized elementary level residuals from the random intercept and slope model', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
ggthemr("fresh")
ehat_st <- residuals(M_growth_mix, type = "normalized", level = 1)

ehat_st <- as.data.frame(ehat_st)

hist <- ggplot(ehat_st, aes(ehat_st)) + 
    geom_histogram(colour = "black", aes(y = ..density..), 
                   size = 0.1, binwidth= 0.2)  + 
             stat_function(fun=dnorm, size = 1,
                         args=list(mean=mean(ehat_st$ehat_st), 
                                  sd=sd(ehat_st$ehat_st))) + 
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "Level 1 standardized residuals")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8))



plot2 <- HLMdiag::ggplot_qqnorm(ehat_st$ehat_st, line = "quantile") +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "Level 1 standardized residuals (EB)")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8))

ggarrange(hist, plot2,  
          ncol = 2, nrow = 1)
```



## 預測軌跡 predicting trajectories

比較只有隨機截距模型，和隨機系數模型給出的擬合曲線是否有差異 如圖\@ref(fig:hier05-12)，其實差異十分微小。可以用下面的 R 代碼: 

```{r hier05-12,  cache=TRUE, echo=TRUE, fig.height=5, fig.width=8, fig.cap='Observed weight and predicted growth profiles of four babies in the Asian growth data', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
growth$traj2 <- fitted(M_growth_mix) 
growth$traj1 <- fitted(M_growth) 

G <- ggplot(growth[growth$id %in% c(258,1141,3148,287),], aes(x = age, y = weight)) + geom_point(shape = 19, size = 4) + 
 # geom_line(aes(y = traj1)) + 
#  geom_line(aes(y = traj2), linetype = 2) +
  stat_smooth(method = "lm", aes(y = traj1), formula = y ~ x + I(x^2), se = F, linetype = 2) + 
  stat_smooth(method = "lm", aes(y = traj2), formula = y ~ x + I(x^2), se = F)  +
   theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8))


G +  facet_wrap( ~ id, ncol = 2) + 
  theme(strip.text = element_text(face = "bold", size = rel(1.5)))
```


## Practical 05-Hier


# 縱向研究數據 longitudinal data 2

本章沒有代碼，學會如何用矩陣標記法寫下你的多元混合效應模型。

## 邊際結構 marginal structures

至此，我們接觸過的各種混合效應模型其實代表的是數據不同的邊際結構關系 (marginal relations)。

### 隨機截距模型

縱向數據中，數據可能是平衡或不平衡數據，簡單的隨機截距模型可以標記如下: 

$$
Y_{ij} = (\beta_0 + u_{0j}) + \beta_1 t_{ij} + e_{ij}
$$

這個模型隱含着如下的條件關系 (conditional relation):

$$
\begin{aligned}
Y_{ij} | t_{ij}, u_{0j} & \sim N(\beta_0 + \beta_1t_{ij} + u_{0j}, \sigma^2_e)\\ 
 u_{0j}|t_{ij} & \sim N(0, \sigma^2_u) \\
 \text{Var}(Y_{ij} | t_{ij}, u_{0j})  & = \sigma^2_e
\end{aligned}
$$

也就是說，觀測值 $Y_{ij}$ 以時間 $t$，和隨機截距 $u_0$ 爲條件的方差，只取決於 $\sigma^2_e$。所以，屬於同一層 (同一患者不同測量時間) 的測量值，以該層 (患者) 的截距爲條件 (conditional on $u_j$) 的協方差是 $\text{Cov} (Y_{ij}, Y_{i*j}|t_{ij}, t_{i*j}, u_j) = 0$。

$Y_{ij}$ 針對 $u_j$ 的邊際期望 (marginal espectation with respect to $u_j$): 

$$
E(Y_{ij}|t_{ij}) = \beta_0 + \beta_1 t_{ij}
$$

其方差爲 $\text{Var}(Y_{ij}|t_{ij}) = \sigma^2_u + \sigma^2_e$，同一層 (同一患者) 的兩個不同時刻測量值之間的邊際協方差就是 $\text{Cov}(Y_{ij}, Y_{i*j}|t_{ij},t_{i*j}) = \sigma^2_u$。

### 隨機系數模型

模型的數學標記是

$$
Y_{ij} = (\beta_0 + u_{0j}) + (\beta_1 + u_{1j})t_{ij} + e_{ij}
$$

等同於

$$
Y_{ij} = (\beta_0 + \beta_1t_{ij}) + (u_{0j} + u_{1j}t_{ij}) + e_{ij}
$$


其**條件關系**是 

$$
Y_{ij}|t_{ij},u_{0j},u_{1j} \sim N( \beta_0 + \beta_1t_{ij} + u_{0j} + u_{1j}t_{ij}, \sigma^2_e)
$$

其中， $\mathbf{u}_j|t_{ij} \sim N(0, \mathbf{\Sigma}_u)$，且

$$
\mathbf{\sum}_{\mathbf{u}}  =\left( \begin{array}{cc}
              \sigma^2_{u_{00}} & \sigma_{u_{01}} \\
              \sigma_{u_{01}}   & \sigma^2_{u_{11}} \\
              \end{array} \right)
\\
\text{Cov} (Y_{ij}, Y_{i*j}|t_{ij}, t_{i*j}, u_{oj}, u_{1j}) = 0
$$

其所指的$Y_{ij}$的邊際分布: 

$$
\begin{aligned}
E(Y_{ij}|t_{ij})   & = \beta_0 + \beta_1t_{ij} \\
\text{Var}(Y_{ij}) & = \sigma^2_{u_{00}}  +2\sigma_{u_{01}}t_{ij} + \sigma^2_{u_{11}}t_{ij}^2 + \sigma^2_e \\
\text{Cov}(Y_{ij}, Y_{i*j}) & = \text{Cov}(u_{0j} + u_{1j}t_{ij} + e_{ij}, u_{0j} + u_{1j}t_{i*j} + e_{i*j}) \\
                   & = \sigma^2_{u_{00}} + \sigma_{u_{01}}(t_{ij} + t_{i*j}) + \sigma^2_{u_{11}}t_{ij}t_{i*j} \text{ (for } i \neq i*) \\
\text{Cov}(Y_{ij}, Y_{i*j*}) & = \text{Cov}(u_{0j} + u_{1j}t_{ij} + e_{ij}, u_{0j*} + u_{1j*}t_{i*j*} + e_{i*j*}) \\ 
& = 0 \text{ (for } j \neq j*) 
\end{aligned}
$$

也就是說**同一層 (同一患者) 的不同測量值之間的協方差不爲零，是時間的函數**。

## 矩陣記法

如果數據本身是**平衡數據**，可以用如下的矩陣標記混合效應模型，

- $j$ 是每個患者 (第二層級)，$\mathbf{Y}_j, \mathbf{e}_j$ 向量被定義爲: 

$$
\begin{aligned}
\mathbf{Y}_j & =  \left( \begin{array}{c}
Y_{1j} \\
Y_{2j} \\
\cdots \\
\cdots \\
Y_{nj}
\end{array}
\right) \\
\mathbf{e}_j & =  \left( \begin{array}{c}
e_{1j} \\
e_{2j} \\
\cdots \\
\cdots \\
e_{nj}
\end{array}
\right) \\
\end{aligned}
$$

用三次測量時間 $t_1, t_2, t_3$ (以簡便標記) 來繼續接下來的推導，定義矩陣 $\mathbf{T}, \mathbf{\beta}, \mathbf{u}_j$: 

$$
\mathbf{T} = \left(\begin{array}{c}
1 & t_1 \\
1 & t_2 \\
1 & t_3 
\end{array}
\right) \\
\mathbf{\beta} = \left( \begin{array}{c}
\beta_0 \\
\beta_1 
\end{array}
\right) \\
\mathbf{u}_j = \left(\begin{array}{c}
u_{0j} \\
u_{1j} 
\end{array}
\right)
$$

如此經過利用定義好的向量，我們就可以把模型用矩陣標記來記錄，從無窮無盡的下標中解放出來: 

$$
\mathbf{Y = T\beta + Tu + e} \\ 
\text{Where } \mathbf{u} \sim N(0, \mathbf{\Sigma}_u) \\ 
              \mathbf{e} \sim N(0, \sigma^2_e\mathbf{I})
$$

那麼 

$$
\text{Var}(\mathbf{Y}) = \mathbf{T\Sigma}_u\mathbf{T}^T + \sigma^2_e \mathbf{I}
$$

## 混合效應模型的一般化公式

前面的例子用的雖然是時間做解釋變量 (縱向數據)，但是也可以推廣到一般的混合效應模型: 

$$
\mathbf{Y = T\beta + Zu + e}
$$

其中 $\mathbf{Z}$ 是類似 $\mathbf{T}$ 的共變量矩陣。類似地，$\mathbf{Y}$ 的方差是: 

$$
\text{Var}(\mathbf{Y}) = \mathbf{Z\Sigma}_u\mathbf{Z}^T + \mathbf{\Sigma}_e \\
\mathbf{Y} \sim N(\mathbf{T\beta}, \mathbf{Z\Sigma}_u\mathbf{Z}^T + \mathbf{\Sigma}_e )
$$

這就是一個多元線性混合效應回歸模型，大多數情況下，$\mathbf{\Sigma}_e = \sigma^2_e\mathbf{I}$。


## 其他可選擇的方差協方差矩陣特徵 

學會了上面的矩陣標記以後，就應該了解在這樣的多元混合效應模型中，對於層內方差，協方差矩陣的 $\mathbf{\Sigma_u}$ 結構初步假設是相當重要的。目前爲止我們接觸過的模型的方差協方差矩陣結構列舉如下 (爲了簡便標記都用$3\times3$ 的矩陣來表示): 

- 復合對稱結構 (compound symmetry structure - compound symmetry model) 又名爲可交換結構 (exchangeable structure)

$$
\mathbf{\sum}_{\mathbf{u}}  =\left( \begin{array}{cc}
              \sigma^2_{u} + \sigma^2_e & \sigma^2_{u}             &  \sigma^2_{u} \\
              \sigma^2_{u}              & \sigma^2_{u} + \sigma^2_e& \sigma^2_{u}  \\
              \sigma^2_{u}              & \sigma^2_{u} & \sigma^2_{u} + \sigma^2_e \\
              \end{array} \right)
$$

- 隨機系數結構 random coefficient (RC) structure

$$
\mathbf{\sum}_{\mathbf{u}}  =\left( \begin{array}{cc}
              \sigma^2_{u_{00}} + \sigma^2_e       & \sigma^2_{u_{00}} + \sigma_{u_{01}} &  \sigma^2_{u_{00}} + 2\sigma_{u_{01}} \\
              \sigma^2_{u_{00}} + \sigma_{u_{01}}  & \sigma^2_{u_{00}} + 2\sigma_{u_{01}} + \sigma^2_{u_{11}} + \sigma^2_e& \sigma^2_{u_{00}} + 3\sigma_{u_{01}} + 2\sigma^2_{u_{11}}  \\
              \sigma^2_{u_{00}} + 2\sigma_{u_{01}} & \sigma^2_{u_{00}} + 3\sigma_{u_{01}} + 2\sigma^2_{u_{11}} & \sigma^2_{u_{00}} + 4\sigma_{u_{01}} + 4\sigma^2_{u_{11}}+\sigma^2_e \\
              \end{array} \right)
$$

除了這兩個結構以外其他常見方差寫方差結構還有: 

- 自回歸結構 (autoregressive structure): 

$$
\frac{\phi}{1-\alpha^2} \left(\begin{array}{ccc}
1 & \alpha & \alpha^2 \\
\alpha & 1  & \alpha \\
\alpha^2 & \alpha  & 1
\end{array}
\right)
$$

- 無固定結構 (unstructure): 


$$
\left(\begin{array}{ccc}
\sigma_{11} & \sigma_{12}  &\sigma_{13} \\
\sigma_{21} & \sigma_{22}  &\sigma_{23} \\
\sigma_{31} & \sigma_{32}  &\sigma_{33}
\end{array}
\right)
$$

最後不要忘記了還有完全獨立結構 (不需要任何復雜模型或校正其數據間的依賴性): 

$$
\sigma^2\left(\begin{array}{ccc}
1 & 0  & 0 \\
0 & 1  & 0 \\
0 & 0  & 1
\end{array}
\right)
$$

## 其他要點評論

- 各種結構模型之間的相互比較 

    - 似然比檢驗法 the likelihood ratio test (LRT) <br> 前提是模型的固定結構不發生改變，兩個嵌套式模型之間的比較是可以使用死然比檢驗的。缺點是統計學效能可能不太理想 (low power) 
    - 模型的比較指標 information criteria <br> 就算是同一個數據，如果不同的協方差結構矩陣模型的固定效應部分也不同，似然比檢驗也不使用，這時候應該求助於赤池信息量 (Akaike's Information Criterion, AIC)，或者貝葉斯信息量 (Bayesian Criterion, BIC) 的比較。這兩個信息量都是使用的模型的似然減去相應模型的參數數量作爲評判標準。差別是 BIC 對參數的調整更加大些。但是，沒人可以保證這些信息會永遠相互認證，他們可能出現互相矛盾，也沒人可以保證使用這些信息的比較可以證明你的模型是"最佳"模型。
    
## 不平衡數據

- 有缺失值的數據，我們無法使用已知的協方差結構矩陣; 
- 隨機效應模型，隨機系數模型可以用於不平衡數據，所以即使有缺失值，我們可以從混合效應模型的結果來推測數據暗示我們數據中存在着怎樣的協方差結構;


## Practical 06-Hier


# 縱向研究數據 longitudinal data 3

## 第一層級的異質性 level 1 heterogeneity

目前爲止，我們使用討論過的模型，其實還默認另一個前提條件: 第一層級和第二層級的隨機誤差的方差是固定不變的 (level 1 and level 2 error variance are constant)。但是實際上我們可以把這個條件放寬，讓模型允許第一層級隨機誤差的方差根據某個解釋變量而不同，使得模型更加接近數據，這種模型被命名爲 **復雜第一層級方差模型 (complex level 1 variation)**。下面繼續使用 Asian growth data 來做說明。該數據測量了幾百名亞洲兒童在0-3歲之間幾個時間點的體重。現在我們來允許其第一層級 (每一個兒童在不同時間點測量的體重) 誤差方差隨着性別的不同而變化: $\sigma_e = f(\text{gender})$。這裏的方程爲了防止標準差變成負的而使用對數函數: 

$$
\text{log} (\sigma_e) = \delta_1I_{\text{gender = boy}} + \delta_2I_{\text{gender=girl}}
$$

這個加入了第一層級方差隨機性的模型在 R 裏可以這樣擬合: 

```{r Hier07-01, cache=TRUE}
M_growth_l1 <- lme(fixed = weight ~ age + age2 + gender, random = ~ age | id, weights = varIdent(form=~1|gender), data = growth, method = "REML", na.action = na.omit)
summary(M_growth_l1)

# 和之間默認男女兒童的誤差方差相等時的模型做比較
# 沒有顯著差異 (p = 0.09)
anova(M_growth_l1, M_growth_mix)
```

## 第二層級異質性 level 2 heterogeneity

我們還可以在模型中允許第二層級的結構不一樣，這等同於認爲這是一個三個層級的模型，其中第二層級分裂成男孩和女孩。

```{r Hier07-02, cache=TRUE}
M_growth_l2 <- lme(fixed = weight ~ age + age2 + gender, 
                   random = ~ age*gender | id,
                   data = growth, method = "REML", na.action = na.omit)
summary(M_growth_l2)

growth <- growth %>%
  mutate(boy = as.numeric(gender == "Boys"), 
         girl = as.numeric(gender == "Girls")) %>%
  mutate(age_boy = age*boy, 
         age_girl = age*girl)         

#M <- lmer(weight ~ age + age2 + girl + (age_boy |id) + (age_girl| id), data = growth, REML = TRUE)

#growth <- growth %>%
#  mutate(boy = ifelse(gender == "Boys", 1, 0), 
#         girl = ifelse(gender == "Girls", 1, 0), 
#         age_boy = age*boy, 
#         age_girl = age*girl)
#M_growth_l22 <- lme(fixed = weight ~ age + age2 + girl, 
#                    random = list( ~ girl + age_girl | id, 
#                                   ~ boy + age_boy | id),
#                   data = growth, method = "REML", na.action = na.omit)
#summary(M_growth_l22)
M_growth <- lme(fixed = weight ~ age + age2 + gender, random = ~ age|id, data = growth, method = "REML",na.action = na.omit) 
anova(M_growth_l2, M_growth)
```


## 分析策略

進行統計建模之前，請思考你想從數據中探尋什麼問題的答案? 

1. 是想了解某一個共變量在層內 (同一個體不同時間，或者統一學校不同學生之間) 的條件效應 (conditional effect)?
2. 是想探索層內和層間數據的變化程度?
3. 是想了解一個共變量的邊際效應 (marginal effect) 嗎?

如果是 1 或 2 兩個問題的話，請使用混合效應模型。如果是 1，但是那個共變量卻不是定義於層水平的，那就只好放棄回到簡單的固定效應模型。如果是 3，需要考慮使用 GEE。

### 模型選擇和建模步驟

詳細請參考 [@Verbeke1997]。

當擬合一個混合效應模型時，意味着均值的結構和協方差的結構可以被確定 (an appropriate mean structure as well as covariance structure is specified)。協方差結構，解釋了均值結構無法解釋的數據隨機變化，所以二者之間彼此高度互相依賴。另外，適當的協方差模型對於用數據進行人羣參數的有效統計推斷過程是必不可少的。

- 第一步: 

由於固定效應部分不能完美解釋數據的變異，所以協方差結構就是用來輔助解釋這部分數據變異的輔助工具。建模的起點就應該是，先建立一個飽和 (甚至是過飽和 overelaborated) 的模型給均值結構 (固定效應部分)，從而確保之後要增加的隨機效應部分不受固定效應部分的擬合錯誤影響。所以，開始建模時，要先把所有可能考慮到的固定效應全部加入模型中去 (包括連續變量的二次方形式/或其他非線性關系，包括所有變量之間的交互作用)。這樣做其實是使用過度飽和的參數使得均值結構在模型中盡量在後面加入隨機效應之前保持不變。在可選的那些數據結構中，我們也應當考慮到數據中不同層級結構可能存在的異質性。要注意的是，隨機效應部分，不能也不應該在沒有把所有可能的一次方程結構都考慮進去之後 (a random effect for the linear effect of time)，就上馬二次方程/或更高次方程的隨機效應(a random effect for the quadratic effect of time)。
然後我們把飽和模型的殘差 (residuals)，異常值 (outliers)，擬合值 (fitted values)，和可能的 (potential) 隨機效應模型作出的這些殘差，異常值，擬合值之間進行比較。

- 第二步:

一旦你在飽和模型的條件下，確認好了隨機效應應該有的形式，接下來就是逐步精簡模型固定效應部分的過程: 

1. 用 Wald 檢驗 (當使用 REML 時)，或者 LRT (使用 ML 時) 來精簡化固定效應部分。
2. 反復檢查殘差，異常值，以及擬合值跟觀測值
3. 使用模型的預測軌跡和觀測值的點做視覺比較
4. 用人話把你的模型解釋給老奶奶聽懂






# Generalized Estimating Equation
 
# Cluster analysis/unsupervised learning 聚類分析

目前爲止，在等級回歸模型部分中，我們接觸到的回歸模型和可能存在相互依賴性的數據，都是建立在我們能夠觀察到或者實驗設計上已知的數據層級結構的前提下的。這樣的層級可以是空間上的，或者時間上的。處在相同層級的研究對象之間存在相關性，換句話說就是：層級內部的對象之間，比起層級之間的對象具有更多的相似性。

但是，在許多情況下，我們其實是無法事先知道數據的內部層級（聚類）結構的。而且我們可能需要儘可能多的獲取數據，並且從測量的數據中學習。學習數據變量與變量之間的相關性(correlation)，變量與變量之間的協方差(covariance)，個體與個體之間的相似性，從而根據獲取的數據來判斷數據內部是否存在不同的層級結構。這樣的一種對數據結構進行探索的過程，在機器學習(maching learning)中也是常常使用的，它又被叫做**非監督學習 (unsupervised learning)**。

之所以把這類尋找數據分類分層結構的過程叫做非監督學習，其實，是爲了和現在越來越豐富，多到令人髮指的那些被歸類於**監督學習(supervised learning)**的方法作爲相互對照。在監督學習中，數據內部的分層，聚類結構是事先知道的，也就是事先能夠測量或者被定義好的。事先被定義好了的數據層級結構中，我們可以使用多元變量分析，來對某些個體的特徵加以分類，也就是給數據中的未知成員分配**已知的分組**的過程。

在醫學中常見的非監督學習過程實例之一是，對於一個（全部相同疾病的）隊列研究中的受試者進行了大量的生物標幟物(biomarker)的測量與收集，可以是血液樣本的 biomarker 的測量，也可以是每名受試者的全部DNA信息。研究者希望通過這些患者的信息對他們進行同一疾病不同等級（類別，或者進程）的分類。那麼研究者需要利用這些收集來的患者信息，建立一套儘可能完善的分類的系統。

另外一個例子是，我們收集了前列腺癌患者的前列腺組織，利用基因轉錄組學 (transcriptomics) 的方法測量了每名患者成千上萬的組織內基因表達。研究者希望通過這些數據來分析，提取，並且分辨這些前列腺癌患者中可能存在的分類，或者亞型。研究者也希望知道這些分析獲得的亞型，是否會和某些已知的癌症的亞型相似或者相重合。

在商業領域中，聚類分析也是不罕見的。例如你爲某商業公司工作，那麼食品供應商可能會上門來要求你把購買食物的顧客進行類別區分，從而提供給食物供應商們一些線索，讓他們能夠更加精準的定位廣告投放人羣。



在統計學，和機器學習領域中，有許多不同的手法，可以用來輔助建立這種分類的規則，它們通常又被叫做判別分析法(discriminant analysis methods)。我們這一章和下一章着重討論

1. 聚類分析法 (cluster analysis)
2. 主成分分析法 (principal component analysis)

## 聚類分析過程

聚類分析法是一種分析不同統計測量值之間相似/差異程度的描述性分析過程。

爲什麼我們總是想對具有相似性質的事物進行歸類？其實，對事物進行區分和歸類，或者打上一些標籤，是人類文明在學習並且理解周圍的世界，從而促進科學發展的核心問題之一。在原始社會，對相似事物進行歸類有時候甚至事關生死。例如人類最初需要判定某些食物的共同特徵，區分哪些是含有毒性的，哪類動物可能是兇猛殘忍的。我們從嬰兒時期開始學習語言，學習事物/事件/人物的名稱，這其實也是一個學習對周圍的世界進行區分的學習過程。古代希臘文明的先賢哲學家亞里士多德曾經主張，人類的本能之一，就是不停地想對這個我們生活的世界發生的事情看到的事物進行類別的區分，尋找相似的特徵，區別不一樣的性質。在生物學中，甚至有由亞里士多德的學生[泰奧夫拉斯托斯(Theophrastos)](https://zh.wikipedia.org/wiki/%E6%B3%B0%E5%A5%A7%E5%BC%97%E6%8B%89%E6%96%AF%E6%89%98%E6%96%AF)創立的專門對生物進行分類的學科，生物分類學 (taxonomy)，後被瑞典人生物學家[卡爾林納斯 (Carl Linnaeus)](https://en.wikipedia.org/wiki/Carl_Linnaeus)進一步發揚光大。18世紀末，[Michel Adanson](https://en.wikipedia.org/wiki/Michel_Adanson)又爲人類引入了多元分析(polythetic)的分類系統概念，取代了之前使用單一因素(monothetic)對事物進行簡單分類的思想。很顯然，生物分類學在人類文明史中扮演了重要的角色。你應該很容易能想到達爾文提出的進化論，就是建立在前人對動植物進行了事無鉅細的分類和整理的基礎之上建立起來的重大理論突破。俄國科學家[門捷列夫](https://zh.wikipedia.org/wiki/%E5%BE%B7%E7%B1%B3%E7%89%B9%E9%87%8C%C2%B7%E4%BC%8A%E4%B8%87%E8%AF%BA%E7%BB%B4%E5%A5%87%C2%B7%E9%97%A8%E6%8D%B7%E5%88%97%E5%A4%AB)發現化學元素週期性，並且製作出了世界上第一章元素週期表，也爲人類理解原子世界奠定了基石。


在對事物進行分類這個任務上，聚類分析(cluster analysis)，和判別分析是相同的。有時候在已知對象的分類情況時我們仍然傾向於使用聚類分析的方法，用它來描述數據的一些特徵。同時也能有助於判定之後可能進行的判別分析是否準確。

簡單歸納，對分類描述過程進行量化的主要步驟有以下幾個：

1. 對於採集來的樣本數據 (statistical sample)，我們儘可能多的對它們的特徵變量進行測量。

2. 根據第一步獲得的變量信息，定義一個能夠幫助我們判定對象與對象之間相似點或者不同程度的測量指標。

3. 對這個測量指標制定一個區分的規則，或者叫做歸類的標準。

4. 對樣本進行分類。

5. 採集更多的樣本，對分類規則進行調整和完善。


### 連續型變量 continuous variables in cluster analysis

我們想象手裏的數據是一個矩陣 $X$，它的維度是 $n \times p$，用 $x_{ik}$，來表示第 $i$ 名觀察對象 $(i = 1, \dots, n)$ 的第 $k$ 個變量 $(k = 1, \dots, p)$ 的值。如果這些被測量的變量全部都是連續型變量的話，每個變量可以被使用幾何學的形式表達的 $p$ 個維度的其中一個平面上。當然，當維度超過3時，人類的無知大腦常常就無法進行有效的想象和推理，我們這裏使用簡單的三個變量，也就是三維空間來表示三個測量獲得的連續型變量：

例如我們測量了三名學生的身高，體重，以及前臂長。數據分別是：Angelo (190, 75, 30)；Dimitris (170, 75, 25)；Soren (170, 65, 30)。

```{r cluster00, echo=FALSE, cache=TRUE, fig.asp=.7, fig.width=6, fig.cap='A physical 3D space showing measurements of three variables.', fig.align='center', out.width='80%'}
library(plotly)

clus_data <- data.frame(Name = c("Angelo", "Dimitris", "Soren"), 
                        Height = c(190, 175, 170), 
                        Weight = c(75, 75, 65), 
                        Forearm = c(30, 25, 30))
p <- plot_ly(clus_data, x = ~Height, y = ~Weight, z = ~Forearm, color = ~Name) %>% 
  add_markers() %>% 
  layout(xaxis = list(range = c(0, 200))) %>% 
  layout(scene = list(xaxis = list(title = "Height (cm)"), 
                      yaxis = list(title = "Weight (cm)"), 
                      zaxis = list(title = "Forearm (cm)")))
p

```


在這個三維立體空間，我們需要定義一個變量用於丈量點與點之間的距離。其中最自然的就是歐幾里德(Euclidean)幾何距離:

$$
d_{ij} = \{\sum_{k = 1}^p(x_{ik} - x_{jk})^2\}^{\frac{1}{2}} 
$$


- 歐幾里德幾何距離又被稱爲 **L2 度量衡 (L2 metric)**。按照這個距離的定義，那麼 Angelo 和 Dimitris 之間的歐幾里德幾何距離就是：

$$
\begin{aligned}
& \{(190 - 175)^2 + (75 - 75)^2 + (30 - 25)^2 \}^{\frac{1}{2}} \\
= & \sqrt{15^2 + 0^2 + 5^2} \\ 
= & \sqrt{240} = 15.5
\end{aligned}
$$

- 曼哈頓距離 (Manhattan distance)：別名城市區塊度量衡 (cityblock metric)，或者**L1 度量衡**

$$
d_{ij} = \sum_{k = 1}^p |x_{ik} - x_{jk}|
$$

按照曼哈頓距離來定義的話，Angelo 和 Dimitris 之間的距離就是：

$$
|190 - 175| + |75 - 75| + |30 - 25| = 15 + 0 + 5 = 20
$$


後來人們發現上面提到的這兩種幾何學距離其實是閔科夫斯基度量衡 (Minkowski metric) 在 L=1 和 L=2 時的特殊情況。

閔科夫斯基度量衡的一般形式表達爲: 

$$
d_{ij} = \{ \sum_{k = 1}^p |x_{ik} - x_{jk}|^\ell \}^\frac{1}{\ell}
$$

閔科夫斯基度量衡試圖給差距較大的測量值之間增加權重用於區分彼此。不論是使用那種距離定義，這些測量距離的度量衡都具有如下的數學性質 (mathematical properties)：

1. 兩點之間的距離大於等於零, positivity <br> $d_{ij} \geqslant 0$，如果 $d_{ij} = 0$，那麼對於任何一個 $k = 1, \dots, p$，它們都是相等的 $x_{ik} = x_{jk}$。
2. 對稱性, symmetry <br> $d_{ij} = d_{ji}$
3. 三角形不等性, triangle inequality <br> $d_{ij} \leqslant d_{ih} + d_{hj}$

### 二分類或者分類型變量之間的距離 distances for binary/categorical variables

假如變量本身並不是連續型的，那麼閔科夫斯基度量衡並不適用，因爲二分量只能取0或者1。如下表所表示的，我們把 **i,j** 兩名對象的所有二分類變量進行下面的歸納總結：

| i/j | 1 | 0 |
|:---:|:-:|:-:|
|  1  | a | b |
|  0  | c | d |

其中，

- a 表示 i, j 兩名研究對象的二分類變量中，同時取 1 的變量的個數，
- b 表示 i, j 兩名研究對象的二分類變量中，i 取 1 但是 j 取 0 的變量的個數，
- c 表示 i, j 兩名研究對象的二分類變量中，j 取 1 但是 i 取 0 的變量的個數，
- d 表示 i, j 兩名研究對象的二分類變量中，同時取 0 的變量的個數。


根據這個總結表格，常用的表示兩個對象之間距離的數學度量是： 

1. 簡單匹配係數 (simple matching coefficient, SMC)，單純地計算所有的變量之中互相不一致的變量所佔的百分比： $$d_{ij} = \frac{b + c}{a+b+c+d}$$

2. 亞卡爾距離係數 (Jaccard coefficient)，則是把簡單匹配係數的分母中，d 的部分拿掉：$$d_{ij} = \frac{b + c}{a + b + c}$$

[其中亞卡爾距離係數更適合用於測量一些表達某些特質存在/不存在時兩名對象之間的距離測量 (see the "Difference with the simple matching coefficient (SMC)" session in the Wikipedia)](https://en.wikipedia.org/wiki/Jaccard_index)。



另外值得注意的是，在測量二分類變量距離的時候，三角形不等性的特質不一定會得到滿足。 (Please note that in general for dichotomous variables, the triangle inequality does not hold.)

用來計算測量對象之間距離的方法，和度量衡其實層出不窮，這裏只是簡單介紹了幾種。其餘的還有比如說由 [@Gower1971] 提出的 [Gower Index](https://cran.r-project.org/web/packages/gower/vignettes/intro.html)，該指標可以同時把測量有連續型變量和分類型變量，二分類變量等都包含進來。值得提醒的是，如果是討論非連續型測量值的對象距離，我們常常用它們之間的相似性(similarities) $s_{ij}$，而不太關注異質性 (dissimilarities) $d_{ij}$，但其是它們之間的簡單轉換關係就是 $d_{ij} = 1 - s_{ij}$。

### 定義分類方法

確定了用於衡量異質性 (dissimilarity) 距離的指標之後，我們就需要來定義分類的方法。首先把這個事先定下來的距離指標應用到我們的多元變量數據矩陣 (multivariate data matrix $\mathbf{X}$) (dimension: $n\times p$, where n indicates number of people, p indicates number of observed variables). 獲得一個形狀爲 $n\times n$ 的距離矩陣 $\mathbf{D}$ (對應上面三條數學性質中的第二條，對稱性 $d_{ij} = d_{ji}$)。獲得觀察對象的距離矩陣 $\mathbf{D}$ 之後需要決定的就是如何給對象進行分組的策略。該分組策略需要能使觀察對象被分組後，組內的對象相對組外對象更加相似，或者組外對象相對組內對象更加不同 (a sensible strategy would be to look for sets of units such that all units in that set are relatively similar to each other but relatively different from all units outside that set)。所以，用於分組策略的算法要有一定的可行性，它還要能夠量化對象之間的相對相似性 (relative similarity) 從而能夠完成以下任務：

1. 決定哪些人/對象被聚類到同一組中 (which pairs of units to join together into a cluster)

2. 每次聚類過程完成以後，重複相同的策略和算法，也就是重新計算新組成的聚類和剩餘的對象之間的距離。

3. 循環往復前兩個步驟直至全部的對象/個體都被分到各自的聚類 (cluster)。

事實上重複上述步驟，最終會把每個個體都分配到一個單獨的聚類中，也就是每個個體本身，那其實就跟沒有做聚類分析沒有區別，也沒有意義了。於是我們需要把聚類分析的過程通過圖形的方式展示出來。這樣的圖形被叫做**樹狀圖 (dendrogram)**，可以在視覺上輔助我們做出要給對象分成多少個聚類的決定。在希臘語中(Greek)，dendron 是樹的意思，樹狀圖的形狀常見的如下圖 (\@ref(fig:cluster01)) 所示，座標軸之一是所有的觀測對象的編號，另一個座標軸則是度量每個聚類或者觀測對象個體之間的距離。

```{r cluster01, cache=TRUE, fig.asp=.7, fig.width=7, fig.cap='Example of dendrogram vertically oriented, with 50 statistical units (average linkage method and Euclidean distance measure).', fig.align='center', out.width='80%', echo=FALSE}
plant <- read_dta("backupfiles/plant.dta")
plant <- plant[, 1:4]

# prepare hierarchical cluster
hc <-  hclust(dist(plant), "ave")


plot(hc, cex = 0.8, hang = -1, 
     main = "", ylab = "L2 dissimilarity measure", 
     xlab = "No. of specimen")
```

那麼回到之前如何決定聚類數量的問題上來，我們有兩種手段來輔助：

1. 層級法 (hierarchical methods)：聚合法，agglomerative； 或者分裂法， divisive。
2. 分區算法 (partitioning methods)。

層級法中的**聚合法 (agglomerative)**是指，從聚類分析的開始階段，每個獨立的對象自成一個聚類 (cluster)，所以起步於 n 個統計單位 (n statistical units)，之後的每一步聚類過程則是將度量距離相近的對象合併成爲一個聚類，直至最終所有個體歸爲唯一一個聚類。所以可以想象爲從各個枝葉彙總到一個樹幹走向各個枝葉的過程。

層級法中的**分裂法 (divisive)**則是和聚合法的聚類方向反過來，它起始於將所有觀察對象視爲唯一一個聚類，之後每一步聚類過程是將和大部分對象不太相似的個體從聚類中分裂出去，直至最終每個獨立的對象自成一個聚類。所以可以想象成從一個樹幹走向枝葉的過程。

分裂法其實十分消耗計算機的運算能力，因爲當樣本量較大時，一個 $k$ 種聚類的步驟就需要比較 $2^{k-1} -1$ 種不同的分區之間的距離。


# Missing data 1

# Principal Component Analysis 主成分分析 

> A big computer, a complex algorithm and a long time does not equal science.
> ~ Robert Gentleman


```{block2, note-text, type='rmdnote'}
PCA lecture was taught by Professor [Luigi Palla](https://scholar.google.co.uk/citations?hl=en&user=p-cHaf0AAAAJ&view_op=list_works&sortby=pubdate).
```

## 數據有相關性時產生的問題

假設我們有 $n$ 個研究對象作爲樣本，我們從這些對象身上採集儘可能多的數據，假設我們一共收集了 $p$ 個不同的變量。那麼這個數據的維度 (dimension) 是 $n \times p$。

如果說，我們在這個樣本中獲取到的 $p$ 個變量中，有一些是相互有依存性的，或者說相關的 (correlated)。我們有沒有辦法描述並展示這些具有相關性的變量在這個數據中扮演的角色，並且保留整個數據本身的變化特徵 (variability)？

Edgeworth (1891) 最早試圖用下面的方程來歸納一組從男性樣本身上測量獲得的存在相關性的變量：身高(H)，前臂長(F)，腿長(L)：

$$
\begin{aligned}
Y_1 & = 0.16H + 0.51F + 0.39L \\ 
Y_2 & = -0.17H + 0.69F + 0.09L \\
Y_3 & = -0.15H + 0.25F + 0.52L
\end{aligned}
$$

這恐怕是最早嘗試將一組有相關性的身體測量數據整理成"不相關"的三個新變量，作爲男性身體測量指標，用於描述樣本個體的身體結構的過程。

下圖 \@ref(fig:PCA00) 中展示的兩個變量，$x_1$ 和 $x_2$ 分別是身高和體重。


```{r PCA00, cache=TRUE, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap="Standardised data of height and weight", fig.align='center', out.width='70%'}
knitr::include_graphics("img/PCA00.png")
```

變量經過標準化處理之後，均值 $\mu = 0$，方差 $\sigma^2 =1$。如果此時已知身高和體重之間的協方差 (covariance, 概念參考 Section \@ref(covariance)) 是 $0.3$。

那麼，可以推導證明的是，他們的相關係數 (correlation, 概念參考 Section \@ref(correlation)) 是：

$$
\begin{aligned}
Corr(X_1,X_2) & = \frac{Cov(X_1,X_2)}{SD(X_1)SD(X_2)} \\
          & =\frac{Cov(X_1,X_2)}{\sqrt{Var(X_1)Var(X_2)}}\\ 
          & = Cov(X_1,X_2) \\
          & = 0.3
\end{aligned}          
$$

以 $x_2$ (體重) 爲結果變量，$X_1$ (身高) 爲單一解釋變量的線性回歸模型的回歸係數 (regression coefficient $\hat\beta$, 概念參考 Section \@ref(beta)) 是：


$$
\begin{aligned}
\hat\beta & = \frac{S_{x_1x_2}}{SS_{x_1x_2}} \\ 
          & = \frac{CV_{x_1x_2}}{SD_{x_1}^2} \\ 
          & = 0.3
\end{aligned}
$$

如果我們有另外一個座標系如下圖 \@ref(fig:PCA01)，從原先的座標系進行了一定角度的旋轉獲得 $y_1, y_2$。你會認爲哪個座標系更適合這個標準化之後身高體重的數據呢？

```{r PCA01, cache=TRUE, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap="Standardised data of height and weight, with a new reference system (y_1, y_2)", fig.align='center', out.width='70%'}
knitr::include_graphics("img/PCA01.png")
```


其實原先 $x_1, x_2$ 座標系之間存在一定的相關性，我們希望經過旋轉之後的新座標系 $y_1, y_2$ 之間是垂直的 (orthogonal)，這一數學上的概念被翻譯成爲統計學的語言就是，希望旋轉之後的新座標(變量)之間沒有相關性 (uncorrelated)。爲了消滅變量之間的相關性，我們要尋找到一個旋轉的角度 $\theta$，使得所有數據的點 $P_j$ 到新的座標軸 $y_1$ 之間的**垂直距離(perpendicualr)** $P_jP_j^\prime$ **之和最小** (minimise the distances between points and the reference axes)。如圖 \@ref(fig:PCA02) 顯示的那樣，從原點到每個數據點 $P_j$ 之間的距離 $OP_j$ 其實是固定不變的。我們希望找到新的座標使得 $P_jP_j^\prime$ 的距離最短。其中 $OP_j^\prime$ 就是數據點在新座標軸上投影的長度。

```{r PCA02, cache=TRUE, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap="Minimise the distance between the points and the reference axes.", fig.align='center', out.width='70%'}
knitr::include_graphics("img/PCA02.png")
```

根據[勾股定理 (Pythagorean theorem)](https://en.wikipedia.org/wiki/Pythagorean_theorem)。圖 \@ref(fig:PCA02) 中直角三角形的三邊的長度關係可以描述爲：

$$
(OP_j)^2 = (P_jP_j^\prime)^2 + (OP_j^\prime)^2
$$

把勾股定理應用到全部的數據點上的話，我們會得到一個關於所有數據點到新的座標軸距離，以及原點之間距離的方程：

$$
\begin{equation}
\sum_j (OP_j)^2 = \sum_j(P_jP_j^\prime)^2 + \sum_j(OP_j^\prime)^2
\end{equation}
(\#eq:PCAeq1)
$$

## 最大化方差等價於最大化數據點到新座標軸**"投影(projection)"**的長度

把等式 \@ref(eq:PCAeq1) 兩邊同時除以數據樣本量，我們獲得等式 \@ref(eq:PCAeq2)：

$$
\begin{equation}
\sum_j (OP_j)^2/n = \sum_j(P_jP_j^\prime)^2/n + \sum_j(OP_j^\prime)^2/n
\end{equation}
(\#eq:PCAeq2)
$$

其中值得注意的是，等式 \@ref(eq:PCAeq2) 左邊的部分 $\sum_j (OP_j)^2/n$ 對於一個樣本來說是固定不變的 (constant)。於是，等式右邊的部分，當我們的目標是最小化 $\sum_j(P_jP_j^\prime)^2/n$ 垂線 (perpendicular) 長度之和時，就等價於把數據點在新座標軸上的投影之和 $\sum_j(OP_j^\prime)^2/n$ 最大化。說白了，數據點在新座標軸上的投影，就是新座標軸上的變量大小。所以，旋轉座標軸之後，我們希望產生的新變量 $y_1,y_2$ 的方差取最大值(maximising the variance of the new data $\sum_j(OP_j^\prime)^2/n$)。利用三角函數(假設座標軸的旋轉角度是$\theta$)，你很容易就能得到新座標軸上新變量的值：

$$
\begin{equation}
\begin{aligned}
y_1  & = x_1\cos\theta + x_2\sin\theta \\ 
y_2  & = -x_1\sin\theta  + x_2\cos\theta
\end{aligned}
\end{equation}
(\#eq:PCAeq3)
$$

**證明**

如圖 \@ref(fig:PCA03) 所示，設座標軸 $X_1,X_2$ 逆時針旋轉角度爲 $\theta$，設新座標爲 $(y_1, y_2)$，且原點於點 $P_j (x_1, x_2)$ 之間的連線 $OP_j$ 長度爲 $r$，$OP_j$ 和新座標軸 $y_1$ 之間的角度爲 $\alpha$。

```{r PCA03, cache=TRUE, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap="Rotation of the coordinates, and the new variables calculation.", fig.align='center', out.width='70%'}
knitr::include_graphics("img/PCA03.png")
```


$$
\begin{aligned}
(OP_j)^2 & = x_1^2 + x_2^2 = y_1^2 + y_2^2 \\
         & = r^2 \\ 
\because  x_1 & = r\times\cos(\alpha +\theta) \\ 
          x_2 & = r\times\sin(\alpha + \theta) \\
          y_1 & = r\times\cos(\alpha) \\
          y_2 & = r\times\sin(\alpha) \\ 
\therefore x_1 & = r[\cos\alpha\cos\theta - \sin\alpha\sin\theta] \\
               & = y_1\cos\theta - y_2\sin\theta \\ 
           x_2 & = r[\sin\alpha\cos\theta + \cos\alpha\sin\theta] \\
               & = y_2\cos\theta+y_1\sin\theta \\
\Rightarrow x_1\cos\theta & = y_1\cos^2\theta -y_2 \sin\theta\cos\theta \\ 
            x_2\sin\theta & = y_2\cos\theta\sin\theta + y_1\sin^2\theta \\ 
\textbf{Sum the}& \textbf{ above two equations} \\
\Rightarrow y_1 & = \frac{x_1\cos\theta + x_2 \sin\theta}{(\cos^2\theta + \sin^2\theta)} \\ 
            y_1 & = x_1\cos\theta + x_2 \sin\theta \\ 
\textbf{Similarly}& \\
\Rightarrow x_1\sin\theta & = y_1\cos\theta\sin\theta -y_2 \sin^2\theta \\ 
            x_2\cos\theta & = y_2\cos^2\theta + y_1\sin\theta\cos\theta \\ 
\textbf{Take substraction}& \textbf{ between the above two equations} \\
\Rightarrow y_2 & = \frac{-x_1\sin\theta + x_2\cos\theta}{(\cos^2\theta + \sin^2\theta)} \\
            y_2 & = -x_1\sin\theta + x_2\cos\theta
\end{aligned}
$$

$y_1, y_2$就是旋轉後新的座標軸的變量。在這個簡單實例中，我們從原始數據 $x_1, x_2$ 經過旋轉，獲得新的數據 $y_1, y_2$，他們二者之間其實只是經過了線性轉換 (linear transformation)。一般地，我們如果要給原始數據矩陣 (維度 $n\times p$)進行座標軸的數據轉換，只需要給原始數據矩陣乘以一個正方形的投影矩陣 $\mathbf{P}$ (projection matrix) (維度 $p\times p$) ($p$ 是變量的個數)即可。

當變量只有兩個 $(p =2)$ 時，我們很容易使用一個平面圖來理解這個轉換過程其實就是對座標軸進行幾何旋轉的過程，這時候的投影矩陣是：

$$
\left[
\begin{array}
\cos\cos\theta & \sin\theta \\
-\sin\theta & \cos\theta
\end{array}
\right]
(\#eq:PCAeq4)
$$

經過旋轉之後獲得的新變量 $y_1, y_2$ 被叫做主成分 (principal components)。主成分有什麼特徵呢？如圖 \@ref(fig:PCA04) 所表示的那樣，當兩個原始變量 $x_1, x_2$ 之間相關係數很高，由於已知方差總和不變 $\text{Var}(x_1)+\text{Var}(x_2) = \text{Var}(y_1) + \text{Var}(y_2)$，座標旋轉之後的第一個主成分 $y_1$，將會擁有原始數據 $x_1, x_2$ 的方差 (variance) 中的絕大部分。那麼理論上，我們就完成了保留數據本身的整體方差，但是把大部分方差歸納到第一個主成分中去的過程。所以，當對樣本測量了很多很多的變量的時候，我們會發現很多變量之間存在內部相關性，於是我們可以通過主成分分析來留下幾個能解釋整體數據的最主要的成分，並且保留數據的整體信息，也就是整體的方差，這是一個把數據降維 (dimension reduction) 的過程，去除掉那些冗餘的不需要的變量 (redundancy removed)。

```{r PCA04, cache=TRUE, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap="Variance of the new axis/prin", fig.align='center', out.width='70%'}
knitr::include_graphics("img/PCA04.png")
```

所以，PCA的過程可以描述如下：

數據如果有 $p$ 個存在內部相關性的連續型變量 $x_1, x_2, \dots, x_p$，那麼一定存在 $p$ 個相互獨立的變量 (principal components)，滿足下面的條件：

1. $p$ 個相互獨立的變量分別都是原始變量 $x_1, x_2, \dots, x_p$ 的線性轉換：
$$
\begin{aligned}
y_1 & = a_{11}x_1 + a_{12}x_2 + \cdots + a_{1p}x_p \\
y_2 & = a_{21}x_1 + a_{22}x_2 + \cdots + a_{2p}x_p \\
\vdots & \\
y_p & = a_{p1}x_1 + a_{p2}x_2 + \cdots + a_{pp}x_p \\
\end{aligned}
$$

2. 這 $p$ 個相互獨立的變量通過最大化它們對數據整體方差的貢獻獲得。
3. 這 $p$ 個相互獨立的變量被叫做這個數據的主成分變量。
4. 這些主成分變量之間相互獨立 (uncorrelated)，並且按照他們各自對數據總體方差的貢獻度從大到小排列 (the principal components are uncorrelated and are ordered by the amount of the total system variability that they explain)：

$$
\text{Cov}(y_j, y_k) = 0 \text{ for any } j, k \in [1, p] \\
\text{Var}(y_1) \geqslant \text{Var}(y_2) \geqslant \text{Var}(y_3) \geqslant \dots \geqslant \text{Var}(y_p)
$$

## 數學推導

如果，

- $\textbf{S}$ 是數據的**方差協方差矩陣 (variance, covriance matrix)**；
- $\textbf{P}$ 是**直角投影矩陣 (orthogonal projection matrix)**，該矩陣的每一列，是旋轉之後的新變量的座標，也就是主成分變量，它們又被叫做**特徵向量 (eigenvectors)**；
- $\bf{\Lambda}$ 是一個**對角矩陣 (diagonal matrix)**，它的對角線上是每個主成分變量的方差，它們又被叫做**特徵值 (eigenvalues)**。特徵值常常又被叫做慣性 (inertia)，特徵值從對角線左上角起往右下角是從大到小排列，每一個特徵值是每個特徵向量的方差，也就是數據整體方差投射在這個主成分變量上的慣性，可以理解爲該主成分能夠解釋多少整個數據的方差 (explained variance)。

```{theorem, name = "Spectral decomposition"}
根據**譜定理 Spectral decomposition**：如果矩陣 $\textbf{S}$ 是對稱的，它總是可以被分解爲：
$$
\textbf{S} = \textbf{P}\bf{\Lambda}\textbf{P}^t
$$
```

值得注意的是，首先，分解方差協方差矩陣的時候，並沒有任何統計學或者概率論上的前提條件；其次，這樣的矩陣分解不一定只用於方差協方差矩陣，你可以對任何對稱矩陣 (symmetrix matrix) 進行分解，它被叫做矩陣縮放 (matrix scaling)；最後，其實數據矩陣本身不一定非要是連續型變量，也不一定要有相似的刻度/取值範圍 (same scale)，如果你願意，對二分類變量或者是計數型變量，均可以進行主成分分析。但是，當變量之間的刻度相差巨大時，可能會產生一些意想不到的假象。所以，在實施主成分分析之前，通常的建議是對原始數據的變量進行標準化，或者直接用其相關係數矩陣 (correlation matrix)。

### 超越對稱矩陣：奇異值分解 (singular value decomposition, SVD)

主成分分析使用的矩陣分解方法，只能應用在方差協方差矩陣或者相關係數矩陣這樣的對稱的正方形矩陣。假如矩陣並非對稱，另一種矩陣分解方法叫做奇異值分解法 (singular value decomposition, SVD)。此時就可以直接應用在原始數據矩陣 $\mathbf{X}_{n\times p}$ 本身，而不需要侷限於數據的方差協方差矩陣/相關係數矩陣：

$$
\mathbf{X}_{n\times p} = \mathbf{U}_{n\times n}\bf{\Sigma}_{n \times p} \mathbf{W}_{p\times p}^t
$$

其中，

- $\mathbf{U}_{n\times n}$ 是含有**左奇異向量 (left singular vectors)** 的矩陣；
- $\Sigma_{n \times p}$ 是含有**奇異值 (singular values)**的矩陣；
- $\mathbf{W}_{p\times p}$ 則是含有**右奇異向量 (right singular vectors)** 的矩陣。

所以你看到任意的形狀都可以被分解，此時分解出來的 $\mathbf{U}_{n\times n}$ 和 $\mathbf{W}_{p\times p}$ 是形狀維度不同的正方形矩陣。

另外，根據這樣的分解我們可以推導：

$$
\begin{aligned}
\mathbf{X}^t \mathbf{X} & = \mathbf{W}\bf{\Sigma}\mathbf{U}^t\times\mathbf{U}\bf{\Sigma}\mathbf{W}^t \\
                        & = \mathbf{W}\bf{\Sigma}^2\mathbf{W}^t \\ 
\Rightarrow \bf{\Sigma}^2 & = \bf{\Lambda}                      
\end{aligned}
$$
 
所以，$\bf{\Sigma}^2 = \bf{\Lambda}$ ，也就是說在奇異值分解中獲得的中間矩陣 $\bf{\Sigma}_{n \times p}$，它對角線上的數值的平方，就是每個原始變量的方差，或者說它們本身是原始數據的標準差。奇異值分解矩陣的方法最常見被用於實施對應分析 (Correspondence Analysis)。


## 主成分分析數據實例

[橙汁數據](http://factominer.free.fr/bookV2/orange.csv)，是邀請美食家對產自世界各地的六種品牌的橙汁進行一個一個的味道/品質描述，並給每個項目打分後彙總獲得的評價數據。你可以用下面的代碼下載這個數據並觀察每個描述的變量，且很容易觀察的到的是，這些變量之間並不完全獨立，有些變量可能和另一些變量相關：

```{r PCAorange, message=FALSE, warning=FALSE, echo=FALSE}
orange <- read.table("http://factominer.free.fr/bookV2/orange.csv", 
                     header = TRUE, sep = ";", dec = ".", row.names = 1)

orange[, 1:7] %>%
  kable() %>%
  kable_styling() %>% 
   scroll_box(width = "100%", height = "300px")
```



進行主成分分析在Stata只需要這樣一行代碼：

```
insheet using "http://factominer.free.fr/bookV2/orange.csv" , delimiter(";") clear
pca odour* pulp* intens* acid* bitter* sweetness, cor
```

你就會獲得十分直觀的結果：

```

Principal components/correlation                 Number of obs    =          6
                                                 Number of comp.  =          5
                                                 Trace            =          7
    Rotation: (unrotated = principal)            Rho              =     1.0000

    --------------------------------------------------------------------------
       Component |   Eigenvalue   Difference         Proportion   Cumulative
    -------------+------------------------------------------------------------
           Comp1 |      4.74369       3.4104             0.6777       0.6777
           Comp2 |      1.33329      .513448             0.1905       0.8681
           Comp3 |      .819842      .735818             0.1171       0.9853
           Comp4 |     .0840232     .0648702             0.0120       0.9973
           Comp5 |      .019153      .019153             0.0027       1.0000
           Comp6 |            0            0             0.0000       1.0000
           Comp7 |            0            .             0.0000       1.0000
    --------------------------------------------------------------------------

Principal components (eigenvectors) 

    ------------------------------------------------------------------------------
        Variable |    Comp1     Comp2     Comp3     Comp4     Comp5 | Unexplained 
    -------------+--------------------------------------------------+-------------
    odourinten~y |   0.2110    0.6534   -0.5174    0.0286    0.0310 |           0 
    odourtypic~y |   0.4524    0.1162   -0.0646    0.2668    0.2952 |           0 
       pulpiness |   0.3313    0.5340    0.3290   -0.3327   -0.2250 |           0 
    intensityo~e |  -0.2984    0.3714    0.6910    0.0189    0.3456 |           0 
         acidity |  -0.4191    0.3017   -0.0237    0.7065   -0.4106 |           0 
      bitterness |  -0.4292    0.1628   -0.3152   -0.0974    0.6712 |           0 
       sweetness |   0.4384   -0.1374    0.2061    0.5553    0.3503 |           0 
    ------------------------------------------------------------------------------
```

根據方差協方差矩陣進行的主成分分析結果，我們發現主成分 6 和 7 可以忽略不計。相同的計算結果可以在R裏面通過方便的計算包 [`FactoMineR`](http://factominer.free.fr/) 來計算並用 [`factoextra`](http://www.sthda.com/english/wiki/factoextra-r-package-easy-multivariate-data-analyses-and-elegant-visualization) 來實現其分析圖形的美觀展示：



```{r PCAorange1, message=FALSE, warning=FALSE}
# library(FactoMineR)
org.pca <- PCA(orange[, 1:7], ncp = 7, graph = FALSE)

# library(factoextra)
eig.val <- get_eigenvalue(org.pca)
eig.val # eigenvalue (variances of each principal components)

# eigen vectors:
org.pca$svd$V
```

於是根據計算獲得的特徵值向量，我們可以寫下這5個主成分變量和原始變量之間的轉換關係方程：

$$
\begin{aligned}
y_1 & = 0.2110x_1 + 0.4524x_2 + 0.3313x_3 - 0.2984x_4 - 0.4191x_5 - 0.4292x_6 + 0.4384x_7 \\
y_2 & = 0.6534x_1 + 0.1162x_2 + 0.5340x_3 + 0.3714x_4 + 0.3017x_5 + 0.1628x_6 - 0.1374x_7 \\
y_3 & =-0.5174x_1 - 0.0646x_2 + 0.3290x_3 + 0.6910x_4 - 0.0237x_5 - 0.3152x_6 + 0.2061x_7 \\
y_4 & = 0.0286x_1 + 0.2668x_2 - 0.3327x_3 + 0.0189x_4 + 0.7065x_5 - 0.0974x_6 + 0.5553x_7 \\
y_5 & = 0.0310x_1 + 0.2952x_2 - 0.2250x_3 + 0.3456x_4 - 0.4106x_5 + 0.6712x_6 + 0.3503x_7 \\
\end{aligned}
$$

於是，解釋完了如何從原始數據變量根據計算獲得的特徵值向量轉換成爲新的變量之後，要面對的問題是，我們要保留多少主成分？
我們通常會使用圖 \@ref(fig:PCAorangeScreeplot) 那樣的碎石圖 (Scree plot) 來輔助判斷。碎石圖通常縱軸是每個主成分能夠解釋的數據總體方差的百分比，然後橫軸是主成分的個數。所以我們會期待出現一個像手肘一樣的形狀提示應該在第幾個主成分的地方停下。通常在統計分析中，我們默認的準則是，至少保留的主成分個數要能夠解釋總體方差的 70%/80% 以上才較爲理想。[Kaiser 準則](https://en.wikipedia.org/wiki/Exploratory_factor_analysis#Kaiser's_(1960)_eigenvalue-greater-than-one_rule_(K1_or_Kaiser_criterion)) 建議的是，最好保留下特徵值大於等於1(也就是標準化數據之後獲得的主成分變量方差大於等於1)的主成分變量。在我們的橙汁數據實例中，顯然保留前兩個主成分就已經能夠解釋 86.81% 的總體方差，我們認爲這是理想的主成分個數。

```{r PCAorangeScreeplot,  cache=TRUE, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap="Orange data: eigenvalues among all variances (varaince explained) by each dimension (principle component) provided by PCA", fig.align='center', out.width='70%'}
fviz_eig(org.pca, addlabels = TRUE, ylim = c(0, 70)) + 
  theme(
    axis.text = element_text(size = 10, face = "bold"),
  axis.text.x = element_text(size = 10, face = "bold"),
  axis.text.y = element_text(size = 10, face = "bold"),
  axis.title = element_text(size = 8, face = "bold"),
  plot.title = element_text(size = 10, face = "bold"),
  )
```

另外一種輔助的圖形是叫做分數圖 (score plot)，又名個人圖 (graph of individuals)，如果個體的變量特徵相近，他們會在圖中聚在較爲靠近的地方：

```{r PCAorangeScoreplot,  cache=TRUE, echo=TRUE, fig.asp=.7, fig.width=4, fig.cap="Score plot/individual plot of the orange data.", fig.align='center', out.width='70%'}
fviz_pca_ind(org.pca, pointsize = "cos2", pointshape = 21, 
             fill = "#E7B800", repel = TRUE, labelsize = 2) 
```

細心觀察的話，你會發現圖 \@ref(fig:PCAorangeScoreplot) 中各個橙汁 (個體,individual) 的座標其實是來自於PCA分析結果中第一和第二主成分變量的結果，展示在第一和第二主成分變量構成的平面。該平面解釋了總體數據慣性 (inertia) 的 86.82% (= 67.77% + 19.05%)。其中第一個主成分 `Dim.1` 把 `Tropicana fr.` 和 `Pampryl amb.` 兩種橙汁分別歸類在最右邊和最左邊。這是因爲原始數據中 `Tropicana fr.` 是 `Odour.typicality` 得分最高 `Bitternes` 得分倒數第二低，同時 `Pampryl amb.` 則是在這兩個項目上得分分別是最低和最高。也就是說這兩種橙汁在這兩個項目上得分分別是左右兩種極端，所以首先在第一主成分中把這兩中橙汁分離開來。接下來，第二主成分變量 `Dim.2` 則是將第一主成分成功分離開的兩個個體(橙汁)從數據中拿掉以後，剩下的四種橙汁的分類。可以看到第二個主成分軸，把 `Pampryl fr.` 和 `Tropicana amb.` 兩種橙汁放在了該軸的兩個極端，這是因爲 `Pampryl fr.` 在 `Intensity.of.taste` 項目上得分最高，而 `Tropicana amb.` 在拿掉了第一主成分分離的兩種橙汁之後，在 `Odour.intensity` 項目上得分最低。



回到 R 幫忙分析的主成分結果報告來：

```{r PCAorange2, message=FALSE, warning=FALSE}
summary(org.pca)
```

可以看到第一部分是特徵值(eigenvalue)的結果描述，第二部分是個人 (individual) 的分析報告：

```
...{omitted}...
Individuals
                       Dist    Dim.1    ctr   cos2    Dim.2    ctr   cos2 
Pampryl amb.       |  3.029 | -2.984 31.288  0.970 | -0.082  0.085  0.001
Tropicana amb.     |  1.976 |  0.886  2.761  0.201 | -1.715 36.771  0.753 
Fruvita fr.        |  2.595 |  1.937 13.182  0.557 |  0.040  0.020  0.000 
Joker amb.         |  2.094 | -1.896 12.631  0.820 | -0.834  8.686  0.158 
Tropicana fr.      |  3.512 |  3.186 35.660  0.823 |  0.589  4.335  0.028 
Pampryl fr.        |  2.338 | -1.129  4.479  0.233 |  2.002 50.102  0.733 
...{omitted}...
```

其中，

- `Dist` 是每個個體(行數據)，到座標軸原點 (平均重心位置) 的距離。此數據中距離原點最遠的兩種橙汁是 `Pampryl amb.` (最左邊) 和 `Tropicana fr.` (最右邊)。
- `Dim.1` 是該個體，在第一個主成分變量座標軸上的座標。
- `ctr` 是該個體在第一個主成分變量提取時貢獻的百分比。
- `cos2` 是該個體在該主成分變量上投影的慣性除以該個體本身的慣性所佔的比例，又叫做該個體對相應主成分變量的代表性評價 (the quality of representation of an individual $i$ on the principle component $s$ is measured by the distance between the point within the space $u_s$ and the projection on the component)。

$$
\text{quality of representation}_s(i) = \frac{\text{Projected inertia of }i \text{ on } u_s}{\text{Total inertia of }i} = \cos^2\theta_i^s 
$$

PCA報告的下半部分，是關於數據中變量與變量之間關係的分析結果。

```
Variables
                      Dim.1    ctr   cos2    Dim.2    ctr   cos2    Dim.3    ctr   cos2  
Odour.intensity    |  0.460  4.452  0.211 |  0.754 42.694  0.569 | -0.468 26.771  0.219 |
Odour.typicality   |  0.985 20.468  0.971 |  0.134  1.350  0.018 | -0.058  0.417  0.003 |
Pulpiness          |  0.722 10.977  0.521 |  0.617 28.519  0.380 |  0.298 10.826  0.089 |
Intensity.of.taste | -0.650  8.902  0.422 |  0.429 13.797  0.184 |  0.626 47.747  0.391 |
Acidity            | -0.913 17.561  0.833 |  0.348  9.100  0.121 | -0.021  0.056  0.000 |
Bitterness         | -0.935 18.420  0.874 |  0.188  2.651  0.035 | -0.285  9.936  0.081 |
Sweetness          |  0.955 19.220  0.912 | -0.159  1.889  0.025 |  0.187  4.246  0.035 |
```

根據這個結果繪製的變量相關關係圖如下：

```{r PCAorangevariableplot,  cache=TRUE, echo=TRUE, fig.asp=.7, fig.width=4, fig.cap="Variable plot of the orange data.", fig.align='center', out.width='70%'}
fviz_pca_var(org.pca, repel = TRUE, labelsize = 2) 
```

- 在第一個主成分軸上 (`Dim.1`)，正相關的變量 `Odour.intensity, Odour.typicality, Pulpiness, Sweetness` 被歸類在右半球，而負相關的變量 `Intensity.of.taste, Acidity, Bitterness` 則被歸類在第一主成分軸的左半球。
- 相似地，在第二個主成分軸上 (`Dim.2`)，只有負相關的 `Sweetness` 被歸類在下半球。
- 每個變量從原點出發時的箭頭長度越長 `cos2`，代表它在該主成分軸上代表質量更好 (the quality of representation of the variable on the component)

如果你願意，且數據和變量不至於多到眼花繚亂，我們還可以把個體圖和變量圖結合起來觀察：

```{r PCAorangebiplot,  cache=TRUE, echo=TRUE, fig.asp=.7, fig.width=4, fig.cap="Biplot of the orange data.", fig.align='center', out.width='70%'}
fviz_pca_biplot(org.pca, repel = TRUE, pointsize = "cos2", pointshape = 21, 
             labelsize = 2) 
```


## 在PCA圖形中加入補充變量和補充個體 (supplementary elements)

在橙汁數據中，除了有美食家給出的各個味道項目的評分之外，其實還有各個橙汁的物理化學特性數據。


```{r OrangePhyChem, message=FALSE, warning=FALSE, echo=FALSE}
orange[, 8:16] %>%
  kable() %>%
  kable_styling() %>% 
   scroll_box(width = "100%", height = "300px")
```

我們可以把這些沒有用於計算主成分分析的變量 (active variables)，和其餘的輔助性變量 (supplementary variables) 同時繪製在變量相關係數圓盤圖中。此時我們只需要在進行PCA運算的時候告訴R這些變量是輔助性的連續/分類變量即可：

```{r PCAorangeSuppl, message=FALSE, warning=FALSE}

org.pca <- PCA(orange, quanti.sup = 8:14, quali.sup = 15:16,
               graph = FALSE)
org.pca$quanti.sup
```


然後用下面的代碼繪製包含了輔助性變量的變量相關圓盤圖：

```{r PCAvarsuppplot,  cache=TRUE, echo=TRUE, fig.asp=.8, fig.width=5.5, fig.cap="Orange juice data: representation of the active and supplementary variables (in blue).", fig.align='center', out.width='90%'}
fviz_pca_var(org.pca, repel = TRUE) 
```


如圖 \@ref(fig:PCAvarsuppplot) 所示，第一個主成分變量分離的左右半球的橙汁味道特徵，和他們的物理特性其實是相呼應的。例如，`pH` 值出現在了圓盤的右半邊，靠近 `Sweetness` 這一變量。因爲 `pH` 越高，酸度越低，那麼味道也就越甜，這是合理的。另外一個有趣的現象是，蔗糖 `saccharose` 含量高的橙汁，`pH` 越高，味道越甜。在圓盤的左邊，是蔗糖在酸環境下分解之後產生的果糖和葡萄糖，所以果糖葡萄糖反而和酸度 `Acidity` 相關性高，因爲橙汁中果糖葡萄糖含量高意味着蔗糖被酸分解。

由此可見，PCA是一個對數據進行初步描述和探索時十分有力的工具。所以，在回歸模型選擇變量之前，建議可以對數據先進行主成分分析，並且把預備考慮放在回歸模型的解釋變量部分的變量用於PCA主成分分析，把想要做預測的變量作爲輔助性變量投射到主成分分析的變量相關圖中，觀察解釋變量之間可能存在的相關性，有助於選取合適的解釋變量。

### 展示分類輔助性變量和個體的關係

根據不同的儲存方式，兩類的橙汁區別很清楚。

```{r PCAindplotsupp,  cache=TRUE, echo=TRUE, fig.asp=.8, fig.width=5.5, fig.cap="Plane representation of the scatterplot of individuals with a supplementary categorical variable (way of preserving).", fig.align='center', out.width='70%'}
p <- fviz_pca_ind(org.pca, habillage = 15, 
             palette = "jco", repel = TRUE)
p
```

根據橙子的產地區分繪製的個人圖：

```{r PCAindplotsupp2,  cache=TRUE, echo=TRUE, fig.asp=.8, fig.width=5.5, fig.cap="Plane representation of the scatterplot of individuals with a supplementary categorical variable (origin).", fig.align='center', out.width='70%'}
p <- fviz_pca_ind(org.pca, habillage = 16, 
             palette = "jco", repel = TRUE)
p
```


## Cluster analysis/PCA practical 

本次練習完成時，你將學會：

1. 如何使用聚類分析，和主成分分析法來探索一組多變量數據之間的關係；
2. 理解並懂得如何選取合適的距離測量尺度，和聚類分析方法；
3. 繪製並能夠解釋由多層聚類分析算法 (hierarchical clustering algorithm) 獲得的樹狀圖；
4. 使用主成分分析法對數據進行座標轉換，計算多個變量之間的方差，協方差矩陣，懂得如何判斷保留主成分的個數；
5. 通過把數據繪製在較低維度的主成分座標軸上來判斷數據中可能存在的潛在分層/分組。


### 使用的數據和簡單背景知識

假設你是一名生物測量技術公司的統計師，現在有這樣一組數據，包含了對某植物測量的4種生物標幟物(biomarkers)。據報道，這四種成分或許能減少你公司生產的某藥物引起的副作用。爲了嘗試分析該植物的生物特性，從該植物的50個不同樣本中，測量了這4種生物標幟物的濃度。你的任務之一是對數據進行初步分析，彙報任何你找到的可能存在的顯著特徵差異。

1. 在R裏讀入你的數據，看看這4種生物標幟物的簡單統計量和分佈，它們用的是相同的測量單位嗎？


```{r pca-1, cache=TRUE}
plant <- read_dta("backupfiles/plant.dta")
plant <- plant[, 1:4]
head(plant)
summ(plant)
psych::describe(plant)
```

觀察這四個生物標幟物的簡單統計量，似乎可以認爲它們使用的應該是相似或者相同的測量單位。它們的均值在53至61之間，標準差分佈在45-51之間，而且最大值最小值之間的範圍也十分接近。

2. 這些生物標幟物能否單獨提供關於該植物的某部分特徵信息呢？思考我們該如何回答這個問題（提示：計算這些指標直接的相關係數）

我們可以通過計算這四個生物標幟物濃度測量值之間的相關係數，來觀察它們之間是否具有相似性或者是否提供了部分相似的信息。

```{r pca-2, cache=TRUE}
cor(plant)
```

從相關係數矩陣的計算結果來看，平均地，這四個生物標幟物濃度之間具有一定程度的相關性。其中，生物標幟物1和3之間呈現了四者之間最高的樣本相關係數 $(r_{13} = 0.5941)$，生物標幟物1和4之間的相關係數則最小 $(r_{14} = 0.2677)$。

3. 請描述前一步中我們計算的相關係數矩陣的維度(dimension)。

該相關係數矩陣的維度是 $4\times4$，事實上，這個矩陣的維度是由我們想要觀察分析的樣本中測量變量的個數決定的（在這裏就是四個生物標幟物）。但是這個相關係數的矩陣並不適合用於做聚類分析 (cluster analysis)，因爲相關係數本身反映的是變量之間的關係 (between variables)，並非觀察對象 (between subjects) 之間的距離(即，不是我們關心的用來把50個樣本進行分組歸類的距離變量)。

4. 再次思考問題1.的答案，思考並選擇合適的測量不同樣本個體之間距離 (distance) 的度量衡。嘗試使用簡單的聚類分析命令對樣本進行分類。

由於每個生物標記物在所有樣本中的數值基本在相似的比例或者刻度，每個標幟物在這個樣本中的標準差/方差數值也較爲相近。我們嘗試用連續變量最常見的均值測量距離指標:

```{r pca-3, cache=TRUE, fig.asp=.7, fig.width=7, fig.cap='Dendrogram for L2_avlink cluster analysis', fig.align='center', out.width='80%'}
# prepare hierarchical cluster
hc <-  hclust(dist(plant), "ave")


plot(hc, cex = 0.8, hang = -1, 
     main = "", ylab = "L2 dissimilarity measure", 
     xlab = "No. of specimen")
```

可以觀察到，樣本編號 31, 27, 17, 48, 8, 30, 3, 14, 6, 42 很快就聚合成爲一組。且這些樣本和其他樣本被聚類在不同組的過程一直維持到差異性達到100以上。我們還可以注意到，聚類過程中其他的分支呈現相對對稱的形狀。


5. 從簡單的歐幾里得距離改成歐幾里得距離平方來測量樣本之間的距離的話，圖形會變成什麼樣？

```{r pca-4, cache=TRUE, fig.asp=.7, fig.width=7, fig.cap='Dendrogram for L2sq_avlink cluster analysis', fig.align='center', out.width='80%'}
hc <- hclust(dist(plant)^2)

plot(hc, cex = 0.8, hang = -1, 
     main = "", ylab = "L2squared dissimilarity measure", 
     xlab = "No. of specimen", sub = "")
```

當使用歐幾里得距離的平方作爲樣本間隔的度量衡時，我們發現聚類的過程其實總體來說和使用歐幾里得距離本身並無本質上的區別。只是在差異性較低的地方聚類加速 (squeeze the dissimilarities at the lower end)，並且在較大的聚類區分之間變得更加明顯，視覺效果上更容易區分。

如果說，不用歐幾里得平方，而是使用簡單的曼哈頓距離 (L1 度量衡)，那麼圖形則又會呈現爲:

```{r pca-5, cache=TRUE, fig.asp=.7, fig.width=7, fig.cap='Dendrogram for L1_avlink cluster analysis', fig.align='center', out.width='80%'}

plot(cluster::agnes(plant, metric = "manhattan", stand = F), which.plots = 2, hang = -1, 
     xlab = "No. of specimen", main = "", ylab = "L1 dissimilarity measure", sub = "", cex = 0.8)
```

可以看出，當使用曼哈頓距離做度量衡時，聚類的過程和之前的沒有本質上的區別，但是圖形的樹狀分支上似乎不再左右對稱。

6. 接下來使用歐幾里得距離做度量衡，與上面的嘗試不同，這裏我們嘗試用完全連接，和單連接


# Missing data 2

# Further issues
