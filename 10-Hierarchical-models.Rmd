# (PART) 等級線性迴歸模型 analysis of hierarchical and other dependent data {-}


# 相互依賴數據及簡單的應對方案 {#Hierarchical}

> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
>
> ~ Sir Ronald Aylmer Fisher


```{block2, note-thankslinda, type='rmdnote'}
The Analysis of Hierarchical and Other Dependent Data lectures were orgainised and taught by Professor [Linda Sharples](https://www.lshtm.ac.uk/aboutus/people/sharples.linda), and Dr. [Edmund Njeru Njagi](https://www.lshtm.ac.uk/aboutus/people/njagi.edmund-njeru).
```


```{r Hier-Session01, child = ('10-Hierarchical/Session01.Rmd')}
```


## Practical Hierarchical 01

```{r Hier-Practical01, child = ('10-Hierarchical/Practical01.Rmd')}
```


# 隨機截距模型 random intercept model {#random-intercept}

```{r Hier-Session02, child = ('10-Hierarchical/Session02.Rmd')}
```




## Practical Hierarchical 02

```{r Hier-Practical02, child = ('10-Hierarchical/Practical02.Rmd')}
```




# 隨機截距模型中加入共變量 random intercept model with covariates {#random-inter-cov}


```{r Hier-Session03, child = ('10-Hierarchical/Session03.Rmd')}
```


## Practical Hierarchical 03

```{r Hier-Practical03, child = ('10-Hierarchical/Practical03.Rmd')}
```



# 隨機回歸系數模型  random coefficient model {#random-coefficient}


```{r Hier-Session04, child = ('10-Hierarchical/Session04.Rmd')}
```


## Practical Hierarchical 04

```{r Hier-Practical04, child = ('10-Hierarchical/Practical04.Rmd')}
```




# 縱向研究數據 longitudinal data 1 {#longitudinal1}
 
本章我們來把目前爲止了解的混合效應 (截距/斜率) 模型應用到一種特殊形態的數據 -- 縱向研究數據 -- 中去。




縱向數據，是一種前瞻性收集的來的數據，它隨着時間的推移，在不同的時間點對相同的觀察對象進行數據的採集。每個研究對象被收集數據時的時間點，可以是相同的，也可以是不同的。在很多臨牀實驗中，患者被觀察隨訪，並且常常在同樣的時間點收集數據，所以在臨牀實驗的特殊形態下，每個患者收集數據的時間點可以做到統一，這樣的縱向研究數據是屬於**固定測量時刻的類型 (fixed occasions)**。但是在流行病學等觀察性研究中獲得的數據，就沒有這麼幸運，他們通常測量收集數據的時間點就不太可能保持一致，收集時間點不一致的縱向數據屬於**不固定測量時刻的類型 (variable occasions)**。

縱向數據英文名是 longitudinal data，它的常見別的名稱是 重復測量數據 (repeated measures data)，計量經濟學中叫做面板型數據 (panel data)，或者是時間序列橫斷面研究數據 (cross sectional time series data)。所以在縱向數據這種特殊形態的的嵌套式數據結構中，第二層級結構就是一個個的個體，第一層級結構，就是每個個體在不同的時間點獲得的測量值。除了和前面幾章討論過的嵌套式數據結構相似可以應用混合效應模型，縱向數據還有一些自己獨特的性質需要加以考量: 

- 層內數據的相關性結構是有測量時間的先後順序的;
- 之前討論的嵌套式結構數據在層內的觀察值則沒有嚴格的時間或者大小的排序 (例如同一所學校的不同學生);
- 換句話說，層內相關系數 (intra-class correlation) 很難被認爲是相似或者相同的。



## 固定測量時刻 fixed occasions

對於臨牀試驗中固定時刻隨訪收集到的病人數據，理想狀態下應該是一種平衡數據 (balanced data)。也就是在不同時間 $t_i , i = 1, \cdots, n$ 我們成功收集到所有患者的所有數據，所以每層 (名患者) 擁有的時間序列數據的樣本量是相同的 $n_j = n, \forall j$。

如同分析其他類型的數據一樣，分析縱向數據也要從描述數據開始。如果是平衡數據，描述性分析就很容易，當有缺失值時，分析就變得有些棘手。例如，我們可以計算每個時間點的平均值作爲所有患者的 "平均特質 average profiles"。或者也可以用每個人的時間序列數據對時間做簡單線性回歸模型，從而獲取每個個體的截距和斜率。


### 缺失值 Missing data

當縱向數據中存在一些缺失值，即使你在計算一些簡單的歸納性分析，也要**特別特別特別**地小心。如果不是所有人都有全部測量時間點的數據的話，總體的平均特徵數據分析了也沒有太大的卵用，因爲缺失值導致這樣計算獲得的並不是真實的平均值 (也因爲不同的患者，貢獻了不同時間點的數據，沒辦法平均)。

如果存在缺失值，那麼當且僅當這些缺失值和觀測值 $Y$ 之間沒有關系時，才能認爲這些簡單計算和簡單模型的建立是不帶有偏倚的。如果說，有些缺失值確實是根據觀測數據有選擇性地缺失 (the mechanism driving the selection depends on measured data)，隨機效應模型的建立可以自動化校正這樣的缺失，從而保證估計無偏。

根據觀測數據選擇性地出現缺失值的機制被叫做隨機缺失 (Missing at random, MAR)。


#### 隨機截距模型 random intercept model

**復合對稱模型 compound symmetry model**， 是常見的一種用於重復測量數據的模型，它是基於隨機截距模型的一種擴展模型。

當模型中沒有解釋變量時，

$$
\begin{equation}
Y_{ij} = \mu_i + u_{0j} + e_{ij}
\end{equation}
(\#eq:hier05-1)
$$

其中， 

- $i$ 是測量時刻; 
- $j$ 是實驗的個體; 
- $\mu_i$ 是測量時刻 $i$ 時的平均截距 -- 這是一個固定效應。

爲了擬合這個模型，我們需要先生成一系列的啞變量用來表示不同的測量時刻: 

$$
Y_{ij} = \sum_{h=1}^n\beta_{0h} I_{i = h,j} + u_{0j} + e_{ij}
$$

其中，

- $I_{i = h,j}$ 是用於表示第 $j$ 名患者的 $i$ 次觀測值，在第 $h$ 次測量時是否被測量到的啞變量。
- 該模型暗示同一個患者收集到的不同時刻的觀察數據是可以互換的，有相同的協方差 
$$
\begin{aligned}
\text{Cov}(Y_{1j} , Y_{2j}) & = \text{Cov}(u_{0j} + e_{1j}, u_{0j} + e_{2j}) \\ 
                            & = \sigma^2_{u_{00}}
\end{aligned}
$$
- 該模型還有另一個暗示是，不同患者之間任意時間點的兩個觀察數據之間是相互獨立的 
$$
\begin{aligned}
\text{Cov}(Y_{1j}, Y_{2j*}) & = \text{Cov}(u_{0j} + e_{1j}, u_{0j*} + e_{2j*}) \\ 
                            & = 0
\end{aligned}
$$

所以當沒有缺失值時，數據是固定測量時刻 (fixed occation) 的數據也是是平衡數據，那麼每一個患者 (第二層級數據) 的觀察值可以寫作是一個向量 $\{ \mathbf{Y}_{ij} \}$，每名患者的觀察值向量的長度都是相同的 $n$。所以，它們的 $n\times n$ 協方差矩陣就是:  

$$
\Omega_y = \left( \begin{array}{cccc} 
 \sigma^2_{u_{00}} + \sigma^2_e & \sigma^2_{u_{00}}  & \cdots & \sigma^2_{u_{00}} \\
 \sigma_{u_{00}}   & \sigma^2_{u_{00}} + \sigma^2_e    & \cdots & \sigma^2_{u_{00}} \\
 \vdots            & \vdots                            & \vdots & \vdots \\
 \sigma^2_{u_{00}} & \sigma^2_{u_{00}}                &  \cdots & \sigma^2_{u_{00}} + \sigma^2_e\\
\end{array} \right)
$$

也正是由於觀測值的協方差矩陣是如此地對稱，該模型被命名爲復合對稱模型 compound symmetric model。

**Adult height measures 數據**

有(閒人)花了數十年時間追蹤隨訪了近2000名女性在 26 歲，36歲，43歲，53歲時的身高。忽略掉可能存在的測量誤差，研究者想知道是否隨着年齡增加，女性的身高會縮水。這些女性在這些年齡時的身高數據總結如下: 

```{r hier05-01, cache=TRUE}
height <- read_dta("backupfiles/height.dta")
epiDisplay::summ(height[, 2:5])
```


原則上每個女性在所有的時間應該都有身高測量值才對，我們暫且認爲擁有缺失測量值的時間點是完全隨機的。先計算樣本中數據完整部分的女性身高在四個時間點時的方差協方差矩陣: 

```{r hier05-02, cache=TRUE}
var(height[, 2:5], use = "complete.obs")
```

要給這個數據擬合混合對稱模型 (compound symmetry model)，需要先把數據從寬變長，之後爲每個測量身高的時間點生成一個啞變量，然後擬合無截距式的隨機截距模型: 


```{r hier05-03, cache=TRUE}
# 把數據格式從寬變長
hei_long <- height %>%
  gather(key, value, -id, -bw, -mht) %>%
    separate(key, into = c("Height", "H_Age"), sep = 2) %>%
      arrange(id, H_Age, bw, mht) %>%
        spread(Height, value)

# 生成四個年齡時間點數據的啞變量
hei_long <- hei_long %>%
  mutate(Age_1 = ifelse(H_Age == 26, 1, 0), 
         Age_2 = ifelse(H_Age == 36, 1, 0),
         Age_3 = ifelse(H_Age == 43, 1, 0),
         Age_4 = ifelse(H_Age == 53, 1, 0))
M_hei <- lmer(ht ~ 0 + Age_1 + Age_2 + Age_3 + Age_4 + (1 | id), data = hei_long, REML = TRUE)
summary(M_hei)

# 檢驗三個年齡點的身高均值是否相同用下面的方法: 
linearHypothesis(M_hei, c("Age_1 - Age_2 = 0", 
                          "Age_1 - Age_3 = 0", 
                          "Age_1 - Age_4 = 0"))
```

所以，用這個模型 (符合對稱模型 compound symmetry model)，其實我是在告訴 R 軟件說，我認爲，這個數據中的女性四次測量的身高之間的方差協方差矩陣是這樣紙的 (因爲 $5.992^2 = 35.91; 1.409^2 = 1.99$): 

$$
\Omega_y = \left( \begin{array}{cccc} 
 37.90 & 35.91  & 35.91 & 35.91 \\
 35.91 & 37.90  & 35.91 & 35.91 \\
 35.91 & 35.91  & 37.90 & 35.91 \\
 35.91 & 35.91  & 35.91 & 37.90\\
\end{array} \right)
$$


分析這個模型第二層階級殘差，和第一層階級殘差可以計算並做圖 \@ref(fig:5-level2-res) \@ref(fig:5-level1-res) 如下: 

```{r hier05-04, cache=TRUE}
# refit the model with lme
M_hei <- lme(fixed = ht ~ 0 + Age_1 + Age_2 + Age_3 + Age_4, random = ~ 1 | id, 
             data = hei_long, method = "REML", na.action=na.omit)
# individual level standardized residuals
ehat_st <- residuals(M_hei, type = "normalized", level = 1)

# extract the EB uhat (level 2 EB residual)
uhat_eb <- ranef(M_hei)$`(Intercept)`

# standardized level 2 residuals
### count number of measures for each women
Nmeas <- 4
### shrinkage factor 
R = 5.992^2/(5.992^2 + 1.409^2/Nmeas)
### use shrinkage factor calculate variance of uhat_eb
var_eb <- R * 5.992^2
### standardize uhat
uhat_st <- uhat_eb/sqrt(var_eb)
```



```{r 5-level2-res, cache=TRUE, echo=FALSE, fig.height=5, fig.width=11, fig.cap='Standardized cluster level residuals (intercept) from the compound symmetry model', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
ggthemr("fresh")

uhat_st <- as.data.frame(uhat_st)

hist <- ggplot(uhat_st, aes(uhat_st)) + 
    geom_histogram(colour = "black", aes(y = ..density..), 
                   size = 0.1, binwidth= 0.2)  + 
             stat_function(fun=dnorm, size = 1,
                         args=list(mean=mean(uhat_st$uhat_st), 
                                  sd=sd(uhat_st$uhat_st))) + 
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "Level 2 cluster standardized residuals (EB)")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8))



plot2 <- HLMdiag::ggplot_qqnorm(uhat_st$uhat_st, line = "quantile") +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "Level 2 cluster standardized residuals (EB)")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8))

ggarrange(hist, plot2,  
          ncol = 2, nrow = 1)
```

```{r 5-level1-res, cache=TRUE, echo=FALSE, fig.height=5, fig.width=11, fig.cap='Standardized elementary level residuals from the compound symmetry model', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
ggthemr("fresh")

ehat_st <- as.data.frame(ehat_st)

hist <- ggplot(ehat_st, aes(ehat_st)) + 
    geom_histogram(colour = "black", aes(y = ..density..), 
                   size = 0.1, binwidth= 0.2)  + 
             stat_function(fun=dnorm, size = 1,
                         args=list(mean=mean(ehat_st$ehat_st), 
                                  sd=sd(ehat_st$ehat_st))) + 
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "Level 1 standardized residuals")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8))



plot2 <- HLMdiag::ggplot_qqnorm(ehat_st$ehat_st, line = "quantile") +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "Level 1 standardized residuals (EB)")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8))

ggarrange(hist, plot2,  
          ncol = 2, nrow = 1)
```


混合對稱模型的前提假設實在是太強了 (它假定個體內的方差保持不變，且個體間的協方差也保持不變)。你我都清楚，當考慮了時間以後，同一個體在時間上比較接近的點測量之間會更相似，也更相關。


#### 隨機參數模型 random intercept and slope model

實際上有多種方法可以放鬆混合對稱模型對方差和協方差的約束性前提，其中之一是在隨機截距模型中允許有隨機斜率成分。


使用隨機參數模型擬合縱向數據時的簡單模型如下: 

$$
Y_{ij} = (\beta_0 + u_{0j}) + (\beta_1 + u_{1j})t_i +e_{ij}
$$

前一章討論過 (滾回 \@ref(random-var))，這裏隨機參數模型的解釋變量是時間 $t_i$，導致的結果之一是觀測值的方差其實是隨着時間變化而變化的 (拋物線關系):

$$
\begin{aligned}
\text{Var}(Y_{ij}) & = \text{Cov}(u_{0j} + u_{ij}t_i + e_{ij}, u_{0j} + u_{ij}t_i + e_{ij})  \\ 
                   & = \sigma^2_{u_{00}} + \sigma^2_{u_{11}}t_i^2 + 2t_i\sigma_{u_{01}} + \sigma^2_e
\end{aligned}
$$

同時，同一患者不同時間測量的觀測值之間的協方差是: 

$$
\begin{aligned}
\text{Cov}(Y_{1j}, Y_{2j}) & = \text{Cov}(u_{0j} + u_{1j}t_1 + e_{1j}, u_{0j} + u_{2j}t_2 + e_{2j}) \\ 
& = \sigma^2_{u_{00}} + \sigma^2_{u_{11}}t_1t_2 + \sigma_{u_{01}}(t_1 + t_2)
\end{aligned}
$$

不同患者任意測量時刻之間的協方差是: 

$$
\begin{aligned}
\text{Cov}(Y_{1j}, Y_{2j*}) & = \text{Cov}(u_{0j} + u_{1j}t_1 + e_{1j}, u_{0j*} + u_{2j*}t_2 + e_{2j*}) \\ 
& = 0
\end{aligned}
$$


**Adult height measures 數據**

利用上面的理論，來對身高數據擬合另一個混合效應模型:

```{r hier05-06, cache=TRUE}
# 對年齡中心化到以 26 歲爲起點
hei_long <- hei_long %>%
  mutate(age = as.numeric(H_Age) - 26)
M_hei_ran <- lme(fixed = ht ~ age, random = ~ age | id, data = hei_long, method = "REML", na.action = na.omit)
#M_hei_ran <- lmer(ht ~ age + (age | id), data = hei_long, REML = TRUE)
summary(M_hei_ran)
```


這個混合效應模型同時包含了隨機截距和隨機斜率兩個部分。你可以用 LRT 比較它和一個只有隨機截距的模型哪個更好，但是我們沒有辦法比較它和混合對稱模型哪個更優於擬合這個數據 (因爲他們的固定效應部分不同，在 REML 方法下實際二者擬合的數據是不同的)。這個隨機系數模型和前一個混合對稱模型都給出了身高隨着年齡增加而減少的相同結論。不同的是，隨機系數模型把同一對象內不同時間觀測值之間的等協方差的約束條件給放開了，因爲用腳趾頭想也知道**同一個人不同時間測量的數據之間的協方差會隨着時間跨度不同而發生改變**。

根據隨機系數模型給出的報告，計算模型估計的觀測值 (身高的4個時間點) 的方差協方差矩陣: 

$$
\begin{aligned}
\hat{\text{Cov}}(Y_{1j}, Y_{2j}) & = \sigma^2_{u_{00}} + \sigma^2_{u_{11}}t_1t_2 +\sigma_{u_{01}} (t_1 + t_2) \\
 & = 6.1588^2 + 0.0599^2t_1t_2 + (-0.28)\times6.1588\times0.0599 (t_1 + t_2)\\ 
 & = 37.93 + 0.004\times t_2 \times t_2 - 0.104 \times(t_1 + t_2) \\
\hat{\text{Var}} (Y_1j) & = \sigma^2_{u_{00}} + \sigma^2_{u_{11}}t_1^2 - 2\sigma_{u_{01}}t_1 + \sigma_e^2 \\ 
& = 37.93 + 0.004 \times t_1^2 - 0.104\times2\times t_1 + 1.59
\end{aligned}
$$

所以，當 $t_1 = 0, t_2 = 10, t_3 = 17, t_4 = 27$ 時，

$$
\mathbf{\hat{\Sigma}_u} =  \left( \begin{array}{cccc} 
 39.52 & 36.90  & 36.17 & 35.14 \\
 36.90 & 37.81  & 35.75 & 35.07 \\
 36.17 & 35.75  & 37.03 & 35.03 \\
 35.14 & 35.07  & 35.03 & 36.54 \\
\end{array} \right)
$$


```{r 5-level2-ress, cache=TRUE, echo=FALSE, fig.height=5, fig.width=11, fig.cap='UN-Standardized cluster level residuals (intercept and slope) from the random intercept and slope model', fig.align='center', out.width='80%', message=FALSE, warning=FALSE, eval=TRUE}
# refit the model with lme
M_hei_ran <- lme(fixed = ht ~ age, random = ~ age | id, data = hei_long, method = "REML", na.action = na.omit)
# individual level standardized residuals
ehat_st <- residuals(M_hei_ran, type = "normalized", level = 1)

# extract the EB uhat (level 2 EB residual) intercept and slope
uhat_eb <- ranef(M_hei_ran)


# standardized level 2 residuals
### count number of measures for each women
Nmeas <- 4
### shrinkage factor 
R1 = 6.15884 ^2/(6.15884 ^2 + 1.25921^2/Nmeas)
R2 = 0.05993^2/(0.05993^2 + 1.25921^2/Nmeas)
### use shrinkage factor calculate variance of uhat_eb
var_eb_inter <- R1 * 6.15884 ^2
var_eb_slope <- R2 * 0.05993
### standardize uhat
uhat_eb$inte_st <- uhat_eb$`(Intercept)`/sqrt(var_eb_inter)
uhat_eb$slop_st <- uhat_eb$age/sqrt(var_eb_slope)


ggthemr("fresh")

plot_u0hat <- HLMdiag::ggplot_qqnorm(uhat_eb$`(Intercept)`, line = "quantile") +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "Level 2 cluster  residuals (EB) of intercept")  +
  theme(axis.title = element_text(size = 14), axis.text = element_text(size = 8))

plot_u1hat <- HLMdiag::ggplot_qqnorm(uhat_eb$slop_st, line = "quantile") +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "Level 2 cluster  residuals (EB) of slope")  +
  theme(axis.title = element_text(size = 14), axis.text = element_text(size = 8))


ggarrange(plot_u0hat, plot_u1hat,  
          ncol = 2, nrow = 1)
```


```{r 5-level1-res0, cache=TRUE, echo=FALSE, fig.height=5, fig.width=11, fig.cap='Standardized elementary level residuals from the random intercept and slope model', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
ggthemr("fresh")
ehat_st <- residuals(M_hei_ran, type = "normalized", level = 1)

ehat_st <- as.data.frame(ehat_st)

hist <- ggplot(ehat_st, aes(ehat_st)) + 
    geom_histogram(colour = "black", aes(y = ..density..), 
                   size = 0.1, binwidth= 0.2)  + 
             stat_function(fun=dnorm, size = 1,
                         args=list(mean=mean(ehat_st$ehat_st), 
                                  sd=sd(ehat_st$ehat_st))) + 
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "Level 1 standardized residuals")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8))



plot2 <- HLMdiag::ggplot_qqnorm(ehat_st$ehat_st, line = "quantile") +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "Level 1 standardized residuals (EB)")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8))

ggarrange(hist, plot2,  
          ncol = 2, nrow = 1)
```


## 不固定測量時刻 variable occasions

當重復收集的數據不是平衡數據時，意味着不同的人數據的收集時間點不一樣，我們就無法像前面那樣用協方差矩陣的方式來描述不同人不同時間點之間測量值可能存在的相關性，也沒有辦法給每個時間點所有人的數據做平均值作爲全部人的平均特質。

但是我們可以把不固定測量時刻的不平衡數據看作是受缺失值數據影響的平衡數據 (unbalanced data can be thought of as balanced data affected by missingness)。所以需要特別小心謹慎，因爲用線性混合效應模型擬合這樣的數據，其實是在含蓄地假設那些應該出現但是沒有出現的測量值的缺失是隨機的。


**Asian growth data 實例**

在本部分開頭的章節介紹過，這是一個收集了亞洲兒童在 6 周，8 個月，12 個月，和 27 個月大時的體重數據。



```{r  Hier05-07, cache=TRUE, echo=FALSE, fig.height=6, fig.width=9, fig.cap='Growth profiles of boys and girls in the Asian growth data', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
growth <- read_dta("backupfiles/asian.dta")
growth <- growth %>%
  mutate(gender = factor(gender, labels= c("Boys", "Girls")))

ggthemr('fresh')

G <- ggplot(growth, aes(x = age, y = weight)) + 
 geom_line(aes(group = id), lty = 1) + 
   theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "Age (years)", y = "Weight (Kg)")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8),
        axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank())
G + facet_grid(. ~ gender) + 
  theme(strip.text = element_text(face = "bold", size = rel(1.5)))
```

如圖 \@ref(fig:Hier05-07) 所示，觀察男孩女孩的體重隨着時間的變化，似乎暗示男孩子體重增加的速度較高，且男孩中體重增加的差異 (方差) 似乎也較女孩子的體重增加曲線來得大。另外，體重和年齡的關系並不是線性的，而且，這些數據中有缺失值。

**隨機截距模型**

第一個想到的合適模型應該包括一個隨機截距，一個固定效應的線性和拋物線性的年齡項，還有最後一個啞變量用以區分男孩和女孩: 

$$
Y_{ij} = (\beta_0 + u_{0j}) + \beta_1t_{ij} + \beta_2 t_{ij}^2 + \beta_3 \text{girl}_j + e_{ij}
$$

在 R 裏擬合這個模型: 

```{r Hier05-08, cache=TRUE}
growth <- growth %>%
  mutate(age2 = age^2)

M_growth <- lme(fixed = weight ~ age + age2 + gender, random = ~ 1 | id, data = growth, method = "REML", na.action = na.omit)
summary(M_growth)

## 由於樣本量較小，這裏如果使用極大似然法估計 ML，結果就和 REML 估計的隨機效應的方差部分不太相同
M_growthml <- lme(fixed = weight ~ age + age2 + gender, random = ~ 1 | id, data = growth, method = "ML", na.action = na.omit)
summary(M_growthml)
```

**隨機截距和斜率模型**

此時我們再來用相同的數據擬合混合效應模型，現在允許線性年齡的斜率有隨機變化: 

```{r Hier05-09, cache=TRUE}
M_growth_mix <- lme(fixed = weight ~ age + age2 + gender, random = ~ age | id, data = growth, method = "REML", na.action = na.omit)
summary(M_growth_mix)
```

這裏可以看到隨機殘差 (residuals) 的標準差 (`StdDev`) 部分在後者(混合系數模型)中明顯變小了 $(0.74\rightarrow 0.54)$。另外，第二層級殘差和第一層級殘差 (未標準化) 如圖 \@ref(fig:hier05-10) 和 \@ref(fig:hier05-11):



```{r hier05-10, cache=TRUE, echo=FALSE, fig.height=5, fig.width=11, fig.cap='UN-Standardized cluster level residuals (intercept and slope) from the random intercept and slope model', fig.align='center', out.width='80%', message=FALSE, warning=FALSE, eval=TRUE}

uhat_eb <- ranef(M_growth_mix)


ggthemr("fresh")

plot_u0hat <- HLMdiag::ggplot_qqnorm(uhat_eb$`(Intercept)`, line = "quantile") +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "Level 2 cluster  residuals (EB) of intercept")  +
  theme(axis.title = element_text(size = 14), axis.text = element_text(size = 8))

plot_u1hat <- HLMdiag::ggplot_qqnorm(uhat_eb$age, line = "quantile") +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "Level 2 cluster  residuals (EB) of slope")  +
  theme(axis.title = element_text(size = 14), axis.text = element_text(size = 8))


ggarrange(plot_u0hat, plot_u1hat,  
          ncol = 2, nrow = 1)
```



```{r hier05-11, cache=TRUE, echo=FALSE, fig.height=5, fig.width=11, fig.cap='Standardized elementary level residuals from the random intercept and slope model', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
ggthemr("fresh")
ehat_st <- residuals(M_growth_mix, type = "normalized", level = 1)

ehat_st <- as.data.frame(ehat_st)

hist <- ggplot(ehat_st, aes(ehat_st)) + 
    geom_histogram(colour = "black", aes(y = ..density..), 
                   size = 0.1, binwidth= 0.2)  + 
             stat_function(fun=dnorm, size = 1,
                         args=list(mean=mean(ehat_st$ehat_st), 
                                  sd=sd(ehat_st$ehat_st))) + 
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(x = "Level 1 standardized residuals")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8))



plot2 <- HLMdiag::ggplot_qqnorm(ehat_st$ehat_st, line = "quantile") +
  theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  labs(y = "Level 1 standardized residuals (EB)")  +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8))

ggarrange(hist, plot2,  
          ncol = 2, nrow = 1)
```



## 預測軌跡 predicting trajectories

比較只有隨機截距模型，和隨機系數模型給出的擬合曲線是否有差異 如圖\@ref(fig:hier05-12)，其實差異十分微小。可以用下面的 R 代碼: 

```{r hier05-12,  cache=TRUE, echo=TRUE, fig.height=5, fig.width=8, fig.cap='Observed weight and predicted growth profiles of four babies in the Asian growth data', fig.align='center', out.width='80%', message=FALSE, warning=FALSE}
growth$traj2 <- fitted(M_growth_mix) 
growth$traj1 <- fitted(M_growth) 

G <- ggplot(growth[growth$id %in% c(258,1141,3148,287),], aes(x = age, y = weight)) + geom_point(shape = 19, size = 4) + 
 # geom_line(aes(y = traj1)) + 
#  geom_line(aes(y = traj2), linetype = 2) +
  stat_smooth(method = "lm", aes(y = traj1), formula = y ~ x + I(x^2), se = F, linetype = 2) + 
  stat_smooth(method = "lm", aes(y = traj2), formula = y ~ x + I(x^2), se = F)  +
   theme(axis.text = element_text(size = 15),
  axis.text.x = element_text(size = 15),
  axis.text.y = element_text(size = 15)) +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 8))


G +  facet_wrap( ~ id, ncol = 2) + 
  theme(strip.text = element_text(face = "bold", size = rel(1.5)))
```


## Practical 05-Hier


# 縱向研究數據 longitudinal data 2 {#longitudinal2}

本章沒有代碼，學會如何用矩陣標記法寫下你的多元混合效應模型。

## 邊際結構 marginal structures

至此，我們接觸過的各種混合效應模型其實代表的是數據不同的邊際結構關系 (marginal relations)。

### 隨機截距模型

縱向數據中，數據可能是平衡或不平衡數據，簡單的隨機截距模型可以標記如下: 

$$
Y_{ij} = (\beta_0 + u_{0j}) + \beta_1 t_{ij} + e_{ij}
$$

這個模型隱含着如下的條件關系 (conditional relation):

$$
\begin{aligned}
Y_{ij} | t_{ij}, u_{0j} & \sim N(\beta_0 + \beta_1t_{ij} + u_{0j}, \sigma^2_e)\\ 
 u_{0j}|t_{ij} & \sim N(0, \sigma^2_u) \\
 \text{Var}(Y_{ij} | t_{ij}, u_{0j})  & = \sigma^2_e
\end{aligned}
$$

也就是說，觀測值 $Y_{ij}$ 以時間 $t$，和隨機截距 $u_0$ 爲條件的方差，只取決於 $\sigma^2_e$。所以，屬於同一層 (同一患者不同測量時間) 的測量值，以該層 (患者) 的截距爲條件 (conditional on $u_j$) 的協方差是 $\text{Cov} (Y_{ij}, Y_{i*j}|t_{ij}, t_{i*j}, u_j) = 0$。

$Y_{ij}$ 針對 $u_j$ 的邊際期望 (marginal espectation with respect to $u_j$): 

$$
E(Y_{ij}|t_{ij}) = \beta_0 + \beta_1 t_{ij}
$$

其方差爲 $\text{Var}(Y_{ij}|t_{ij}) = \sigma^2_u + \sigma^2_e$，同一層 (同一患者) 的兩個不同時刻測量值之間的邊際協方差就是 $\text{Cov}(Y_{ij}, Y_{i*j}|t_{ij},t_{i*j}) = \sigma^2_u$。

### 隨機系數模型

模型的數學標記是

$$
Y_{ij} = (\beta_0 + u_{0j}) + (\beta_1 + u_{1j})t_{ij} + e_{ij}
$$

等同於

$$
Y_{ij} = (\beta_0 + \beta_1t_{ij}) + (u_{0j} + u_{1j}t_{ij}) + e_{ij}
$$


其**條件關系**是 

$$
Y_{ij}|t_{ij},u_{0j},u_{1j} \sim N( \beta_0 + \beta_1t_{ij} + u_{0j} + u_{1j}t_{ij}, \sigma^2_e)
$$

其中， $\mathbf{u}_j|t_{ij} \sim N(0, \mathbf{\Sigma}_u)$，且

$$
\mathbf{\sum}_{\mathbf{u}}  =\left( \begin{array}{cc}
              \sigma^2_{u_{00}} & \sigma_{u_{01}} \\
              \sigma_{u_{01}}   & \sigma^2_{u_{11}} \\
              \end{array} \right)
\\
\text{Cov} (Y_{ij}, Y_{i*j}|t_{ij}, t_{i*j}, u_{oj}, u_{1j}) = 0
$$

其所指的$Y_{ij}$的邊際分布: 

$$
\begin{aligned}
E(Y_{ij}|t_{ij})   & = \beta_0 + \beta_1t_{ij} \\
\text{Var}(Y_{ij}) & = \sigma^2_{u_{00}}  +2\sigma_{u_{01}}t_{ij} + \sigma^2_{u_{11}}t_{ij}^2 + \sigma^2_e \\
\text{Cov}(Y_{ij}, Y_{i*j}) & = \text{Cov}(u_{0j} + u_{1j}t_{ij} + e_{ij}, u_{0j} + u_{1j}t_{i*j} + e_{i*j}) \\
                   & = \sigma^2_{u_{00}} + \sigma_{u_{01}}(t_{ij} + t_{i*j}) + \sigma^2_{u_{11}}t_{ij}t_{i*j} \text{ (for } i \neq i*) \\
\text{Cov}(Y_{ij}, Y_{i*j*}) & = \text{Cov}(u_{0j} + u_{1j}t_{ij} + e_{ij}, u_{0j*} + u_{1j*}t_{i*j*} + e_{i*j*}) \\ 
& = 0 \text{ (for } j \neq j*) 
\end{aligned}
$$

也就是說**同一層 (同一患者) 的不同測量值之間的協方差不爲零，是時間的函數**。

## 矩陣記法

如果數據本身是**平衡數據**，可以用如下的矩陣標記混合效應模型，

- $j$ 是每個患者 (第二層級)，$\mathbf{Y}_j, \mathbf{e}_j$ 向量被定義爲: 

$$
\begin{aligned}
\mathbf{Y}_j & =  \left( \begin{array}{c}
Y_{1j} \\
Y_{2j} \\
\cdots \\
\cdots \\
Y_{nj}
\end{array}
\right) \\
\mathbf{e}_j & =  \left( \begin{array}{c}
e_{1j} \\
e_{2j} \\
\cdots \\
\cdots \\
e_{nj}
\end{array}
\right) \\
\end{aligned}
$$

用三次測量時間 $t_1, t_2, t_3$ (以簡便標記) 來繼續接下來的推導，定義矩陣 $\mathbf{T}, \mathbf{\beta}, \mathbf{u}_j$: 

$$
\mathbf{T} = \left(\begin{array}{c}
1 & t_1 \\
1 & t_2 \\
1 & t_3 
\end{array}
\right) \\
\mathbf{\beta} = \left( \begin{array}{c}
\beta_0 \\
\beta_1 
\end{array}
\right) \\
\mathbf{u}_j = \left(\begin{array}{c}
u_{0j} \\
u_{1j} 
\end{array}
\right)
$$

如此經過利用定義好的向量，我們就可以把模型用矩陣標記來記錄，從無窮無盡的下標中解放出來: 

$$
\mathbf{Y = T\beta + Tu + e} \\ 
\text{Where } \mathbf{u} \sim N(0, \mathbf{\Sigma}_u) \\ 
              \mathbf{e} \sim N(0, \sigma^2_e\mathbf{I})
$$

那麼 

$$
\text{Var}(\mathbf{Y}) = \mathbf{T\Sigma}_u\mathbf{T}^T + \sigma^2_e \mathbf{I}
$$

## 混合效應模型的一般化公式

前面的例子用的雖然是時間做解釋變量 (縱向數據)，但是也可以推廣到一般的混合效應模型: 

$$
\mathbf{Y = T\beta + Zu + e}
$$

其中 $\mathbf{Z}$ 是類似 $\mathbf{T}$ 的共變量矩陣。類似地，$\mathbf{Y}$ 的方差是: 

$$
\text{Var}(\mathbf{Y}) = \mathbf{Z\Sigma}_u\mathbf{Z}^T + \mathbf{\Sigma}_e \\
\mathbf{Y} \sim N(\mathbf{T\beta}, \mathbf{Z\Sigma}_u\mathbf{Z}^T + \mathbf{\Sigma}_e )
$$

這就是一個多元線性混合效應回歸模型，大多數情況下，$\mathbf{\Sigma}_e = \sigma^2_e\mathbf{I}$。


## 其他可選擇的方差協方差矩陣特徵 

學會了上面的矩陣標記以後，就應該了解在這樣的多元混合效應模型中，對於層內方差，協方差矩陣的 $\mathbf{\Sigma_u}$ 結構初步假設是相當重要的。目前爲止我們接觸過的模型的方差協方差矩陣結構列舉如下 (爲了簡便標記都用$3\times3$ 的矩陣來表示): 

- 復合對稱結構 (compound symmetry structure - compound symmetry model) 又名爲可交換結構 (exchangeable structure)

$$
\mathbf{\sum}_{\mathbf{u}}  =\left( \begin{array}{cc}
              \sigma^2_{u} + \sigma^2_e & \sigma^2_{u}             &  \sigma^2_{u} \\
              \sigma^2_{u}              & \sigma^2_{u} + \sigma^2_e& \sigma^2_{u}  \\
              \sigma^2_{u}              & \sigma^2_{u} & \sigma^2_{u} + \sigma^2_e \\
              \end{array} \right)
$$

- 隨機系數結構 random coefficient (RC) structure

$$
\mathbf{\sum}_{\mathbf{u}}  =\left( \begin{array}{cc}
              \sigma^2_{u_{00}} + \sigma^2_e       & \sigma^2_{u_{00}} + \sigma_{u_{01}} &  \sigma^2_{u_{00}} + 2\sigma_{u_{01}} \\
              \sigma^2_{u_{00}} + \sigma_{u_{01}}  & \sigma^2_{u_{00}} + 2\sigma_{u_{01}} + \sigma^2_{u_{11}} + \sigma^2_e& \sigma^2_{u_{00}} + 3\sigma_{u_{01}} + 2\sigma^2_{u_{11}}  \\
              \sigma^2_{u_{00}} + 2\sigma_{u_{01}} & \sigma^2_{u_{00}} + 3\sigma_{u_{01}} + 2\sigma^2_{u_{11}} & \sigma^2_{u_{00}} + 4\sigma_{u_{01}} + 4\sigma^2_{u_{11}}+\sigma^2_e \\
              \end{array} \right)
$$

除了這兩個結構以外其他常見方差寫方差結構還有: 

- 自回歸結構 (autoregressive structure): 

$$
\frac{\phi}{1-\alpha^2} \left(\begin{array}{ccc}
1 & \alpha & \alpha^2 \\
\alpha & 1  & \alpha \\
\alpha^2 & \alpha  & 1
\end{array}
\right)
$$

- 無固定結構 (unstructure): 


$$
\left(\begin{array}{ccc}
\sigma_{11} & \sigma_{12}  &\sigma_{13} \\
\sigma_{21} & \sigma_{22}  &\sigma_{23} \\
\sigma_{31} & \sigma_{32}  &\sigma_{33}
\end{array}
\right)
$$

最後不要忘記了還有完全獨立結構 (不需要任何復雜模型或校正其數據間的依賴性): 

$$
\sigma^2\left(\begin{array}{ccc}
1 & 0  & 0 \\
0 & 1  & 0 \\
0 & 0  & 1
\end{array}
\right)
$$

## 其他要點評論

- 各種結構模型之間的相互比較 

    - 似然比檢驗法 the likelihood ratio test (LRT) <br> 前提是模型的固定結構不發生改變，兩個嵌套式模型之間的比較是可以使用死然比檢驗的。缺點是統計學效能可能不太理想 (low power) 
    - 模型的比較指標 information criteria <br> 就算是同一個數據，如果不同的協方差結構矩陣模型的固定效應部分也不同，似然比檢驗也不使用，這時候應該求助於赤池信息量 (Akaike's Information Criterion, AIC)，或者貝葉斯信息量 (Bayesian Criterion, BIC) 的比較。這兩個信息量都是使用的模型的似然減去相應模型的參數數量作爲評判標準。差別是 BIC 對參數的調整更加大些。但是，沒人可以保證這些信息會永遠相互認證，他們可能出現互相矛盾，也沒人可以保證使用這些信息的比較可以證明你的模型是"最佳"模型。
    
## 不平衡數據

- 有缺失值的數據，我們無法使用已知的協方差結構矩陣; 
- 隨機效應模型，隨機系數模型可以用於不平衡數據，所以即使有缺失值，我們可以從混合效應模型的結果來推測數據暗示我們數據中存在着怎樣的協方差結構;


## Practical 06-Hier


# 縱向研究數據 longitudinal data 3 {#longitudinal3}

## 第一層級的異質性 level 1 heterogeneity

目前爲止，我們使用討論過的模型，其實還默認另一個前提條件: 第一層級和第二層級的隨機誤差的方差是固定不變的 (level 1 and level 2 error variance are constant)。但是實際上我們可以把這個條件放寬，讓模型允許第一層級隨機誤差的方差根據某個解釋變量而不同，使得模型更加接近數據，這種模型被命名爲 **復雜第一層級方差模型 (complex level 1 variation)**。下面繼續使用 Asian growth data 來做說明。該數據測量了幾百名亞洲兒童在0-3歲之間幾個時間點的體重。現在我們來允許其第一層級 (每一個兒童在不同時間點測量的體重) 誤差方差隨着性別的不同而變化: $\sigma_e = f(\text{gender})$。這裏的方程爲了防止標準差變成負的而使用對數函數: 

$$
\text{log} (\sigma_e) = \delta_1I_{\text{gender = boy}} + \delta_2I_{\text{gender=girl}}
$$

這個加入了第一層級方差隨機性的模型在 R 裏可以這樣擬合: 

```{r Hier07-01, cache=TRUE}
M_growth_l1 <- lme(fixed = weight ~ age + age2 + gender, random = ~ age | id, weights = varIdent(form=~1|gender), data = growth, method = "REML", na.action = na.omit)
summary(M_growth_l1)

# 和之間默認男女兒童的誤差方差相等時的模型做比較
# 沒有顯著差異 (p = 0.09)
anova(M_growth_l1, M_growth_mix)
```

## 第二層級異質性 level 2 heterogeneity

我們還可以在模型中允許第二層級的結構不一樣，這等同於認爲這是一個三個層級的模型，其中第二層級分裂成男孩和女孩。

```{r Hier07-02, cache=TRUE}
M_growth_l2 <- lme(fixed = weight ~ age + age2 + gender, 
                   random = ~ age*gender | id,
                   data = growth, method = "REML", na.action = na.omit)
summary(M_growth_l2)

growth <- growth %>%
  mutate(boy = as.numeric(gender == "Boys"), 
         girl = as.numeric(gender == "Girls")) %>%
  mutate(age_boy = age*boy, 
         age_girl = age*girl)         

#M <- lmer(weight ~ age + age2 + girl + (age_boy |id) + (age_girl| id), data = growth, REML = TRUE)

#growth <- growth %>%
#  mutate(boy = ifelse(gender == "Boys", 1, 0), 
#         girl = ifelse(gender == "Girls", 1, 0), 
#         age_boy = age*boy, 
#         age_girl = age*girl)
#M_growth_l22 <- lme(fixed = weight ~ age + age2 + girl, 
#                    random = list( ~ girl + age_girl | id, 
#                                   ~ boy + age_boy | id),
#                   data = growth, method = "REML", na.action = na.omit)
#summary(M_growth_l22)
M_growth <- lme(fixed = weight ~ age + age2 + gender, random = ~ age|id, data = growth, method = "REML",na.action = na.omit) 
anova(M_growth_l2, M_growth)
```


## 分析策略

進行統計建模之前，請思考你想從數據中探尋什麼問題的答案? 

1. 是想了解某一個共變量在層內 (同一個體不同時間，或者統一學校不同學生之間) 的條件效應 (conditional effect)?
2. 是想探索層內和層間數據的變化程度?
3. 是想了解一個共變量的邊際效應 (marginal effect) 嗎?

如果是 1 或 2 兩個問題的話，請使用混合效應模型。如果是 1，但是那個共變量卻不是定義於層水平的，那就只好放棄回到簡單的固定效應模型。如果是 3，需要考慮使用 GEE。

### 模型選擇和建模步驟

詳細請參考 [@Verbeke1997]。

當擬合一個混合效應模型時，意味着均值的結構和協方差的結構可以被確定 (an appropriate mean structure as well as covariance structure is specified)。協方差結構，解釋了均值結構無法解釋的數據隨機變化，所以二者之間彼此高度互相依賴。另外，適當的協方差模型對於用數據進行人羣參數的有效統計推斷過程是必不可少的。

- 第一步: 

由於固定效應部分不能完美解釋數據的變異，所以協方差結構就是用來輔助解釋這部分數據變異的輔助工具。建模的起點就應該是，先建立一個飽和 (甚至是過飽和 overelaborated) 的模型給均值結構 (固定效應部分)，從而確保之後要增加的隨機效應部分不受固定效應部分的擬合錯誤影響。所以，開始建模時，要先把所有可能考慮到的固定效應全部加入模型中去 (包括連續變量的二次方形式/或其他非線性關系，包括所有變量之間的交互作用)。這樣做其實是使用過度飽和的參數使得均值結構在模型中盡量在後面加入隨機效應之前保持不變。在可選的那些數據結構中，我們也應當考慮到數據中不同層級結構可能存在的異質性。要注意的是，隨機效應部分，不能也不應該在沒有把所有可能的一次方程結構都考慮進去之後 (a random effect for the linear effect of time)，就上馬二次方程/或更高次方程的隨機效應(a random effect for the quadratic effect of time)。
然後我們把飽和模型的殘差 (residuals)，異常值 (outliers)，擬合值 (fitted values)，和可能的 (potential) 隨機效應模型作出的這些殘差，異常值，擬合值之間進行比較。

- 第二步:

一旦你在飽和模型的條件下，確認好了隨機效應應該有的形式，接下來就是逐步精簡模型固定效應部分的過程: 

1. 用 Wald 檢驗 (當使用 REML 時)，或者 LRT (使用 ML 時) 來精簡化固定效應部分。
2. 反復檢查殘差，異常值，以及擬合值跟觀測值
3. 使用模型的預測軌跡和觀測值的點做視覺比較
4. 用人話把你的模型解釋給老奶奶聽懂






# 廣義估計方程式 Generalized Estimating Equation
 
# 聚類分析 Cluster analysis/unsupervised learning  {#cluster-ana}

目前爲止，在等級回歸模型部分中，我們接觸到的回歸模型和可能存在相互依賴性的數據，都是建立在我們能夠觀察到或者實驗設計上已知的數據層級結構的前提下的。這樣的層級可以是空間上的，或者時間上的。處在相同層級的研究對象之間存在相關性，換句話說就是：層級內部的對象之間，比起層級之間的對象具有更多的相似性。

但是，在許多情況下，我們其實是無法事先知道數據的內部層級（聚類）結構的。而且我們可能需要儘可能多的獲取數據，並且從測量的數據中學習。學習數據變量與變量之間的相關性(correlation)，變量與變量之間的協方差(covariance)，個體與個體之間的相似性，從而根據獲取的數據來判斷數據內部是否存在不同的層級結構。這樣的一種對數據結構進行探索的過程，在機器學習(maching learning)中也是常常使用的，它又被叫做**非監督學習 (unsupervised learning)**。

之所以把這類尋找數據分類分層結構的過程叫做非監督學習，其實，是爲了和現在越來越豐富，多到令人髮指的那些被歸類於**監督學習(supervised learning)**的方法作爲相互對照。在監督學習中，數據內部的分層，聚類結構是事先知道的，也就是事先能夠測量或者被定義好的。事先被定義好了的數據層級結構中，我們可以使用多元變量分析，來對某些個體的特徵加以分類，也就是給數據中的未知成員分配**已知的分組**的過程。

在醫學中常見的非監督學習過程實例之一是，對於一個（全部相同疾病的）隊列研究中的受試者進行了大量的生物標幟物(biomarker)的測量與收集，可以是血液樣本的 biomarker 的測量，也可以是每名受試者的全部DNA信息。研究者希望通過這些患者的信息對他們進行同一疾病不同等級（類別，或者進程）的分類。那麼研究者需要利用這些收集來的患者信息，建立一套儘可能完善的分類的系統。

另外一個例子是，我們收集了前列腺癌患者的前列腺組織，利用基因轉錄組學 (transcriptomics) 的方法測量了每名患者成千上萬的組織內基因表達。研究者希望通過這些數據來分析，提取，並且分辨這些前列腺癌患者中可能存在的分類，或者亞型。研究者也希望知道這些分析獲得的亞型，是否會和某些已知的癌症的亞型相似或者相重合。

在商業領域中，聚類分析也是不罕見的。例如你爲某商業公司工作，那麼食品供應商可能會上門來要求你把購買食物的顧客進行類別區分，從而提供給食物供應商們一些線索，讓他們能夠更加精準的定位廣告投放人羣。



在統計學，和機器學習領域中，有許多不同的手法，可以用來輔助建立這種分類的規則，它們通常又被叫做判別分析法(discriminant analysis methods)。我們這一章和下一章着重討論

1. 聚類分析法 (cluster analysis)
2. 主成分分析法 (principal component analysis)

## 聚類分析過程

聚類分析法是一種分析不同統計測量值之間相似/差異程度的描述性分析過程。

爲什麼我們總是想對具有相似性質的事物進行歸類？其實，對事物進行區分和歸類，或者打上一些標籤，是人類文明在學習並且理解周圍的世界，從而促進科學發展的核心問題之一。在原始社會，對相似事物進行歸類有時候甚至事關生死。例如人類最初需要判定某些食物的共同特徵，區分哪些是含有毒性的，哪類動物可能是兇猛殘忍的。我們從嬰兒時期開始學習語言，學習事物/事件/人物的名稱，這其實也是一個學習對周圍的世界進行區分的學習過程。古代希臘文明的先賢哲學家亞里士多德曾經主張，人類的本能之一，就是不停地想對這個我們生活的世界發生的事情看到的事物進行類別的區分，尋找相似的特徵，區別不一樣的性質。在生物學中，甚至有由亞里士多德的學生[泰奧夫拉斯托斯(Theophrastos)](https://zh.wikipedia.org/wiki/%E6%B3%B0%E5%A5%A7%E5%BC%97%E6%8B%89%E6%96%AF%E6%89%98%E6%96%AF)創立的專門對生物進行分類的學科，生物分類學 (taxonomy)，後被瑞典人生物學家[卡爾林納斯 (Carl Linnaeus)](https://en.wikipedia.org/wiki/Carl_Linnaeus)進一步發揚光大。18世紀末，[Michel Adanson](https://en.wikipedia.org/wiki/Michel_Adanson)又爲人類引入了多元分析(polythetic)的分類系統概念，取代了之前使用單一因素(monothetic)對事物進行簡單分類的思想。很顯然，生物分類學在人類文明史中扮演了重要的角色。你應該很容易能想到達爾文提出的進化論，就是建立在前人對動植物進行了事無鉅細的分類和整理的基礎之上建立起來的重大理論突破。俄國科學家[門捷列夫](https://zh.wikipedia.org/wiki/%E5%BE%B7%E7%B1%B3%E7%89%B9%E9%87%8C%C2%B7%E4%BC%8A%E4%B8%87%E8%AF%BA%E7%BB%B4%E5%A5%87%C2%B7%E9%97%A8%E6%8D%B7%E5%88%97%E5%A4%AB)發現化學元素週期性，並且製作出了世界上第一章元素週期表，也爲人類理解原子世界奠定了基石。


在對事物進行分類這個任務上，聚類分析(cluster analysis)，和判別分析是相同的。有時候在已知對象的分類情況時我們仍然傾向於使用聚類分析的方法，用它來描述數據的一些特徵。同時也能有助於判定之後可能進行的判別分析是否準確。

簡單歸納，對分類描述過程進行量化的主要步驟有以下幾個：

1. 對於採集來的樣本數據 (statistical sample)，我們儘可能多的對它們的特徵變量進行測量。

2. 根據第一步獲得的變量信息，定義一個能夠幫助我們判定對象與對象之間相似點或者不同程度的測量指標。

3. 對這個測量指標制定一個區分的規則，或者叫做歸類的標準。

4. 對樣本進行分類。

5. 採集更多的樣本，對分類規則進行調整和完善。


### 連續型變量 continuous variables in cluster analysis

我們想象手裏的數據是一個矩陣 $X$，它的維度是 $n \times p$，用 $x_{ik}$，來表示第 $i$ 名觀察對象 $(i = 1, \dots, n)$ 的第 $k$ 個變量 $(k = 1, \dots, p)$ 的值。如果這些被測量的變量全部都是連續型變量的話，每個變量可以被使用幾何學的形式表達的 $p$ 個維度的其中一個平面上。當然，當維度超過3時，人類的無知大腦常常就無法進行有效的想象和推理，我們這裏使用簡單的三個變量，也就是三維空間來表示三個測量獲得的連續型變量：

例如我們測量了三名學生的身高，體重，以及前臂長。數據分別是：Angelo (190, 75, 30)；Dimitris (170, 75, 25)；Soren (170, 65, 30)。

```{r cluster00, echo=FALSE, cache=TRUE, fig.asp=.7, fig.width=6, fig.cap='A physical 3D space showing measurements of three variables.', fig.align='center', out.width='80%'}
library(plotly)

clus_data <- data.frame(Name = c("Angelo", "Dimitris", "Soren"), 
                        Height = c(190, 175, 170), 
                        Weight = c(75, 75, 65), 
                        Forearm = c(30, 25, 30))
p <- plot_ly(clus_data, x = ~Height, y = ~Weight, z = ~Forearm, color = ~Name) %>% 
  add_markers() %>% 
  layout(xaxis = list(range = c(0, 200))) %>% 
  layout(scene = list(xaxis = list(title = "Height (cm)"), 
                      yaxis = list(title = "Weight (cm)"), 
                      zaxis = list(title = "Forearm (cm)")))
p

```


在這個三維立體空間，我們需要定義一個變量用於丈量點與點之間的距離。其中最自然的就是歐幾里德(Euclidean)幾何距離:

$$
d_{ij} = \{\sum_{k = 1}^p(x_{ik} - x_{jk})^2\}^{\frac{1}{2}} 
$$


- 歐幾里德幾何距離又被稱爲 **L2 度量衡 (L2 metric)**。按照這個距離的定義，那麼 Angelo 和 Dimitris 之間的歐幾里德幾何距離就是：

$$
\begin{aligned}
& \{(190 - 175)^2 + (75 - 75)^2 + (30 - 25)^2 \}^{\frac{1}{2}} \\
= & \sqrt{15^2 + 0^2 + 5^2} \\ 
= & \sqrt{240} = 15.5
\end{aligned}
$$

- 曼哈頓距離 (Manhattan distance)：別名城市區塊度量衡 (cityblock metric)，或者**L1 度量衡**

$$
d_{ij} = \sum_{k = 1}^p |x_{ik} - x_{jk}|
$$

按照曼哈頓距離來定義的話，Angelo 和 Dimitris 之間的距離就是：

$$
|190 - 175| + |75 - 75| + |30 - 25| = 15 + 0 + 5 = 20
$$


後來人們發現上面提到的這兩種幾何學距離其實是閔科夫斯基度量衡 (Minkowski metric) 在 L=1 和 L=2 時的特殊情況。

閔科夫斯基度量衡的一般形式表達爲: 

$$
d_{ij} = \{ \sum_{k = 1}^p |x_{ik} - x_{jk}|^\ell \}^\frac{1}{\ell}
$$

閔科夫斯基度量衡試圖給差距較大的測量值之間增加權重用於區分彼此。不論是使用那種距離定義，這些測量距離的度量衡都具有如下的數學性質 (mathematical properties)：

1. 兩點之間的距離大於等於零, positivity <br> $d_{ij} \geqslant 0$，如果 $d_{ij} = 0$，那麼對於任何一個 $k = 1, \dots, p$，它們都是相等的 $x_{ik} = x_{jk}$。
2. 對稱性, symmetry <br> $d_{ij} = d_{ji}$
3. 三角形不等性, triangle inequality <br> $d_{ij} \leqslant d_{ih} + d_{hj}$

### 二分類或者分類型變量之間的距離 distances for binary/categorical variables

假如變量本身並不是連續型的，那麼閔科夫斯基度量衡並不適用，因爲二分量只能取0或者1。如下表所表示的，我們把 **i,j** 兩名對象的所有二分類變量進行下面的歸納總結：

| i/j | 1 | 0 |
|:---:|:-:|:-:|
|  1  | a | b |
|  0  | c | d |

其中，

- a 表示 i, j 兩名研究對象的二分類變量中，同時取 1 的變量的個數，
- b 表示 i, j 兩名研究對象的二分類變量中，i 取 1 但是 j 取 0 的變量的個數，
- c 表示 i, j 兩名研究對象的二分類變量中，j 取 1 但是 i 取 0 的變量的個數，
- d 表示 i, j 兩名研究對象的二分類變量中，同時取 0 的變量的個數。


根據這個總結表格，常用的表示兩個對象之間距離的數學度量是： 

1. 簡單匹配係數 (simple matching coefficient, SMC)，單純地計算所有的變量之中互相不一致的變量所佔的百分比： $$d_{ij} = \frac{b + c}{a+b+c+d}$$

2. 亞卡爾距離係數 (Jaccard coefficient)，則是把簡單匹配係數的分母中，d 的部分拿掉：$$d_{ij} = \frac{b + c}{a + b + c}$$

[其中亞卡爾距離係數更適合用於測量一些表達某些特質存在/不存在時兩名對象之間的距離測量 (see the "Difference with the simple matching coefficient (SMC)" session in the Wikipedia)](https://en.wikipedia.org/wiki/Jaccard_index)。



另外值得注意的是，在測量二分類變量距離的時候，三角形不等性的特質不一定會得到滿足。 (Please note that in general for dichotomous variables, the triangle inequality does not hold.)

用來計算測量對象之間距離的方法，和度量衡其實層出不窮，這裏只是簡單介紹了幾種。其餘的還有比如說由 [@Gower1971] 提出的 [Gower Index](https://cran.r-project.org/web/packages/gower/vignettes/intro.html)，該指標可以同時把測量有連續型變量和分類型變量，二分類變量等都包含進來。值得提醒的是，如果是討論非連續型測量值的對象距離，我們常常用它們之間的相似性(similarities) $s_{ij}$，而不太關注異質性 (dissimilarities) $d_{ij}$，但其是它們之間的簡單轉換關係就是 $d_{ij} = 1 - s_{ij}$。

### 定義分類方法

確定了用於衡量異質性 (dissimilarity) 距離的指標之後，我們就需要來定義分類的方法。首先把這個事先定下來的距離指標應用到我們的多元變量數據矩陣 (multivariate data matrix $\mathbf{X}$) (dimension: $n\times p$, where n indicates number of people, p indicates number of observed variables). 獲得一個形狀爲 $n\times n$ 的距離矩陣 $\mathbf{D}$ (對應上面三條數學性質中的第二條，對稱性 $d_{ij} = d_{ji}$)。獲得觀察對象的距離矩陣 $\mathbf{D}$ 之後需要決定的就是如何給對象進行分組的策略。該分組策略需要能使觀察對象被分組後，組內的對象相對組外對象更加相似，或者組外對象相對組內對象更加不同 (a sensible strategy would be to look for sets of units such that all units in that set are relatively similar to each other but relatively different from all units outside that set)。所以，用於分組策略的算法要有一定的可行性，它還要能夠量化對象之間的相對相似性 (relative similarity) 從而能夠完成以下任務：

1. 決定哪些人/對象被聚類到同一組中 (which pairs of units to join together into a cluster)

2. 每次聚類過程完成以後，重複相同的策略和算法，也就是重新計算新組成的聚類和剩餘的對象之間的距離。

3. 循環往復前兩個步驟直至全部的對象/個體都被分到各自的聚類 (cluster)。

事實上重複上述步驟，最終會把每個個體都分配到一個單獨的聚類中，也就是每個個體本身，那其實就跟沒有做聚類分析沒有區別，也沒有意義了。於是我們需要把聚類分析的過程通過圖形的方式展示出來。這樣的圖形被叫做**樹狀圖 (dendrogram)**，可以在視覺上輔助我們做出要給對象分成多少個聚類的決定。在希臘語中(Greek)，dendron 是樹的意思，樹狀圖的形狀常見的如下圖 (\@ref(fig:cluster01)) 所示，座標軸之一是所有的觀測對象的編號，另一個座標軸則是度量每個聚類或者觀測對象個體之間的距離。

```{r cluster01, cache=TRUE, fig.asp=.7, fig.width=7, fig.cap='Example of dendrogram vertically oriented, with 50 statistical units (average linkage method and Euclidean distance measure).', fig.align='center', out.width='80%', echo=FALSE}
plant <- read_dta("backupfiles/plant.dta")
plant <- plant[, 1:4]

# prepare hierarchical cluster
hc <-  hclust(dist(plant), "ave")


plot(hc, cex = 0.8, hang = -1, 
     main = "", ylab = "L2 dissimilarity measure", 
     xlab = "No. of specimen")
```

那麼回到之前如何決定聚類數量的問題上來，我們有兩種手段來輔助：

1. 層級法 (hierarchical methods)：聚合法，agglomerative； 或者分裂法， divisive。
2. 分區算法 (partitioning methods)。

層級法中的**聚合法 (agglomerative)**是指，從聚類分析的開始階段，每個獨立的對象自成一個聚類 (cluster)，所以起步於 n 個統計單位 (n statistical units)，之後的每一步聚類過程則是將度量距離相近的對象合併成爲一個聚類，直至最終所有個體歸爲唯一一個聚類。所以可以想象爲從各個枝葉彙總到一個樹幹走向各個枝葉的過程。

層級法中的**分裂法 (divisive)**則是和聚合法的聚類方向反過來，它起始於將所有觀察對象視爲唯一一個聚類，之後每一步聚類過程是將和大部分對象不太相似的個體從聚類中分裂出去，直至最終每個獨立的對象自成一個聚類。所以可以想象成從一個樹幹走向枝葉的過程。

分裂法其實十分消耗計算機的運算能力，因爲當樣本量較大時，一個 $k$ 種聚類的步驟就需要比較 $2^{k-1} -1$ 種不同的分區之間的距離。




# 主成分分析 Principal Component Analysis  {#PCA}

> A big computer, a complex algorithm and a long time does not equal science.
> ~ Robert Gentleman


```{block2, note-text, type='rmdnote'}
PCA lecture was taught by Professor [Luigi Palla](https://scholar.google.co.uk/citations?hl=en&user=p-cHaf0AAAAJ&view_op=list_works&sortby=pubdate).
```


```{r Hier-Session11, child = ('10-Hierarchical/Session11.Rmd')}
```



## Cluster analysis/PCA practical 

本次練習完成時，你將學會：

1. 如何使用聚類分析，和主成分分析法來探索一組多變量數據之間的關係；
2. 理解並懂得如何選取合適的距離測量尺度，和聚類分析方法；
3. 繪製並能夠解釋由多層聚類分析算法 (hierarchical clustering algorithm) 獲得的樹狀圖；
4. 使用主成分分析法對數據進行座標轉換，計算多個變量之間的方差，協方差矩陣，懂得如何判斷保留主成分的個數；
5. 通過把數據繪製在較低維度的主成分座標軸上來判斷數據中可能存在的潛在分層/分組。


### 使用的數據和簡單背景知識

假設你是一名生物測量技術公司的統計師，現在有這樣一組數據，包含了對某植物測量的4種生物標幟物(biomarkers)。據報道，這四種成分或許能減少你公司生產的某藥物引起的副作用。爲了嘗試分析該植物的生物特性，從該植物的50個不同樣本中，測量了這4種生物標幟物的濃度。你的任務之一是對數據進行初步分析，彙報任何你找到的可能存在的顯著特徵差異。

1. 在R裏讀入你的數據，看看這4種生物標幟物的簡單統計量和分佈，它們用的是相同的測量單位嗎？


```{r pca-1, cache=TRUE}
plant <- read_dta("backupfiles/plant.dta")
plant <- plant[, 1:4]
head(plant)
epiDisplay::summ(plant)
psych::describe(plant)
```

觀察這四個生物標幟物的簡單統計量，似乎可以認爲它們使用的應該是相似或者相同的測量單位。它們的均值在53至61之間，標準差分佈在45-51之間，而且最大值最小值之間的範圍也十分接近。

2. 這些生物標幟物能否單獨提供關於該植物的某部分特徵信息呢？思考我們該如何回答這個問題（提示：計算這些指標直接的相關係數）

我們可以通過計算這四個生物標幟物濃度測量值之間的相關係數，來觀察它們之間是否具有相似性或者是否提供了部分相似的信息。

```{r pca-2, cache=TRUE}
cor(plant)
```

從相關係數矩陣的計算結果來看，平均地，這四個生物標幟物濃度之間具有一定程度的相關性。其中，生物標幟物1和3之間呈現了四者之間最高的樣本相關係數 $(r_{13} = 0.5941)$，生物標幟物1和4之間的相關係數則最小 $(r_{14} = 0.2677)$。

3. 請描述前一步中我們計算的相關係數矩陣的維度(dimension)。

該相關係數矩陣的維度是 $4\times4$，事實上，這個矩陣的維度是由我們想要觀察分析的樣本中測量變量的個數決定的（在這裏就是四個生物標幟物）。但是這個相關係數的矩陣並不適合用於做聚類分析 (cluster analysis)，因爲相關係數本身反映的是變量之間的關係 (between variables)，並非觀察對象 (between subjects) 之間的距離(即，不是我們關心的用來把50個樣本進行分組歸類的距離變量)。

4. 再次思考問題1.的答案，思考並選擇合適的測量不同樣本個體之間距離 (distance) 的度量衡。嘗試使用簡單的聚類分析命令對樣本進行分類。

由於每個生物標記物在所有樣本中的數值基本在相似的比例或者刻度，每個標幟物在這個樣本中的標準差/方差數值也較爲相近。我們嘗試用連續變量最常見的均值測量距離指標:

```{r pca-3, cache=TRUE, fig.asp=.7, fig.width=7, fig.cap='Dendrogram for L2_avlink cluster analysis', fig.align='center', out.width='80%'}
# prepare hierarchical cluster
hc <-  hclust(dist(plant), "ave")


plot(hc, cex = 0.8, hang = -1, 
     main = "", ylab = "L2 dissimilarity measure", 
     xlab = "No. of specimen")
```

可以觀察到，樣本編號 31, 27, 17, 48, 8, 30, 3, 14, 6, 42 很快就聚合成爲一組。且這些樣本和其他樣本被聚類在不同組的過程一直維持到差異性達到100以上。我們還可以注意到，聚類過程中其他的分支呈現相對對稱的形狀。


5. 從簡單的歐幾里得距離改成歐幾里得距離平方來測量樣本之間的距離的話，圖形會變成什麼樣？

```{r pca-4, cache=TRUE, fig.asp=.7, fig.width=7, fig.cap='Dendrogram for L2sq_avlink cluster analysis', fig.align='center', out.width='80%'}
hc <- hclust(dist(plant)^2)

plot(hc, cex = 0.8, hang = -1, 
     main = "", ylab = "L2squared dissimilarity measure", 
     xlab = "No. of specimen", sub = "")
```

當使用歐幾里得距離的平方作爲樣本間隔的度量衡時，我們發現聚類的過程其實總體來說和使用歐幾里得距離本身並無本質上的區別。只是在差異性較低的地方聚類加速 (squeeze the dissimilarities at the lower end)，並且在較大的聚類區分之間變得更加明顯，視覺效果上更容易區分。

如果說，不用歐幾里得平方，而是使用簡單的曼哈頓距離 (L1 度量衡)，那麼圖形則又會呈現爲:

```{r pca-5, cache=TRUE, fig.asp=.7, fig.width=7, fig.cap='Dendrogram for L1_avlink cluster analysis', fig.align='center', out.width='80%'}

plot(cluster::agnes(plant, metric = "manhattan", stand = F), which.plots = 2, hang = -1, 
     xlab = "No. of specimen", main = "", ylab = "L1 dissimilarity measure", sub = "", cex = 0.8)
```

可以看出，當使用曼哈頓距離做度量衡時，聚類的過程和之前的沒有本質上的區別，但是圖形的樹狀分支上似乎不再左右對稱。

6. 接下來使用歐幾里得距離做度量衡，與上面的嘗試不同，這裏我們嘗試用完全連接，和單連接


# 缺失數據 Missing data 1 


```{r Hier-Session10, child = ('10-Hierarchical/Session10.Rmd')}
```

## Practical 10-Hier

```{r Hier-Practical10, child = ('10-Hierarchical/Practical10.Rmd')}
```


# 缺失數據 Missing data 2

# Further issues
