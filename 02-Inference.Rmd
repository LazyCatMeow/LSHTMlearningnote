# (PART) 統計推斷 Inference {-}

# 統計推斷的概念 {#inference-basic}

> If people do not believe that mathematics is simple, it is only because they do not realize how complicated life is.
> ~ [John von Neumann](https://zh.wikipedia.org/wiki/%E7%BA%A6%E7%BF%B0%C2%B7%E5%86%AF%C2%B7%E8%AF%BA%E4%BC%8A%E6%9B%BC)

```{block2, note-thankDan, type='rmdnote'}
The Inference lectures were orgainised and taught by Professor [Daniel Altmann](https://www.lshtm.ac.uk/aboutus/people/altmann.daniel), Professor [John Gregson](https://www.lshtm.ac.uk/aboutus/people/gregson.john), and Dr. Katy Morgan.
```

```{r Infer-Session01, child = ('02-Inference/Session01.Rmd')}
```



# 估計和精確度 Estimation and Precision {#estimation-and-precision}

```{r Infer-Session02, child = ('02-Inference/Session02.Rmd')}
```


# 卡方分佈 Chi-square distribution {#chi-square-distribution}


```{r Infer-Session03, child = ('02-Inference/Session03.Rmd')}
```

# 似然 Likelihood {#likelihood-definition}

```{r Infer-Session04, child = ('02-Inference/Session04.Rmd')}
```



# 對數似然比 Log-likelihood ratio {#llr}


```{r Infer-Session05, child = ('02-Inference/Session05.Rmd')}
```


## Inference Practical 05 



```{r Infer-practical05, child = ('02-Inference/Practical05.Rmd')}
```


# 二次方程近似法求對數似然比  approximate log-likelihood ratios {#quadratic-llr}

```{r Infer-Session06, child = ('02-Inference/Session06.Rmd')}
```



## Inference Practical 06

```{r Infer-practical06, child = ('02-Inference/Practical06.Rmd')}
```


# 假設檢驗的構建 Construction of a hypothesis test {#hypothesis-test}


## 什麼是假設檢驗 Hypothesis testing {#null-and-alter}

一般來說，我們的**假設**(或者叫**假說**) 是對與我們實驗觀察數據來自的總體(或人羣) 的**概率分佈**的描述。在參數檢驗的背景下，就是要檢驗描述這個總體(或人羣) 的**概率分佈**的參數 (parameters)。最典型的情況是，我們提出兩個互補的假設，一個叫作**零假設**(或者叫**原假設**) ，null hypothesis ($H_0$)；另一個是與之對應的(互補的) 替代假設，althernative hypothesis ($H_1/H_A$)。

例如，若 $X$ 是一個服從二項分佈的隨機離散變量 $X\sim Bin(5, \theta)$。可以考慮如下的零假設和替代假設：$H_0: \theta=\frac{1}{2}; H_1: \theta=\frac{2}{3}$。

當建立了零假設和替代假設以後，假設檢驗就是要建立如下的規則以確定：

1. 從樣本中計算所得的參數估計值爲多少時，拒絕零假設。(接受替代假設爲“真”)
2. 從樣本中計算所得的參數估計值爲多少時，零假設不被拒絕。(接受零假設爲“真”)

注意：(這一段很繞)

上面的例子是零假設和替代假設均爲簡單假設的情況，實際操作中常常會設計更加複雜的(不對稱的) 假設：即簡單的 $H_0$，複雜的 $H_1$。如此一來當零假設 $H_0$ 不被拒絕時，我們並不一定就接受之。因爲無證據證明 $H_1$ 不等於有證據證明 $H_0$。**(Absence of evidence is not evidence of absence).** 換句話說，無證據讓我們拒絕 $H_0$ 本身並不成爲支持 $H_0$ 爲“真”的證據。因爲在實際操作中，當我們設定的簡單的零假設沒有被拒絕，可能還存在其他符合樣本數據的零假設；相反地，當樣本數據的計算結果拒絕了零假設，我們只能接受替代假設。所以，反對零假設的證據，同時就是支持替代假設的證據。

在樣本空間 sample space 中，決定了零假設 $H_0$ 會被拒絕的子集 subset，被命名爲拒絕域 rejection region 或者 判別區域 critical region，用 $\mathfrak{R}$ 來標記。

## 錯誤概率和效能方程 error probabilities and the power function

這一部分也可以參考本書臨牀試驗樣本量計算 (Section \@ref(sample-size)) 部分。

```{r inference05,echo=FALSE, eval=FALSE, cache=TRUE}
dt <- read.csv("/home/ccwang/Documents/full-website-content/static/files/type12errorInfer.csv", header = T)
kable(dt, "html",align = "c",caption = "Definition of Type I and Type II error") %>%
  kable_styling(bootstrap_options = c("striped", "bordered"))  %>%
  collapse_rows(columns = c(1)) %>%
  add_header_above(c(" " = 2, "SAMPLE" = 2))
```


<table class="table table-striped table-bordered" style="margin-left: auto; margin-right: auto;">
<caption> 表 15.1 : Definition of Type I and Type II error</caption>
 <thead>
<tr>
<th style="border-bottom:hidden" colspan="2"></th>
<th style="text-align:center; border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;" colspan="2"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px;">SAMPLE</div></th>
</tr>
<tr>
<th style="text-align:center;">  </th>
   <th style="text-align:center;">  </th>
   <th style="text-align:center;"> $\underline{x} \notin \mathfrak{R}$ Accept $H_0$ </th>
   <th style="text-align:center;"> $\underline{x} \in \mathfrak{R}$ Reject $H_0$ </th>
  </tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;vertical-align: middle !important;" rowspan="2"> TRUTH </td>
   <td style="text-align:center;"> $H_0$ is true </td>
   <td style="text-align:center;"> $\checkmark$ </td>
   <td style="text-align:center;"> $\alpha$ <br> Type I error </td>
  </tr>
<tr>
<td style="text-align:center;"> $H_1$ is true </td>
   <td style="text-align:center;"> $\beta$ <br> Type II error </td>
   <td style="text-align:center;"> $\checkmark$ </td>
  </tr>
</tbody>
</table>


假如一個假設檢驗是關於總體參數 $\theta$ 的：

$$H_0: \theta=\theta_0 \text{ v.s. } H_1: \theta=\theta_1 $$

這個檢驗的效能被定義爲當替代假設爲“真”時，拒絕零假設的概率(該檢驗方法能夠檢驗出有真實差別的能力) ：

$$\text{Power}=\text{Prob}(\underline{x}\in\mathfrak{R}|H_1\text{ is true}) = 1-\text{Prob}(\text{Type II error})$$

觀察數據只有兩種可能：落在拒絕域內，或者落在拒絕域之外。第二類錯誤我們常常使用 $\beta$ 來表示，所以 $\text{Power}=1-\beta$。

檢驗的顯著性水平用 $\alpha$ 來表示。$\alpha$ 的直觀意義就是，檢驗結果錯誤的拒絕了零假設 $H_0$，接受了替代假設 $H_1$，即假陽性的概率。

$$\text{Prob}(\underline{x}\in \mathfrak{R} |H_0 \text{ is true})=\text{Prob(Type I error)}$$

### 以二項分佈爲例

用本文開頭的例子： $X\sim Bin(5,\theta)$。和我們建立的零假設和替代假設：$H_0: \theta=\frac{1}{2}; H_1: \theta=\frac{2}{3}$：

考慮兩種檢驗方法：

1. A 方法：當且僅當5次觀察都爲“成功”時才拒絕 $H_0 (\text{i.e.}\; X=5)$。所以此時判別區域 $\mathfrak{R}$ 爲 $5$。檢驗效能 $\text{Power}=1-\beta$ 爲：$Prob(X=5|H_1 \text{ is true})=(\frac{2}{3})^5=0.1317$。顯著性水平 $\alpha$ 爲 $Prob(X=5|H_0 \text{ is true})=(\frac{1}{2})^5=0.03125$。
2. B 方法：當觀察到3,4,5次“成功”時，拒絕 $H_0 (\text{i.e.} X=3,4,5)$。此時判別區域  $\mathfrak{R}$ 爲 $3,4,5$。檢驗效能 $Power$ 爲：$Prob(X=3,4,\text{ or }5|H_1 \text{ is ture})=\sum_{i=3}^5(\frac{2}{3})^i(\frac{1}{3})^{5-i}\approx0.7901$；顯著性水平 $\alpha$ 爲：$Prob(X=3,4,5|H_0 \text{ is true})=\sum_{i=3}^5(\frac{1}{2})^i(\frac{1}{2})^{5-i}=0.5$

```{r inference06, cache=TRUE}
# the power in test B
dbinom(3,5,2/3)+dbinom(4,5,2/3)+dbinom(5,5,2/3)
# the size in test B
dbinom(3,5,0.5)+dbinom(4,5,0.5)+dbinom(5,5,0.5)
```


比較上面兩種檢驗方法，可以看到，用B方法時，我們有更高的概率獲得假陽性結果(犯第一類錯誤，錯誤地拒絕 $H_0$，接受 $H_1$)，但是也有更高的檢驗效能 $1-\beta$(真陽性更高) 。這個例子就說明了，試圖提高檢驗效能的同時，會提高犯第一類錯誤的概率。實際操作中我們常常將第一類錯誤的概率固定，例如 $\alpha=0.05$，然後儘可能選擇檢驗效能最高的檢驗方法。

## 如何選擇要檢驗的統計量 {#Neyman-Pearson}

在上面的二項分佈的實驗中，“成功的次數” 是我們感興趣的要檢驗的統計量。但也可能是第一次出現 “成功” 之前的實驗次數，或者，任何與假設相關的統計量。相似的，如果觀察不是離散變量而是連續的，可以拿來檢驗的指標就有很多，如均值，中位數，衆數，幾何平均值等。

幸運地是，當明確了零假設和替代假設後，我們可以利用 [Neyman-Pearson lemma](https://en.wikipedia.org/wiki/Neyman%E2%80%93Pearson_lemma) 似然比公式^[區分與之前討論的對數似然比 (Section \@ref(llr))，之前討論的對數似然比指的是**所有的似然和極大似然**之間的比，此處的似然比只是純粹在探討兩個假設之間的似然比，**與極大似然無關**。]:

來決定使用哪個統計量做檢驗**最有效**：

$$\text{Neyman-Pearson lemma}=\frac{L_{H_0}}{L_{H_1}}$$

這公式很直觀，因爲當觀察數據更加支持 $H_1$ 時 ($L_{H_1}$ 更大)，$H_0$ 的可能性相對更小，就更應該被拒絕。而且，由於似然比越小，他的對數就越小，實際計算時我們常使用對數似然比：$\ell_{H_0}-\ell_{H_1}$。

問題來了，那到底要多小才算小？這個進入拒絕域的閾值由兩個指標來決定：

1. 被檢驗統計量的樣本分佈 (the sampling distribution of the test statistic)
2. 第一類錯誤概率 $\alpha$ (the required value of $\alpha$)

### 以已知方差的正態分佈爲例

假如已知 $X_1, \cdots, X_n \stackrel{i.i.d}{\sim} N(\mu, \sigma^2)$  而且方差 $\sigma^2$ 也是已知的。如果令 $H_0: \mu=5\; ;H_1: \mu=10$  可以通過如下的方法找到我們需要的最佳檢驗統計量 <u>best statistic</u> 根據之前的推導 (Section \@ref(llr)) 可知正態分佈的似然方程如下：

$$\ell(\mu|\underline{x}) =-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2$$

所以已知 $\sigma^2$ 時，我們的零假設和替代假設之間的對數似然比 $\ell_{H_0}-\ell_{H_1}$ 爲:

$$\ell_{H_0}-\ell_{H_1}=-\frac{1}{2\sigma^2}(\sum_{i=1}^n(x_i-5)^2-\sum_{i=1}^n(x_i-10)^2)$$

然而，我們只需要考慮隨着數據變化的部分，所以忽略掉不變的部分^[Rememer that $\ell_{H_0}-\ell_{H_1}$ is a random variable: the data varies **each time** we sample, with consequently varying relative support for the hypotheses, and so we are only interested in that part of  $\ell_{H_0}-\ell_{H_1}$ which depends on the results, the data, which vary with each sample (i.e. which contains the random part); the constant part provides no information on the relative support the data give to the hypotheses, so we ignore it.]：



$$
\begin{aligned}
\ell_{H_0}-\ell_{H_1} & = -(\sum_{i=1}^n(x_i-5)^2-\sum_{i=i}^n(x_i-10)^2)\\
                & = 75n - 2\times(10-5)\sum_{i=1}^nx_i \\
\end{aligned}
$$

所以只要樣本和 (sum of sample) $\sum_{i=1}^nx_i$ <u>(最佳統計量 best statistic)</u> 足夠大，零假設就會被拒絕。而且注意到最佳統計量可以乘以任何常數用作新的最佳統計量。爲了方便我們就用樣本均數 $\frac{1}{n}\sum_{i=1}^nx_i$ 作此處的最佳統計量。所以此時，我們的最佳檢驗就是當樣本均值足夠大，超過某個閾值時，我們拒絕零假設。而且，樣本均值的樣本分佈是可以知道的，這樣就便於我們繼續計算下一步：拒絕域 (判別區域) 。



## 複合假設 composite hypotheses

目前爲止我們討論的假設檢驗限制太多，實際操作時，我們多考慮類似如下的假設：

1.  $H_0: \theta=\theta_0 \;\text{v.s.}\; H_1: \theta>\theta_0$ [**單側**的替代假設]
2.  $H_0: \theta=\theta_0 \;\text{v.s.}\; H_1: \theta\neq\theta_0$ [**雙側**的替代假設]

所以我們面臨的問題是簡單假設中用於判定的最佳統計量，是始終如一地適用？我們一一來看：

### 單側替代假設

本章目前爲止的推導中我們發現，樣本均值越大，零假設和替代假設的對數似然比 $\ell_{H_0}-\ell_{H_1}$ 越小。所以我們在樣本均值較大時，拒絕零假設，那麼就可以把原來使用的簡單替代假設 $H_1: \mu=10$ 擴展爲，任意大於 $5$ 的 $\mu$ ，即 $\mu>5$ 。因爲大於 $5$ 的任何均值，都提供了更小的對數似然比，都會讓我們拒絕零假設。所以在正態分佈時，單側替代假設的最佳檢驗統計量還是**樣本均值**。

### 雙側替代假設

雙側替代假設的情況下，我們無法繼續使用樣本均值作爲最佳統計量。因爲當我們想檢驗：$H_0: \mu=5 \;\text{v.s.}\; H_1: \mu<5$ 時，必須獲得足夠小的樣本均值才能讓我們拒絕零假設。此處暫且先按下不表。

## 爲反對零假設 $H_0$ 的證據定量

重新再考慮複合假設：$H_0: \theta=\theta_0\;\text{v.s.}\;H_1: \theta>\theta_0$ 假如存在一個總是可用的最佳檢驗統計量，用 $T$ 來標記 (或 $T(x)$)， 這個統計量足夠大時，我們拒絕 $H_0$。 別忘了我們還要給事先固定好的顯著性水平 $\alpha$ 定義與之相關的判別區域：

$$\text{Prob}(\underline{x}\in\mathfrak{R}|H_0)=\alpha$$

如果我們知道 $T$ 的樣本分佈，我們就可以使用一個閾值 $c$ 來定義這個判別區域：

$$Prob(T\geqslant c|H_0)=\alpha$$

更加正式的，我們定義判別區域 $\mathfrak{R}$ 爲：

$$\{\underline{x}:\text{Prob}(T(x)\geqslant c|H_0)=\alpha\}$$

換句話說，當統計量 $T>c$ 時，我們拒絕 $H_0$ 。如果先不考慮拒絕或不拒絕的二元判定，我們可以用一個連續型測量值來量化反對零假設 $H_0$ 的證據。再考慮從觀察數據中獲得的 $T$ ，即數據告訴我們的 $t$ 。所以，當 $t$ 值越大，說明觀察值相對零假設 $H_0$ 越往極端的方向走。因此我們可以用 $T$ 的樣本分佈來計算觀察值大大於等於這個閾值(極端值) 時的概率：

$$p=\text{Prob}(T\geqslant t|H_0)$$

這個概率公式被稱爲是單側 $p$ 值 **(one-side p-value)**。單側 $p$ 值越小，統計量 $T$ 的樣本空間就有越小比例(越強) 的證據支持零假設 $H_0$。

我們把這以思想用到假設檢驗中時，就可以認爲：

$$p<\alpha \Leftrightarrow t>c$$

所以用我們一貫的設定 $\alpha=0.05$，所以如果計算獲得 $p<0.05$ 我們就認爲獲得了足夠強的拒絕零假設 $H_0$ 的證據。

### 回到正態分佈的均值比較問題上來(單側替代假設) {#normal-mean-compare}

繼續考慮 $X_1,\cdots,X_n\stackrel{i.i.d}{\sim} N(\mu, \sigma^2)$，假設 $\sigma^2=10$，我們要檢驗的是 $H_0: \mu=5 \;\text{v.s}.\; H_1: \mu>5$

1.  確定最佳檢驗統計量：已經證明過，單側替代假設的最佳檢驗統計量是**樣本均值 $\bar{x}$**。
2.  確定該統計量的樣本分佈：已知樣本均數的樣本分佈是 $\bar{X}\sim N(\mu,\sigma^2/n)$ 。<br>$\Rightarrow Z=\frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \sim N(0,1)$，所以在 $H_0$ 條件下，$\Rightarrow Z=\frac{\bar{X}-5}{\sqrt{10}/\sqrt{n}} \sim N(0,1)$
3.  所以當一個檢驗的顯著性水平設定爲 $\alpha=0.05$ 時，我們用判別區域 $\mathfrak{R}$，使統計量據落在該判別區域內的概率爲 $0.05$：<br> $\text{Prob}(\bar{X}\geqslant c|H_0) = 0.05$ <br> 已知在標準正態分佈時，$\text{Prob}(Z\geqslant1.64)=0.05=\text{Prob}(\frac{\bar{X}-5}{\sqrt{10}/\sqrt{n}}\geqslant1.64)$
4.  假設樣本量是 $10$，那麼數據的判別區域 $\mathfrak{R}$ 就是 $\bar{X}\geqslant6.64$。
5.  假設觀察數據告訴我們，$\bar{X}=7.76$ 。那麼這一組觀察數據計算得到的統計量落在了判別區域內，就提供了足夠的證據拒絕接受 $H_0$。
6.  我們可以給這個觀察數據計算相應的單側 $p$ 值：<br> $p=\text{Prob}(\bar{X}\geqslant7.76|H_0)=\text{Prob}(Z+5\geqslant7.76)\\=\text{Prob}(Z\geqslant2.76)=0.003$ <br> 所以，觀察數據告訴我們，在 $H_0$ 的前提下，觀察值出現的概率是 $0.3\%$ 。即，在無數次**重複**取樣實驗中，僅有 $0.3\%$ 的結果可以給出支持 $H_0$ 的證據。因此我們拒絕 $H_0$ 接受 $H_1$。


## 雙側替代假設情況下，雙側 $p$ 值的定量方法


```{r assymmetric, echo=FALSE, fig.asp=.7, fig.width=5, fig.cap='Deliberately use an assymmetrical distribution to highlight the issues', fig.align='center', out.width='90%', cache=TRUE, cache=TRUE}
x <- rchisq(1000000, 5)
q100 <- quantile(x, 1)
q95 <- quantile(x, .95)
q05 <- quantile(x, 0.05)
q00 <- quantile(x,0)
dens <- density(x)
plot(dens, xlim=c(0,20), frame.plot = FALSE, main=" ", yaxs="i",
     ylim=c(0,0.18), xlab="")
x1 <- max(which(dens$x <= q05))
x2 <- min(which(dens$x > q00))
x3 <- min(which(dens$x >= q95))
x4 <- max(which(dens$x <  q100))
with(dens, polygon(x=c(x[c(x1,x1:x2,x2)]), y= c(0, y[x1:x2], 0), col="gray"))
with(dens, polygon(x=c(x[c(x3,x3:x4,x4)]), y= c(0, y[x3:x4], 0), col="gray"))

axis(1, at=c(1.17,3,11.17), labels = c(expression(t[obs]), expression(paste("E(T|",theta,")", sep = "")), expression(t^"'")))
```

此處故意使用一個左右不對稱的概率密度分佈來解釋。

現在的替代假設是雙側的：

$$H_0: \theta=\theta_0 \;\text{v.s.}\; H_1:  \theta\neq\theta_0$$

正常來說，雙側的假設檢驗應該分成兩個單側檢驗。即：

1. $H_1: \theta>\theta_0$;
2. $H_1: \theta<\theta_0$.

每個單側檢驗都有自己的最佳檢驗統計量。令 $T$ 是 1. 的最佳檢驗統計量，該統計量的樣本分佈如上圖 \@ref(fig:assymmetric) 所示(左右不對稱) 。假如觀察數據給出的統計量爲 $t_{\text{obs}}$，那麼在概率上反對零假設的情況可以有兩種：

1. $T\geqslant t_{\text{obs}}$ 其中， $\text{Prob}(T\geqslant t_{\text{obs}}|H_0)=\tilde p$;
2. $T\leqslant t^\prime$ 其中，$t^\prime$ 滿足： $\text{Prob}(T\leqslant t^\prime|H_0) =\tilde p$。(圖\@ref(fig:assymmetric))

所以概率密度分佈兩側的距離可以不對稱，但是只要左右兩側概率密度分佈的面積($=\tilde p$)相同，那麼就可以直接認爲，雙側 $p$ 值是兩側面積之和 ($p=2\times \tilde p$)，且觀察數據提供的統計量落在這兩個面積內的話，都足以提供證據拒絕零假設 $H_0$。


注意：

- 被選中的 $t^\prime$ 值大小不大可能滿足：$|t^\prime - E(T|\theta_0)|=|t_{obs}-E(T|\theta_0)|$。因爲那只有在完全左右對稱的分佈中才會出現。但是，此處我們關心的是面積左右兩邊的尾部要相等即可，所以我們只需要知道右半邊，較大的那個 $t_{obs}$ 就完全足夠了。

回到上面的均值比較問題 (Section \@ref(normal-mean-compare))。現在我們要進行雙側假設檢驗，即： $H_0: \mu=5 \text{ v.s. } H_1: \mu\neq5$，最佳統計量依然還是樣本均數 $\bar{X}$。數據告訴我們說 $\bar{X}=7.76$，因此雙側 $p$ 值就是將已求得的單側 $\tilde p$ 值乘以 $2$： $\text{two-sided } p=2\tilde p= 0.006$

當然，實際操作中我們很少進行這樣繁瑣的論證，多數情況下就直接報告雙側 $p$ 值。


## 假設檢驗構建之總結 {#test-summary}

按照如下的步驟一一構建我們的假設檢驗過程：

1. 先建立**零假設，和替代假設** (Section \@ref(null-and-alter))；
2. 定義**最佳檢驗統計量** (用 Neyman-Pearson lemma) (Section \@ref(Neyman-Pearson))；
3. 取得零假設條件下，最佳統計量的樣本分佈(通常都較爲困難，有時候我們會傾向於使用“不太理想”，但是計算較爲簡便的過程。) ；
4. 定義**拒絕域(判別區域) ** (常用 $\alpha=0.05$) ；
5. 計算**觀察數據**的檢驗統計量；
6. 如果觀察數據的檢驗統計量落在了提前定義好的拒絕域內，那麼我們的檢驗結論就是：觀察數據**拒絕了零假設支持替代假設**。然而在實際操作時，如果發現數據的檢驗統計量不在拒絕域內，我們僅僅只能下結論說：觀察數據**無法拒絕零假設**(**而不是接受零假設！**) ；
7. 報告計算得到的反對零假設的定量 $p$ 值。

作爲統計學家，我們的任務是評價數據提供的證據，而不是簡單的去接受或者拒絕一個假設。

## 練習題

### Q1

某種藥物有兩種使用方法：可以口服，也可以注射。兩種方法都被認爲可以使血漿中藥物濃度在24小時候達到相似的平均水平，$3 \mu \text{g/L}$。已知口服該藥物後，濃度的方差爲 $1$，而如果是注射的話方差只有 $1/4$。因此設計了一個口服臨牀實驗，觀察到24小時後血漿中藥物濃度數據爲：2.54, 0.93, 2.75, 4.51, 3.71, 1.62, 3.01, 4.13, 2.08, 3.33。假設這組觀察數據獨立同分佈 $\stackrel{i.i.d}{\sim} N(3, \sigma^2)$

1. 證明以下的假設的最佳檢驗統計量是 $\sum_{i=1}^{10}(x_i-3)^2$：
    $$H_0: \sigma^2=1/4 \text{ v.s. } H_1: \sigma^2=1$$

**解**

根據 Neyman-Pearson lemma (Section \@ref(Neyman-Pearson)) 來判斷最佳檢驗統計量：

下面用 $\sigma^2_0, \sigma^2_1$ 分別標記零假設和替代假設時的方差。

$$
\begin{aligned}
L(\sigma^2|\underline{x},\mu=3) &= \prod_{i=1}^n\frac{1}{\sqrt{2\pi\sigma^2}}\text{exp}(-\frac{1}{2}(\frac{x_i-3}{\sigma})^2) \\
\Rightarrow \ell(\sigma^2) &=-\frac{1}{2}\sum_{i=1}^n\text{log}\sigma^2-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-3)^2 \\
  &= -\frac{n}{2}\text{log}\sigma^2-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-3)^2 \\
\Rightarrow \ell(\sigma_0^2)-\ell(\sigma_1^2)&= \frac{n}{2}\text{log}\sigma_1^2+\frac{1}{2\sigma_1^2}\sum_{i=1}^n(x_i-3)^2\\
&\;\;\;\;\;\;-\frac{n}{2}\text{log}\sigma_0^2-\frac{1}{2\sigma_0^2}\sum_{i=1}^n(x_i-3)^2\\
&=\frac{n}{2}(\text{log}\sigma_1^2-\text{log}\sigma_0^2)+\frac{1}{2}(\frac{1}{\sigma_1^2}-\frac{1}{\sigma_0^2})\sum_{i=1}^n(x_i-3)^2\\
&=\frac{n}{2}\text{log}\frac{\sigma_1^2}{\sigma_0^2}+\frac{1}{2}(\frac{1}{\sigma_1^2}-\frac{1}{\sigma_0^2})\sum_{i=1}^n(x_i-3)^2
\end{aligned}
$$

觀察上面的式子就會發現，當實驗重複後唯一會發生變化的就是後面的 $\sum_{i=1}^n(x_i-3)^2$。
由於，$\sigma_0^2=1/4, \; \sigma_1^2=1$，所以 $(\frac{1}{\sigma_1^2}-\frac{1}{\sigma_0^2})<0$。那麼當 $\sum_{i=1}^n(x_i-3)^2$ 越大，$\ell(\sigma_0^2)-\ell(\sigma_1^2)$ 就越小。因此，這就是我們尋找的最佳檢驗統計量。

2. 證明上面的檢驗統計量總是可以作爲最佳檢驗統計量，用於檢驗單側替代假設：$H_1: \sigma^2>1/4$。

上面的替代假設中 $\sigma_1^2=1$，如果將替代假設改成 $\sigma_1^2>1/4$，那麼 $(\frac{1}{\sigma_1^2}-\frac{1}{\sigma_0^2})<0$ 依然成立。所以，$\sum_{i=1}^n(x_i-3)^2$，或者這部分乘以任何一個不變的常數依然是替代假設爲 $H_1: \sigma^2>1/4$ 時的最佳檢驗統計量。

3. 在 $H_0$ 條件下，樣本分佈 $\sum_{i=1}^{10}(x_i-3)^2$ 是怎樣的分佈？利用這個分佈來定義顯著性水平爲 $\alpha=0.05$ 時的拒絕域。

在$H_0$ 條件下，有：
$$X_1,\cdots,X_n\stackrel{i.i.d}{\sim}N(3,1/4)\\
\Rightarrow \frac{X_i-3}{\sqrt{1/4}}\sim N(0,1)\\
\Rightarrow (\frac{X_i-3}{\sqrt{1/4}})^2 \sim \mathcal{X}_1^2\\
\Rightarrow \sum_{i=1}^{10}(\frac{X_i-3}{\sqrt{1/4}})^2 \sim \mathcal{X}_{10}^2\\
\Rightarrow 4\sum_{i=1}^{10}(X_i-3)^2\sim \mathcal{X}_{10}^2\\
\text{Let } T=\sum_{i=1}^{10}(X_i-3)^2\\
\Rightarrow 4T \sim \mathcal{X}_{10}^2$$

拒絕域被定義爲檢驗統計量取大於等於某個臨界值時概率爲 $0.05$，即 $\text{Prob}(T\geqslant t)=0.05$

$$\text{Prob}(4T\geqslant \mathcal{X}^2_{10,0.95})=0.05\\
\Rightarrow \text{Prob}(T\geqslant 1/4\mathcal{X}^2_{10,0.95})=0.05$$

所以，此處當顯著性水平定爲 $\alpha=0.05$ 時，拒絕域就是要大於自由度爲 $10$ 的卡方分佈的 $95\%$ 分位點。

4. 在 $H_0$ 條件下，該檢驗統計量的正態分佈模擬是怎樣的？

根據**中心極限定理**(Section \@ref(CLT)) 和 **卡方分佈的性質** (Section \@ref(chi-square-distribution))

$$n\rightarrow \infty, X_n^2\sim N(n, 2n)$$

所以近似地，

$$\mathcal{X}_{10}^2\sim N(\text{E}(\mathcal{X}_{10}^2)=10,\text{Var}(\mathcal{X}_{10}^2)=20)\\
\Rightarrow 4T\sim \text{approx} N(10,20)\\
\Rightarrow \frac{4T-10}{\sqrt{20}} \stackrel{\cdot}{\sim} N(0,1)$$


5. 用上面的正態分佈模擬，和觀察嘗試對單側替代假設作統計檢驗並依據所得結果作出結論：$$H_0: \sigma^2=1/4 \text{ v.s. } H_1: \sigma^2>1/4$$

用上面的正態分佈近似法，我們可以計算拒絕域：

$$\text{Prob}(\frac{4T-10}{\sqrt{20}}\geqslant Z_{0.95})=0.05$$

已知標準正態分佈的 $95\%$ 分位點取值 $1.64$，所以拒絕域：

$$\frac{4T-10}{\sqrt{20}}\geqslant 1.64\\
\Rightarrow T\geqslant1/4(10+1.64\sqrt{20})=1/4\times17.33$$

由觀察數據可得：$T=11.5$ ，所以觀察數據的檢驗統計量落在了拒絕域內。我們的結論是：觀察數據提供了極強的證據證明在顯著性水平爲 $5\%$ 時，口服該藥物24小時後的血漿藥物濃度的方差大於 $1/4$。


# 假設檢驗的近似方法 {#approximation-hypo-test}

本章教你怎麼徒手搞似然比檢驗 (likelihood ratio test)，Wald 檢驗 (Wald test)，和 Score 檢驗 (Score test)。

## 近似和精確檢驗 approximate and exact tests

前一章描述了如何用對數似然比尋找最佳檢驗統計量 (Section \@ref(Neyman-Pearson))。一旦找到並確定了最佳檢驗統計量，接下去還需要確定這個最佳檢驗統計量的樣本分佈，用定好的顯著性水平($\alpha=0.05$)確定拒絕域，再使用觀察數據計算數據本身的統計量，然後對反對零假設的證據定量(計算 $p$ 值) 。前一章用的例子均來自於正態分佈，所以我們都能夠不太複雜地獲得樣本均值，樣本方差等較容易取得樣本分佈的檢驗統計量。正如我們在前一章最後部分 (Section \@ref(test-summary)) 總結的那樣，**大多數情況下我們沒有那麼幸運**。最佳檢驗統計量的樣本分佈會很難確定。所以另一個進行假設檢驗的途徑就是近似檢驗法 (approximate tests)。

## 精確檢驗法之 -- 似然比檢驗法 Likelihood ratio test {#LRT}

記得我們之前說到，簡單假設 $H_0: \theta=\theta_0\text{ v.s. } H_1: \theta=\theta_1$ 的檢驗的最佳檢驗統計量可以使用 Neyman-Pearson lemma (尼曼皮爾森輔助定理) (Section \@ref(Neyman-Pearson)) 來確定：

$$\ell_{H_0}-\ell_{H_1} = \ell(\theta_0)-\ell(\theta_1)$$

如果假設變成了複合型假設：$H_0: \theta\in\omega_0 \text{ v.s. } H_1: \theta\in\omega_1$。此時，$\omega_0, \omega_1$ 分別指兩種假設條件下我們關心的總體參數的可能取值範圍。那麼可以把上面的定理擴展成，在 $\omega_0, \omega_1$ 兩個取值範圍內，零假設和對立假設在給出的觀察數據條件下的極大似然之比：

$$\text{log}\frac{\text{max}_{H_0}[L(\theta|data)]}{\text{max}_{H_1}[L(\theta|data)]}=\text{max}_{H_0}[\ell(\theta|data)]-\text{max}_{H_1}[\ell(\theta|data)]\\
=\text{max}_{\theta\in\omega_0}[\ell(\theta|data)]-\text{max}_{\theta\in\omega_1}[\ell(\theta|data)]$$

典型的假設檢驗情況下，我們面對的是簡單的零假設和複合型的替代假設：

$$H_0: \theta=\theta_0 \text{ v.s. } H_1: \theta\neq\theta_0$$

所以在這個情況下，套用擴展以後的 Neyman-Pearson lemma：

$$\text{max}_{H_0}[\ell(\theta)]-\text{max}_{H_1}[\ell(\theta)]=\ell(\theta_0) - \ell(\hat\theta)=llr(\theta_0)$$

之前討論對數似然比 (Section \@ref(llr-chi)) 時我們已知：

$$\text{Under }H_0: \theta=\theta_0\Rightarrow -2llr(\theta_0)\stackrel{\cdot}{\sim}\mathcal{X}_1^2$$

於是利用自由度爲 $1$ 的卡方檢驗的特徵我們就可以爲反對零假設的證據定量，計算關鍵的拒絕域。如果說顯著性水平爲 $\alpha$ 那麼，我們拒絕零假設 $H_0:\theta=\theta_0$ 的拒絕域是：

$$-2llr(\theta_0)>\mathcal{X}^2_{1,1-\alpha}$$

當使用 $\alpha=0.05$ 時，這個關鍵的拒絕域就是：$-2llr(\theta_0)>3.84$。

這就是傳說中的 (對數) 似然比檢驗，(log-)Likelihood ratio test (LRT)。

LRT 的優點：

1. 簡單；
2. $p$ 值不會被參數尺度 (parameter scale) 左右，也就是說如果我們對參數進行了數學轉換 (Section \@ref(para-trans)) 也不會影響似然比檢驗計算得到的 $p$ 值大小。

LRT 的缺點：

1. 非正態分佈的數據時，LRT 只能算是漸進有效 (asymptotic valid)，即樣本量要足夠大時結果才能令人滿意；
2. 無法總是保證這是最佳檢驗統計量；
3. 需要計算兩次對數似然 (MLE 和 零假設時)。

## 練習題
假設有在觀察對象 $n=100$ 人中發生了 $k=40$ 個事件。假定數據服從二項分佈，已知人羣中每個人發生該事件的概率爲 $\pi_0=0.5$。嘗試計算似然比檢驗統計量：$-2llr(\pi_0)$，並進行顯著性水平爲 $\alpha=0.05$ 的假設檢驗：$H_0: \pi=\pi_0 \text{ v.s. }H_1: \pi\neq\pi_0$

**解**

$$
\begin{aligned}
&\because f(k=40|\pi) = \binom{100}{40}\pi^{40}(1-\pi)^{100-40} \\
&\text{Ignoring terms}  \text{ not with }  \pi \\
&\therefore \ell(\pi|k=40) = 40\text{log}\pi+60\text{log}(1-\pi) \\
&\Rightarrow \ell^\prime(\pi|k=40) = \frac{40}{\pi}-\frac{60}{1-\pi} \\
&\text{Let }   \ell^\prime(\pi|k=40) = 0 \\
&\Rightarrow   \frac{40}{\pi}-\frac{60}{1-\pi} =0 \\
&\Rightarrow  \text{ MLE } \hat\pi=0.4 \\
&\Rightarrow llr(\pi_0)=\ell(\pi_0)-\ell(\hat\pi) \\
&\;\;\;\;\;\;\;\;\;=40\text{log}0.5+60\text{log}(1-0.5)-40\text{log}0.4-60\text{log}(1-0.4)\\
&\;\;\;\;\;\;\;\;\;=-2.013\\
&\Rightarrow -2llr=4.026 > \text{Pr}(\mathcal{X}^2_{1,0.95})=3.84
\end{aligned}
$$

所以當顯著性水平爲 $\alpha=0.05$ 時，數據提供了足夠拒絕零假設的證據。該事件在此人羣中發生的概率要低於人羣的 $0.5$。

## 近似檢驗法之 -- Wald 檢驗 {#Wald}

和 LRT 一樣， Wald 檢驗也適用於檢驗 $H_0: \theta=\theta_0 \text{ v.s. } H_1: \theta\neq\theta_0$。但是本方法其實是使用對數似然比方程的近似二次方程 (Section \@ref(quadratic-llr))。相比之下，LRT 使用的是精確的對數似然比，只對檢驗統計量 $-2llr$ 進行了自由度爲 $1$ 的卡方分佈 $\mathcal{X}_1^2$ 近似。本節介紹的 Wald 檢驗過程中使用了兩次近似，一次是計算對數似然比時使用了二次方程，一次則是和 LRT 一樣對檢驗統計量進行 $\mathcal{X}_1^2$ 近似。

根據之前的對數似然比近似結論 (Section \@ref(quadratic-llr2)) ：

$$llr(\theta)\approx-\frac{1}{2}(\frac{M-\theta}{S})^2\text{ asymptotically}$$

其中，$M$ 是 $\text{MLE }\hat\theta$，$S=\sqrt{\left.-\frac{1}{\ell^{\prime\prime}(\theta)}\right\vert_{\theta=\hat{\theta}}}$

而且前一節我們也看到，

$$
\text{Under }H_0: \theta=\theta_0\Rightarrow -2llr(\theta_0) \stackrel{\cdot}{\sim}\mathcal{X}_1^2\\
\Rightarrow -2\times-\frac{1}{2}(\frac{M-\theta_0}{S})^2 \stackrel{\cdot}{\sim}\mathcal{X}_1^2 \\
\Rightarrow (\frac{M-\theta_0}{S}) \stackrel{\cdot}{\sim} N(0,1)\\
\text{Let } W=(\frac{M-\theta_0}{S})
$$

$W$ 就是我們在 Wald 檢驗中用到的檢驗統計量。接下來就可以計算給定顯著水平 $\alpha$ 時的拒絕域，給 $p$ 值定量：

當 $W>N(0,1)_{1-\alpha/2}$ 或 $W<N(0,1)_{\alpha/2}$時，拒絕 $H_0: \theta=\theta_0$；

或者，當 $W^2>\mathcal{X}^2_{1,1-\alpha}$ 時，拒絕 $H_0: \theta=\theta_0$。

這就是我們心心念念的 Wald 檢驗。

```{r llr-wald, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap='Likelihood ratio and Wald tests: solid (green) line is log-likelihood ratio, dashed (red) is quadratic approximation', fig.align='center', out.width='90%', cache=TRUE}
knitr::include_graphics("img/Selection_083.png")
```
上圖 \@ref(fig:llr-wald) 解釋了 LRT 和 Wald 檢驗的不同之處。紅色虛線是二次方程，用於近似似然比方程(綠色實線) 。二者在 $\text{MLE}=\hat\theta$ 時同時取極大值。Wald 檢驗的是，數據提供的 $\hat\theta$ 和我們想要比較的零假設 $\theta_0$ 之間的橫軸差距。在檢驗量 $W$ 中我們還把這個差除以觀察數據均值的標準差(數據的標準誤) 。 如果數據本身波動大，$W$ 的分母(標準誤) 較大，那麼即使 $\hat\theta - \theta_0$ 保持不變，統計量變小，反對零假設的證據也就越小。反觀，LRT 檢驗的檢驗統計量就是上圖 \@ref(fig:llr-wald) 顯示的縱軸差 $\ell(\theta_0)-\ell(\hat\theta)$ 的大小。二者之間的關係被直觀的顯示在圖中。

Wald 檢驗優點：

1. 比 LRT 略簡單；
2. 不必再計算零假設時的對數似然，只需要 $MLE$ 和它的標準誤。

Wald 檢驗缺點：

1. 兩次近似(LRT只用了一次近似) ；
2. 無法總是保證這是最佳檢驗統計量；
3. 參數如果被數學轉換 (Section \@ref(para-trans))，$p$ 值會跟着變化。



### 再以二項分佈爲例

在 $n$ 個實驗對象中觀察到 $k$ 個事件，使用參數爲 $\pi$ 的二項分佈模型來模擬。使用 Wald 檢驗法對下列假設做出統計檢驗： $H_0: \pi=\pi_0 \text{ v.s. } H1: \pi\neq\pi_0$。將參數 logit 轉換 (log-odds) 之後，對轉換後的新參數再做一次 Wald 檢驗。

**解**

根據之前的二次方程近似法推導 (Section \@ref(quadratic-binomial-approx))：

$$
\begin{aligned}
& M=\text{MLE}=\hat\pi=\frac{k}{n}=p\\
& S=se(\hat\pi)=\sqrt{\frac{p(1-p)}{n}}\\
& \Rightarrow \text{Under } H_0: \pi=\pi_0\\
& W=(\frac{p-\pi_0}{\sqrt{\frac{p(1-p)}{n}}})\stackrel{\cdot}{\sim} N(0,1)
\end{aligned}
$$

根據參數數學轉換的性質 (Section \@ref(para-trans))

$$
\begin{aligned}
&\text{New parameter } \beta=g(\pi)=\text{logit}(\pi)=\text{log}\frac{\pi}{1-\pi}\\
& \text{MLE}=\text{logit}(\hat\pi)=\text{log}\frac{\hat\pi}{1-\hat\pi} \\
& \text{Here we need to use delta-method to approximate standard error of } g(\pi)\\
& S=se[g(\hat\pi)]\approx g^\prime(\pi)\times se(\hat\pi) \\
& = \frac{1}{\hat\pi(1-\hat\pi)}\sqrt{\frac{p(1-p)}{n}}\\
& =\sqrt{\frac{1}{k}+\frac{1}{n-k}} \\
& \text{So the Wald test becomes}\\
& H_0: \beta=\beta_0\\
& \Rightarrow W=\frac{\text{log}(\frac{\hat\pi}{1-\hat\pi})-\text{log}(\frac{\pi_0}{1-\pi_0})}{\sqrt{\frac{1}{k}+\frac{1}{n-k}}}\stackrel{\cdot}{\sim} N(0,1)
\end{aligned}
$$

可見對參數進行了數學轉換之後，檢驗統計量的計算式發生了變化。因此 $p$ 值也會不同。

## 近似檢驗法之 -- Score 检验 {#Score}

注意到 Wald 檢驗使用的近似二次方程是在 MLE， 也就是極大似然比時的點 $\hat\theta$ 和對數似然比方程取相同的值和相同曲率 (二次導數)。
可以類比的是，Score 检验是基于另一種二次方程模擬，Score 檢驗的近似二次方程和對數似然比方程在零假設 ($\theta_0$) 時取相同的曲率。所以，Score 檢驗使用的近似方程在 $\theta_0$ 時和對數似然比方程在相同位置時的傾斜度 (一階導數)，和曲率 (坡度的變化程度，二階導數) 相同。所以令 $U$ 爲對數似然比方程在 $\theta_0$ 時的坡度，定義 $V$ 是對數似然比方程在 $\theta_0$ 時的曲率的負數：

$$
\begin{aligned}
& U=\ell^\prime(\theta)|_{\theta=\theta_0}=\ell^\prime(\theta_0)\\
& V=-E[\ell^{\prime\prime}(\theta)]|_{\theta=\theta_0}=-E[\ell^{\prime\prime}(\theta_0)]
\end{aligned}
$$

注：此處的 $V=-E[l^{\prime\prime}(\theta_0)]$ 又常常被叫做 Expected Fisher information。

記得在 Wald 檢驗中使用的近似方程：
$$llr(\theta)\approx-\frac{1}{2}(\frac{M-\theta}{S})^2\text{ asymptotically}$$

令 $q(\theta)=-\frac{1}{2}(\frac{M-\theta}{S})^2$
就有：

$$
\begin{aligned}
& q^\prime(\theta)                      =\frac{M-\theta}{S^2}\\
& \Rightarrow q^\prime(\theta_0)        =\frac{M-\theta_0}{S^2}\\
& q^{\prime\prime}(\theta)              =-\frac{1}{S^2}\\
& \Rightarrow q^{\prime\prime}(\theta_0)=E[l^{\prime\prime}(\theta_0)]\\
& \Rightarrow \frac{1}{S^2}             =-E[l^{\prime\prime}(\theta_0)]\\
& q^\prime(\theta_0)                    = \frac{M-\theta_0}{S^2} = -E[l^{\prime\prime}(\theta_0)](M-\theta_0)\\
&                                       = \ell^\prime(\theta_0)\\
& \Rightarrow     M-\theta_0  = -\frac{\ell^\prime(\theta_0)}{E[l^{\prime\prime}(\theta_0)]}\\
& \Rightarrow     M  =  -\frac{\ell^\prime(\theta_0)}{E[l^{\prime\prime}(\theta_0)]}+\theta_0\\
& q(\theta)=-\frac{1}{2}(\frac{M-\theta}{S})^2=\frac{E[l^{\prime\prime}(\theta_0)]}{2}(-\frac{\ell^\prime(\theta_0)}{E[l^{\prime\prime}(\theta_0)]}+\theta_0-\theta)^2\\
& q(\theta)=-\frac{V}{2}(\frac{U}{V}+\theta_0-\theta)^2\\
& \Rightarrow \text{ Under } H_0: \theta=\theta_0\\
& \Rightarrow q(\theta_0)=-\frac{V}{2}(\frac{U}{V})^2=-\frac{U^2}{2V}\\
& \Rightarrow -2q(\theta_0)=\frac{U^2}{V} \stackrel{\cdot}{\sim}\mathcal{X}_1^2\\
& \text{Or equivalently} \frac{U}{\sqrt{V}} \stackrel{\cdot}{\sim} N(0,1)
\end{aligned}
$$

這就是 Score 檢驗時使用的檢驗統計量。相應的拒絕域就可以被定義爲：
當 $\frac{U^2}{V}>\mathcal{X}_{1,1-\alpha}^2$ 時，拒絕 $H_0$

如下面的示意圖 \@ref(fig:score-test) 所示，Score 檢驗，比較的是 $\theta_0$ 時的校正後似然方程的坡度 (一階導數/二階導數)，和極大似然時的坡度 (一階導數=0) 的差別。如果這個值越大，說明零假設時的似然和極大似然 (觀察數據的信息) 的距離越遠，拒絕零假設的證據就越有力。


```{r score-test, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap='Score test: solid (green) line is log-likelihood ratio, dashed (red) is quadratic approximation', fig.align='center', out.width='90%', cache=TRUE}
knitr::include_graphics("img/Selection_084.png")
```

Score 檢驗優點：

1. 比 LRT 簡單；
2. 不需要計算 MLE，只需要計算零假設時的對數似然比方程之坡度和曲率；
3. 在流行病學用到的檢驗方法中最常用，也最容易擴展 (Mantel-Haenszel test, log rank test, generalised linear models such as logistic, Poisson, Cox regressions)。

Score 檢驗缺點：

1. 和 Wald 檢驗一樣用到了兩次近似；
2. 無法總是保證這是最佳檢驗統計量；
3. 參數如果被數學轉換 (Section \@ref(para-trans))，$p$ 值會跟着變化。



### 再再以二項分佈爲例

$K\sim Bin(n, \pi)$ 假如已知人羣中事件發生的概率是 $\pi_0$。試推導此時的 Score 檢驗的檢驗統計量。

**解**

對二項分佈數據進行 Score 檢驗的時候我們需要計算 $U, V$，然後計算統計量 $\frac{U^2}{V}$ 和 $\mathcal{X}_1^2$ 比較即可。

$$
\begin{aligned}
& \text{Let } p=\frac{k}{n} \\
& \ell(\pi|k) = k\text{log}(\pi)+(n-k)\text{log}(1-\pi)\\
& \ell^\prime(\pi)=\frac{k}{\pi}-\frac{n-k}{1-\pi}=\frac{k-n\pi}{\pi(1-\pi)}\\
& = \frac{p-\pi}{\pi(1-\pi)/n}\\
& \Rightarrow U = \ell^\prime(\pi_0)=\frac{p-\pi_0}{\pi_0(1-\pi_0)/n}\\
& \ell^{\prime\prime}(\pi|K)=-\frac{K}{\pi^2}-\frac{n-K}{(1-\pi)^2}\\
& \Rightarrow -\ell^{\prime\prime}(\pi|K)=\frac{K}{\pi^2}+\frac{n-K}{(1-\pi)^2}\\
& \because E(K)=n\pi\\
& \therefore -E[\ell^{\prime\prime}(\pi|K)]=\frac{n\pi}{\pi^2}+\frac{n-n\pi}{(1-\pi)^2}\\
& =\frac{n}{\pi}+\frac{n}{1-\pi}=\frac{n}{\pi(1-\pi)}\\
& \text{ Under } H_0: \pi=\pi_0 \Rightarrow V=-E[\ell^{\prime\prime}(\pi_0)]=\frac{n}{\pi_0(1-\pi_0)}\\
& \Rightarrow \frac{U^2}{V}=\frac{(p-\pi_0)^2}{\pi_0(1-\pi_0)/n} \stackrel{\cdot}{\sim}\mathcal{X}_1^2\\
& \text{OR } \frac{U}{\sqrt{V}} = \frac{p-\pi_0}{\sqrt{\pi_0(1-\pi_0)/n}} \stackrel{\cdot}{\sim} N(0,1)
\end{aligned}
$$

## LRT, Wald, Score 檢驗三者的比較 {#LRTwaldScore-Compare}

1. LRT 比較的是對數似然方程在零假設 $H_0$ 和極大似然估計 (MLE) 時之間的縱軸差 (圖 \@ref(fig:llr-wald))；Wald 檢驗試圖直接比較 MLE 和 $H_0$ 的橫軸差 (二次方程近似法，並用標準誤校正) (圖 \@ref(fig:llr-wald))；Score 檢驗比較的是對數似然方程在 $H_0$ 時的切線斜率 (二次方程近似法，用曲率也就是二階導數校正) (圖 \@ref(fig:score-test))。三種檢驗比較的東西各不相同，但是這種差距大到進入拒絕域時，數據就會拒絕零假設。其中 Score 檢驗的計算過程最爲簡便，只需要計算 $H_0$ 時對數似然方程的一階和二階導數，而不用去計算 MLE，因此更多的被應用在流行病學數據計算中。

2. 如果對數似然方程本身就是左右對稱的 (正態分佈的情況下)，這三個檢驗方法計算的所有結果都是完全一致的。如果對數似然方程只是近似左右對稱，那麼三者的計算結果會十分接近。可以說，三種檢驗方法是漸進等價的。

3. 如果對觀測值進行了數學轉換，三者中只有 LRT 的計算結果保持不變。如果對參數的數學轉換使得對數似然方程更加接近左右對稱的二次方程，那麼 Wald 和 Score 檢驗的計算結果可以得到改善。

4. 如果說，MLE 和 零假設之間的差距很大，那麼 Wald 或者 Score 檢驗所使用的二次方程近似法的誤差會增加，此時傾向於使用 LRT 來進行精確檢驗。當然如果當樣本量較大，要檢驗的差距也很大，三種檢驗方案都能夠提供證據拒絕零假設 ($p$ 值都會很小)。

5. 如果三種檢驗方案給出的計算結果迥異，即使使用了數學轉換結果也沒有明顯改善的話，那麼最大的問題是樣本量太小。這時候還是老老實實用 LRT 吧。

6. 幾乎所有的參數檢驗都歸類與這章節介紹的三種檢驗方法。比如說 $Z$ 檢驗， $t$ 檢驗， $F$ 檢驗都是 LRT。在流行病學研究中最常用的還是 Score 檢驗。

我們的結論是，當條件允許的情況下，統計檢驗都推薦儘量使用精確檢驗 LRT。

## 練習題

### Q1

在對數似然比章節 (Section \@ref(llr-chi1))，我們曾經證明過，已知方差時：

$$
\begin{aligned}
& llr(\mu|\underline{x})=\ell(\mu|\underline{x})=-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2\\
& \Rightarrow -2llr(\mu|\underline{x})=-2\ell(\mu|\underline{x})=(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2
\end{aligned}
$$

當觀察數據 $X_1,\cdots,X_n\sim N(\mu,1^2)$ ，求 LRT, Wald, Score 三種檢驗方法對下列假設進行檢驗時的檢驗統計量：
$H_0: \mu=\mu_0 \text{ v.s. } H_1: \mu\neq\mu_0$

**解**

$$
\begin{aligned}
& \text{Model: } X_1, \cdots, X_n \stackrel{i.i.d}{\sim} N(\mu, 1)\\
& H_0: \mu=\mu_0 \text{ v.s. } H_1: \mu\neq\mu_0\\
& \text{Model } \Rightarrow \bar{X} \sim N(\mu, \frac{1}{n}) \\
& \text{If we observe } \bar{X} = \bar{x}\\
& \ell(\mu|\bar{x})=-\frac{1}{2}(\frac{\bar{x}-\mu}{1/\sqrt{n}})^2\\
& \textbf{For LRT, under } H_0: \mu=\mu_0 \Rightarrow -2llr(\mu_0) \stackrel{\cdot}{\sim}\mathcal{X}_1^2\\
& \Rightarrow \frac{\bar{x}-\mu}{1/\sqrt{n}} \sim N(0,1)\\
& \textbf{For Wald test, under } H_0: \mu=\mu_0 \Rightarrow \frac{M-\mu_0}{S}\sim N(0,1) \\
& \Rightarrow \frac{\bar{x}-\mu}{1/\sqrt{n}} \sim N(0,1)\\
& \textbf{For Score test, under } H_0: \mu=\mu_0 \Rightarrow U=\ell^\prime(\mu_0), V=-E[\ell^{\prime\prime}(\mu_0)]\\
& U=\ell^\prime(\mu_0)=(\frac{\bar{x}-\mu_0}{1/\sqrt{n}})\sqrt{n}=\frac{\bar{x}-\mu_0}{1/n}\\
& \ell^{\prime\prime}(\mu_0)=-\frac{1}{1/n}=-n \Rightarrow V=-E[n]=n\\
& \frac{U^2}{V}=(\frac{\bar{x}-\mu_0}{1/n})^2/n=(\frac{\bar{x}-\mu_0}{1/\sqrt{n}})^2\\
& \Rightarrow \frac{U^2}{V} \sim \mathcal{X}_1^2 \Rightarrow \frac{U}{\sqrt{V}}=\frac{\bar{x}-\mu_0}{1/\sqrt{n}} \sim N(0,1)
\end{aligned}
$$

**本題證明了，當數據服從正態分佈時，三種檢驗方法使用的檢驗統計量，是完全一致的。**

### Q2

根據醫生的觀察，某種癌症患者的生存時間服從平均值爲 $1/\beta_0$ 的指數分佈 (exponentially distributed)。有一種新藥物可以改善平均生存時間 (仍然服從指數分佈)。已知指數分佈的密度方程是：$f(x|\beta)=\beta \text{exp} (-\beta x), \text{ where } \beta, x>0$。

1. 證明指數分佈的均值是 $1/\beta$

**解**

$$
\begin{aligned}
& X\sim f(x|\beta), x>0 \Rightarrow E(X)=\int_0^\infty x\cdot f(x)\text{d} x = \int_0^\infty x\cdot \beta \cdot e^{-\beta x} \text{d}x\\
& E(x)= - \int_0^\infty x\cdot \frac{\text{d}e^{-\beta x}}{\text{d}x} \cdot \text{d}x\\
& \text{We can now integrate by parts, using } \int_a^b u \frac{\text{d}v}{\text{d}x} \text{d}x = [uv]_a^b-\int_a^b v \frac{\text{d}u}{\text{d}x} \text{d}x \\
& E(X) = -[x\cdot e^{-\beta x}]_0^\infty + \int_0^\infty e^{-\beta x} \text{d} x \\
& \;\;\;\; = -0+\int_0^\infty e^{-\beta x} \text{d} x\\
& \;\;\;\; = \int_0^\infty\frac{\text{d}}{\text{d}x} \frac{e^{-\beta x}}{-\beta} \text{d} x\\
& \;\;\;\; = [\frac{e^{-\beta x}}{-\beta}]_0^\infty = \frac{1}{-\beta}[0-1]=\frac{1}{\beta}
\end{aligned}
$$

2. 請寫下本題設定條件下的數學模型，零假設和替代假設

**解：** 假設患者人數爲 $n$，他們的生存時間爲相互獨立的隨機變量： $X_1,\cdots,X_n$。那麼本例中的數學模型爲：$\text{Model: } X_1,\cdots,X_n\stackrel{i.i.d}{\sim}f(x|\beta)=\beta e^{-\beta x}$。我們可以提出如下的零假設和替代假設：$H_0: \beta=\beta_0 \text{ v.s. } H_1: \beta\neq\beta_0$。

3. 推導此模型參數 $\beta$ 的極大似然估計 (MLE)，試使用似然比檢驗法來推導進行假設檢驗時使用的檢驗統計量。

**解**

$$
\begin{aligned}
& L(\beta|\underline{x}) = \prod_{i=1}^n f(x_i|\beta)=\prod_{i=1}^n\beta e^{-\beta x_i} \\
& \ell(\beta)=\sum_{i=1}^n\text{log}(\beta e^{-\beta x_i})=\sum\text{log}\beta-\sum\beta x_i=n\text{log}\beta-\beta\sum x_i \\
& \;\;\;\; = n\text{log}\beta-\beta n \bar{x} \\
& \Rightarrow \ell^\prime(\beta)=\frac{n}{\beta}-n\bar{x}\text{ MLE solves } \ell^\prime(\beta)=0 \text{ when }\ell^{\prime\prime}(\beta) < 0 \\
& \ell^\prime(\beta)=0 \Rightarrow \hat\beta=\frac{1}{\bar{x}}, \text{ and } \ell^{\prime\prime}(\beta)=-n\frac{1}{\beta^2} < 0\\
& \Rightarrow \text{ LRT test statistic: Under } H_0: \beta=\beta_0 \Rightarrow -2llr(\beta_0) \sim \mathcal{X}_1^2\\
& llr(\beta_0)=\ell(\beta_0)-\ell(\hat\beta)=n\text{log}\beta_0-\beta_0n\bar{x}-n\text{log}\hat\beta+\hat\beta n \bar{x}\\
& \text{ Substituting with MLE } \hat\beta=\frac{1}{\bar{x}}\\
& \;\;\;\;\;\;\;\;\;\; = n\text{log}\beta_0-\beta_0n\bar{x}+n\text{log}\bar{x}+ n\\
& \;\;\;\;\;\;\;\;\;\; = n(\text{log}\beta_0\bar{x}-\beta_0\bar{x}+1) \textbf{ this is the statistic for LRT}
\end{aligned}
$$

4. 推導  Score 和 Wald 檢驗法時的檢驗統計量

**解**

$$
\begin{aligned}
& \textbf{Score test: under } H_0 \Rightarrow \frac{U^2}{V}\sim \mathcal{X}_1^2 \text{ where } U=\ell^\prime(\beta_0), V=-E[\ell^{\prime\prime}(\beta_0)]\\
& \Rightarrow U=\frac{n}{\beta_0}-n\bar{x}; V = -E[-n\frac{1}{\beta_0^2}] = n\frac{1}{\beta_0^2} \\
& \Rightarrow \frac{U^2}{V}=(\frac{n}{\beta_0}-n\bar{x})^2\cdot\frac{\beta_0^2}{n} = (\frac{(\frac{n}{\beta_0}-n\bar{x})\beta_0}{\sqrt{n}})^2\\
& \;\;\;\;\;\;\;\;\; = n(1-\bar{x}\beta_0)^2\\
& \textbf{This is the statistic for Score test}\\
& \textbf{Wald test: under } H_0: \beta=\beta_0 \Rightarrow W=(\frac{M-\beta_0}{S})^2 \sim \mathcal{X}_1^2, \\
& \text{ where } M=\hat\beta=\frac{1}{\bar{x}}, \text{ and } S^2=-\frac{1}{\ell^{\prime\prime}(\hat\beta)}\\
& \ell^{\prime\prime}(\beta)=-n\frac{1}{\beta^2}\Rightarrow \ell^{\prime\prime}(\hat\beta)=-n\bar{x}^2\Rightarrow S^2=\frac{1}{n\bar{x}^2}\\
& \Rightarrow W=(\frac{M-\beta_0}{S})^2=\frac{(\frac{1}{\bar{x}}-\beta_0)^2}{\frac{1}{n\bar{x}^2}}=n(1-\beta_0\bar{x})^2\\
& \textbf{This is the statistic for Wald test}
\end{aligned}
$$

注意到在這個特例中， Score 和 Wald 檢驗的統計量竟然不謀而合。

5. 觀察5名患者，獲得診斷後的生存數據 (年)： $0.5,1,1.25,1.5,0.75$。用上面推導的統計量對這個數據進行假設檢驗：$H_0: \beta=0.5 \text{ v.s. } \beta\neq0.5$，你如何下結論？

**解**

$$
\begin{aligned}
&\text{Data: } x_1,\cdots,x_n=0.5,1,1.25,1.5,0.75. \Rightarrow \bar{x}=1\\
&H_0: \beta=0.5 \text{ v.s. } \beta\neq0.5\\
&\textbf{LRT test: } \\
& llr(\beta_0) = n(\text{log}\beta_0\bar{x}-\beta_0\bar{x}+1) = 5\times(\text{log}0.5-0.5\times1+1) = -0.966\\
&\Rightarrow -2llr=1.93 < \text{Prob}(\mathcal{X}^2_{1,0.95}) = 3.84 \\
& \text{There is no evidence that } \beta\neq0.5.\\
&\textbf{Score test: } \\
& \frac{U^2}{V} = n(1-\bar{x}\beta_0)^2 = 5\times(1-1\times0.5)^2=1.25 < \text{Prob}(\mathcal{X}^2_{1,0.95}) = 3.84 \\
& \text{There is no evidence that } \beta\neq0.5.\\
&\textbf{Wald test: } \\
& W=n(1-\beta_0\bar{x})^2=5\times(1-0.5\times1)^2=1.25< \text{Prob}(\mathcal{X}^2_{1,0.95}) = 3.84 \\
& \text{There is no evidence that } \beta\neq0.5.\\
\end{aligned}
$$

### Q3

隨機變量 $X_1,\cdots,X_n$ 互相獨立且在區間 $[0,\alpha]$ 內服從相同的恆定概率分佈 (identical uniform distribution)。
試着畫出參數 $\alpha$ 的似然方程示意圖。不進行任何數學計算，試着想象一下如果對 $\alpha$ 進行某種假設檢驗會出現什麼問題嗎？

# 正態誤差模型  Normal error models {#normal-error-models}

正態誤差模型，其實沒有其名字那麼複雜，就是討論在正態分布條件下，**均值和方差都需要被估計** (都是未知狀態) 的模型。

本章還介紹

1. $F$ 分佈和 $t$ 分佈，試着闡述如何將 $t$ 分佈應用於兩個獨立樣本均值的比較；
2. $\chi^2$ 分佈在統計學中各種常用分佈中的中心位置。

## 服從正態分佈的隨機變量


$$
X_1,\cdots,X_n \stackrel{i.i.d}{\sim} N(\mu,\sigma^2) \Leftrightarrow \bar{X} \sim N(\mu, \frac{\sigma^2}{n})
$$

如果總體方差 $\sigma^2$ 已知 (理想狀態，現實中不太可能)：

$$
\begin{aligned}
& Z=\frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \sim N(0,1) \\
& 95\% \text{CI for } \mu = \bar{X} \pm Z_{0.975}\frac{\sigma}{\sqrt{n}} \\
& \text{H}_0: \mu=\mu_0 \Rightarrow \frac{\bar{X}-\mu_0}{\sigma/\sqrt{n}} \sim N(0,1)
\end{aligned}
$$


如果總體方差 $\sigma^2$ 是未知的，腫麼辦？ (模型中出現了兩個參數 $\mu \;\&\; \sigma^2$)

$$
T=\frac{\bar{X}-\mu_0}{\hat\sigma/\sqrt{n}} \sim ?????????
$$

## $F$ 分佈和 $t$ 分佈的概念 {#Fandtdistr}

如果 $X\sim N(0,1)$，那麼 $X^2 \sim \chi^2_1$ (Section \@ref(chi-square-distribution))。類似地，如果 $X_1,\cdots,X_n \stackrel{i.i.d}{\sim} N(0,1)$ 那麼 $\sum_{i=1}^n X^2_i \sim \chi^2_k$。

$F$ 分佈和 $t$ 分佈是建立在 $\chi^2$ 分佈的基礎上的：

- $F$ 分佈： $Y_1, Y_2$ 是獨立的兩個隨機變量，且 $Y_1 \sim \chi^2_{k_1}; Y_2 \sim \chi^2_{k_2}$，那麼

$$
F=\frac{Y_1/k_1}{Y_2/k_2} \sim F_{k_1, k_2}
$$

- $t$ 分佈，是 $F$ 分佈的特殊情況 $(k_1=1)$：

$$
T\sim t_{k_2} \Rightarrow T^2 = \frac{Y_1/1}{Y_2/k_2} \sim F_{1,k_2}
$$

此時我們再來考慮正態分佈模型中有兩個參數 $\mu, \sigma^2$ 需要被估計的模型：

$$
Y_i \stackrel{i.i.d}{\sim} N(\mu,\sigma^2) \text{ where } i = 1, \cdots, n
$$

其實可以改寫爲

$$
\begin{aligned}
& Y_i = \mu + \varepsilon_i \\
& \text{Where } \varepsilon_i \stackrel{i.i.d}{\sim} N(0,\sigma^2)
\end{aligned}
$$

其中 $\varepsilon_i \stackrel{i.i.d}{\sim} N(0,\sigma^2)$ 就是正態誤差 normal (random) error。$Y_i = \mu + \varepsilon_i$ 就是正態誤差模型 normal error model。誤差的含義就是統計模型中的隨機誤差 (模型不能解釋的部分)。如果一個正態誤差模型像前面的式子這樣沒有其他變量，那麼所有的觀察值 $Y_i$，就是由總體均值 $\mu$ population mean，和隨機誤差 $\varepsilon$ random error 來說明 (就是這個式子 $Y_i = \mu + \varepsilon_i$)。

如果觀察值 $Y_i$ 的一部分除了可以用均值解釋，還可以由某個變量 $x$ 來說明 (叫做解釋變量 explanatory variable 詳見線性迴歸部分 Section \@ref(defLM))，即：

$$
\begin{aligned}
&Y_i | x \stackrel{i.i.d}{\sim} N(\mu+\beta x_i, \sigma^2)\\
& E(Y|x) = \mu+\beta x, \text{Var}(Y|x) = \sigma^2 \\
& \text{ or } Y_i|x = \mu + \beta x_i + \varepsilon_i ; \text{ where }  \varepsilon_i \stackrel{i.i.d}{\sim} N(0, \sigma^2)
\end{aligned}
$$

上面的模型會在後面講線性迴歸的部分深入探討，此處簡單用下面的圖形來輔助理解。圖 \@ref(fig:normal-error0) 中繪製的是 $Y_i|x = \mu + \beta x_i + \varepsilon_i ; \text{ where }  \varepsilon_i \stackrel{i.i.d}{\sim} N(0, \sigma^2)$ 的示意圖，用 $x_i$ 標記兩個組，其中 $x_i = 0$ 時爲組 A 的人的觀察值，$x_i=1$ 時爲組 B 的人的觀察值。兩組的平均值如 Y 軸顯示的那樣，組 A 是 $\mu$，組 B 是 $\mu+\beta$。所以，這裏可以看到，正態誤差模型是假定兩組具有相同的方差的 common variance，如圖 \@ref(fig:normal-error1)。如果解釋變量 (explanatory variable) 是一個連續型變量，則解釋爲在 X 軸上的任意一點對應的 Y 值的誤差都服從相同的方差，如圖 \@ref(fig:normal-error2)


```{r normal-error0, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap='Normal error models with categorical explanatory variable', fig.align='center', out.width='90%', cache=TRUE}
knitr::include_graphics("img/Selection_105.png")
```

```{r normal-error1, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap='Normal error models shown with common error variance', fig.align='center', out.width='90%', cache=TRUE}
knitr::include_graphics("img/Selection_106.png")
```

```{r normal-error2, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap='Normal error models shown with continuous variable and common error variance', fig.align='center', out.width='80%', cache=TRUE}
knitr::include_graphics("img/Selection_107.png")
```

## 兩個參數的模型

### 一組數據兩個參數

如果觀察數據 $\underline{x} = x_1, \cdots, x_n$ 是互相獨立的，該觀察數據的模型可以用一個包含兩個參數 $\theta,\phi$ 的概率方程 $f$ 來描述，那麼這個包含兩個參數的概率方程的似然和對數似然分別是：

$$
\begin{aligned}
L(\theta, \phi | \underline{x}) &=  \prod_{i=1}^nf(x_i | \theta, \phi) \\
\ell(\theta, \phi | \underline{x}) &= \sum_{i=1}^n\text{log}f(x_i | \theta, \phi)
\end{aligned}
$$

兩個參數的 $\text{MLE}$ 可以通過對對數似然方程進行兩次偏微分，然後解連立方程組：

$$
\left\{
\begin{array}{ll}
\frac{\partial\ell}{\partial\theta} = 0\\
\frac{\partial\ell}{\partial\phi} = 0 \\
\end{array}
\right.
$$

### 兩組數據各一個參數

如果是兩組獨立數據，各由一個參數描述他們各自的概率方程：


$$
X_1, \cdots, X_n \stackrel{i.i.d}\sim f(\theta_1) \\
Y_1, \cdots, Y_m \stackrel{i.i.d}\sim f(\theta_2)
$$

那麼以兩組數據爲聯合條件 (應該可以理解爲同時觀察到時的) 的聯合似然 (joint likelihood)：

We describe the likelihood as the joint likelihood, conditional on jointly observing both datasets:

$$
L(\theta_1, \theta_2|\underline{x},\underline{y}) = \prod_{i=1}^nf_1(x_{1i}|\theta_1) \times \prod_{i=1}^mf_2(y_{i}|\theta_2)
$$

所以，聯合之後的對數似然方程就是兩個對數似然方程之和：

$$
\ell(\theta_1,\theta_2|\underline{x},\underline{y}) = \sum_{i=1}^n\text{log} f(x_i|\theta_1) + \sum_{i=1}^m\text{log} f(y_i|\theta_2)
$$

你會發現，分成兩組數據兩個獨立的概率方程之後的聯合對數似然方程求 $\text{MLE}$ 時需要用偏微分。可是偏微分之後的結果，和兩組數據合二爲一，用含有兩個參數的概率方程，計算其 $\text{MLE}$ 的結果會**完全相同**。

## 正態分佈概率密度方程中總體均值和方差都未知 (單樣本 $t$ 檢驗 one sample $t$ test 的統計學推導)

此時的情況如同前面的把兩組數據合二爲一的情況，用正態分佈的概率方程，然後有兩個參數 $\mu, \sigma^2$。

$$
Y_1,\cdots,Y_n \stackrel{i.i.d}{\sim} N(\mu, \sigma^2) \\
\ell(\mu, \sigma^2 | \underline{y}) = -\frac{n}{2}\text{log}\sigma^2 - \frac{1}{2\sigma^2}\sum^n_{i=1} (x_i - \mu)^2
$$

$$
\begin{aligned}
& \mu: \frac{\partial \ell}{\partial \mu} = \frac{\sum^n_{i=1}(y_i-\mu)}{2\sigma^2} = 0 \Rightarrow \hat\mu = \bar{y}\\
& \sigma^2: \frac{\partial \ell}{\partial (\sigma^2)} = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}\sum^n_{i=1}(y_i-\mu)^2 \\
& \text{ Substituting } \mu=\hat\mu = \bar{y} \text{ and set equal to } 0\\
& \frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}\sum^n_{i=1}(y_i-\bar{y})^2 = 0 \\
& \Rightarrow \hat\sigma^2 = \frac{1}{n}\sum^n_{i=1}(y_i - \bar{y})^2
\end{aligned}
$$

有沒有覺得這裏的方差的極大似然估計似曾相識 (Section \@ref(samplevarbias))。在早期的章節中，我們學到了分部法 (“把樣本和總體均值之間的差的平方和分成兩部分”)：

$$
\begin{aligned}
\sum^n_{i=1}(y_i-\mu)^2 & = \sum^n_{i=1}(y_i - \bar{y} + \bar{y} -\mu)^2  \\
                        & = \sum^n_{i=1}(y_i - \bar{y})^2 + \sum^n_{i=1}(\bar{y}-\mu)^2 \\
\Rightarrow \sum^n_{i=1}(y_i - \bar{y})^2 & = \sum^n_{i=1}(y_i-\mu)^2 - \sum^n_{i=1}(\bar{y}-\mu)^2
\end{aligned}
$$

當時分的是平方和，這裏再介紹一種把概率分部的方法 **partition the probabilities**。

We can "partition" the probability of observing the data, conditional on unknown $\mu$ and $\sigma^2$, into

1. the probability of observing the data conditional on the observed sample mean $\bar{y}$ and unknown $\sigma^2$ ;
2. the probability of observing the sample mean $\bar{y}$ conditional on the two unknown parameters.

$$
\begin{aligned}
& \text{Prob}(\underline{y} | \mu, \sigma^2) = \text{Prob}(\underline{y}|\bar{y}, \sigma^2) \times \text{Prob}(\bar{y}|\mu, \sigma^2) \\
&\Rightarrow \text{Prob}(\underline{y} | \bar{y}, \sigma^2) = \frac{\text{Prob}(\underline{y} | \mu, \sigma^2)}{\text{Prob}(\bar{y}|\mu, \sigma^2)}
\end{aligned}
$$

看到這裏你是否會想起概率論中討論的條件概率方程 (Section \@ref(conditonalProb))：

$$
f(x|Y=y) = \frac{f(x,y)}{f(y)}
$$

利用上述概率分佈的方法，我們可以進而推導方差 $\sigma^2$ 的 $\text{MLE}$：

$$
\begin{aligned}
f(\underline{y} | \bar{y}, \sigma^2) &= \frac{
\color{red}{f(\underline{y} | \mu, \sigma^2)}
}{f(\bar{y}|\mu, \sigma^2)} \\
  &=  \frac{
  \color{red}{(\frac{1}{\sqrt{2\pi\sigma^2}})^ne^{-\frac{1}{2\sigma^2}\sum^n_{i=1}(y_i - \mu)^2}}
  }{(\frac{1}{\sqrt{2\pi\sigma^2/n}})e^{-\frac{1}{2\sigma^2/n}(\bar{y}-\mu)^2}} \\
\Rightarrow \ell(\sigma^2| \underline{y}, \bar{y}) &=
\color{red}{-\frac{n}{2}\text{log}\sigma^2 - \frac{1}{2\sigma^2}\sum^n_{i=1}(y_i-\mu)^2} \\ & \;\;\;+\frac{1}{2}\text{log}\frac{\sigma^2}{n} + \frac{1}{2\sigma^2/n}(\bar{y}-\mu)^2 \\
&= -\frac{n-1}{2}\text{log}\sigma^2 - \frac{1}{2\sigma^2}(\sum^n_{i=1}(y_i-\mu)^2 - n(\bar{y}-\mu)^2) \\
 \text{Because }  &\sum^n_{i=1}(y_i - \bar{y})^2  = \sum^n_{i=1}(y_i-\mu)^2 - \sum^n_{i=1}(\bar{y}-\mu)^2 \\
 \Rightarrow \ell(\sigma^2| \underline{y}, \bar{y}) &= -\frac{n-1}{2}\text{log}\sigma^2 -\frac{1}{2\sigma^2}\sum^n_{i=1}(y_i - \bar{y})^2 \\
 \text{Note that the } &\text{above conditional log-likelihood is now free of } \mu \\
 \Rightarrow \ell^\prime(\sigma^2) &= -\frac{n-1}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}\sum^n_{i=1}(y_i-\bar{y})^2 \\
 \text{Set equal } & \text{to zero and rearrange} \\
 \Rightarrow \hat\sigma^2 &= \frac{1}{n-1}\sum^n_{i=1}(y_i-\bar{y})^2\\
 \text{This is the } &\color{red}{\text{unbiased estimate of } \sigma^2}
\end{aligned}
$$


現在再重新考慮對數據 $Y_1, \cdots, Y_n \stackrel{i.i.d}{\sim} N(\mu, \sigma^2)$ 進行均值的假設檢驗：

$$
\text{H}_0: \mu = \mu_0 \text{ v.s H}_1: \mu > \mu_0
$$

當 $\sigma^2$ 是**已知的**，在零假設條件下的檢驗統計量是：

$$
\begin{aligned}
& \text{H}_0 \Rightarrow (\frac{\bar{Y}-\mu_0}{\sigma/\sqrt{n}}) \sim N(0,1) \\
& \text{Or equivalently, } \\
& (\frac{\bar{Y}-\mu_0}{\sigma/\sqrt{n}})^2 \sim \chi_1^2
\end{aligned}
(\#eq:infer8-1)
$$

當 $\sigma^2$ 是**未知的**，它需要通過樣本數據來估計時。我們就該使用前面從條件對數似然方程推導出的方差無偏估計：

$$
\hat\sigma^2 = S^2 = \frac{1}{n-1}\sum^n_{i=1}(y_i-\bar{y})^2
$$

但是，假如只把無偏估計的方差放到公式 \@ref(eq:infer8-1) 裏去，可以當作新的檢驗統計量嗎？有這麼簡單嗎？

$$
(\frac{\bar{Y}-\mu_0}{s/\sqrt{n}})^2
$$

當然沒有這麼簡單！這種方式僅僅考慮了樣本的方差估計，卻忽略了這個估計是有不確定性的 (uncertainty)，它並不是真實的 $\sigma^2$，只是個估計 (estimator)。我們需要找到一種方法把方差的不確定性也考慮進新的檢驗統計量裏去。利用章節 \@ref(samplevar) 的結論：

$$
\begin{equation}
\frac{n-1}{\sigma^2}S^2 \sim \chi^2_{n-1}\\
\Rightarrow \frac{S^2}{\sigma^2} = \frac{\chi^2_{n-1}}{n-1}
\end{equation}
(\#eq:infer8-2)
$$

把公式 \@ref(eq:infer8-1) 除以 \@ref(eq:infer8-2) 獲得：

$$
\frac{(\bar{Y}-\mu_0)^2}{S^2/n} \sim \frac{\chi^2_1/1}{\chi^2_{n-1}/n-1} = F_{1,n-1}
$$

這樣我們就同時考慮了方差估計本身，和它的不確定性了。這個新的統計量被定義爲 $T$：

$$
\begin{aligned}
& T=\frac{\bar{Y}-\mu_0}{S/\sqrt{n}} \\
& \text{Then under H}_0: T^2 \sim F_{1,n-1} \text{ or equivalently } T \sim \sqrt{F_{1,n-1}}=t_{n-1}
\end{aligned}
$$

這個特殊的 $F$ 分佈，就是我們之前定義過的，這裏用手紮紮實實地推導出來的檢驗統計量 $t$ 和 $t$ 分佈。利用這個方差未知時的分佈，均值的 $95\%$ 信賴區間的估計就是：

$$
95\% \text{ CI for } \mu: \bar{Y} \pm t_{n-1,0.975}\frac{S}{\sqrt{n}}
$$



## 比較兩組獨立數據的均值  two sample $t$ test with equal unknown $\sigma^2$

本節要來推導**方差齊時**的兩個獨立樣本的均值比較 two sample $t$ test。兩個獨立樣本用下面的數學符號標記：

$$
X_1, \cdots, X_n \stackrel{i.i.d}{\sim} N(\mu_1, \sigma^2); Y_1, \cdots, Y_m, \stackrel{i.i.d}{\sim} N(\mu_2, \sigma^2)
$$

要進行的假設檢驗是：

$$
\text{H}_0: \mu_1 = \mu_2 \text{ v.s. } \text{H}_1: \mu_1 > \mu_2
$$

此時，兩組獨立樣本的共同方差 $\hat\sigma^2$ 需要被估計，利用上面相同的推導過程，可以獲得合併後的共同方差的無偏估計：

$$
\begin{equation}
\hat\sigma^2 = S^2_p = \frac{\sum^n_{i=1}(X_i-\bar{X})^2 + \sum^m_{i=1}(Y_i-\bar{Y})^2}{n+m-2}\\
\end{equation}
(\#eq:infer8-3)
$$

因爲兩組數據互相獨立，所以有：

$$
\begin{aligned}
& \frac{1}{\sigma^2}\sum^n_{i=1}(X_i - \bar{X})^2 \sim \chi^2_{n-1} \\
& \frac{1}{\sigma^2}\sum^m_{i=1}(Y_i - \bar{Y})^2 \sim \chi^2_{m-1} \\
\Rightarrow &\frac{1}{\sigma^2}\{ \sum^n_{i=1}(X_i - \bar{X})^2 + \sum^m_{i=1}(Y_i - \bar{Y})^2 \} \sim \chi^2_{n+m-2}
\end{aligned}
$$

把公式 \@ref(eq:infer8-3) 代入此式可得：


$$
\begin{equation}
(n+m-2)\frac{S^2_p}{\sigma^2} \sim \chi^2_{n+m-2}
\end{equation}
(\#eq:infer8-4)
$$

由於 $\bar{X} \sim N(\mu_1, \frac{\sigma^2}{n}); \bar{Y} \sim N(\mu_2, \frac{\sigma^2}{m})$，所以在零假設條件下 $\text{H}_0: \mu_1=\mu_2\Rightarrow \bar{X}-\bar{Y} \sim N(0,\sigma^2(\frac{1}{n}+\frac{1}{m}))$。

$$
\begin{equation}
\Rightarrow \frac{\bar{X}-\bar{Y}}{\sqrt{\sigma^2(\frac{1}{n}+\frac{1}{m})}} \sim N(0,1) \\
\Leftrightarrow \frac{(\bar{X}-\bar{Y})^2}{\sigma^2(\frac{1}{n}+\frac{1}{m})} \sim \chi^2_1
\end{equation}
(\#eq:infer8-5)
$$

現在把公式 \@ref(eq:infer8-5) 除以 \@ref(eq:infer8-4) 可得：

$$
\begin{aligned}
&\frac{(\bar{X}-\bar{Y})^2}{\sigma^2(\frac{1}{n}+\frac{1}{m})} \times \frac{\sigma^2}{S^2_p(n+m-2)} = \frac{\chi^2_1/1}{\chi^2_{n+m-2}} \\
&\Rightarrow T^2 = \frac{(\bar{X}-\bar{Y})^2}{S^2_p(\frac{1}{n}+\frac{1}{m})} = \frac{\chi^2_1/1}{\chi^2_{n+m-2}/(n+m-2)} \sim F_{1,n+m-2} \\
&\Rightarrow T = \frac{\bar{X}-\bar{Y}}{S_p\sqrt{\frac{1}{n}+\frac{1}{m}}} \sim t_{n+m-2}
\end{aligned}
$$

這就是標準的兩個齊方差的獨立樣本均值比較的 $t$ 檢驗，two-sample $t$ test with pooled variance。這裏推導的兩個 $t$ 檢驗，是都是精確的**似然比檢驗 (likelihood ratio test)** (Section \@ref(LRT))。壯士請自己跟着似然比檢驗的方法推導一次。

## 各個統計分佈之間的關係

卡方分佈 $\chi^2$ 是統計學常用分佈中極爲重要的分佈，其他的許多分佈都與之相關。

$$
\{N(0,1)\}^2 = \chi^2_1 \\
\chi^2_k = \sum_{i-1}^k \chi^2_1 \\
F_{k,n} = \frac{\chi^2_k/k}{\chi^2_n/n}\\
t^2_n = F_{1,n} =\frac{\chi^2_1/1}{\chi^2_n/n}
$$


# 多個參數時的統計推斷 Inference with multiple parameters I {#inference-with-multiple-para}

前一章介紹單樣本和雙樣本 $t$ 檢驗時已經接觸到了 2 個未知參數情況下的檢驗統計量推導，本章把之前用到的方法擴展到 2 個以上參數的情況。帶你推導兩個以上參數的似然比檢驗 likelihood ratio test，Wald 檢驗，和 Score 檢驗推論。

## 多參數 multiple parameters - LRT

### 似然 likelihood

如果一個觀察數據 $\underline{x} = (x_1, \cdots, x_n)$ 相互獨立，可以用含有 $k$ 個參數 $\theta_1,\cdots,\theta_k$ 的數學模型 $f$ 來描述，那麼它的似然公式爲：

$$
L(\theta_1,\cdots,\theta_k | \underline{x}) = f(\underline{x} | \theta_1,\cdots,\theta_k) = \prod^n_{i=1}f(x_i|\theta_1,\cdots,\theta_k)
$$

它的對數似然公式爲：

$$
\ell(\theta_1,\cdots,\theta_k|\underline{x}) = \sum^n_{i=1}\text{log}f(x_1|\theta_1,\cdots,\theta_k)
$$

每個參數的 $\text{MLE}$ 通過解下面的 $k$ 個連立方程組獲得：

$$
\left\{
\begin{array}{c}
\frac{\partial \ell}{\partial \theta_1} = \ell^\prime(\theta_1) = 0 \\
\frac{\partial \ell}{\partial \theta_2} = \ell^\prime(\theta_k) = 0 \\
\vdots \\
\frac{\partial \ell}{\partial \theta_k} = \ell^\prime(\theta_k) = 0 \\
\end{array}
\right.
$$

- 這些連立方程有時被叫做 **score equations**；
- $\text{MLE}$ 的恆定性，不變性 invariance 在多個參數時同樣適用。

- 當參數只有一個 $\theta$ 時，其 $\text{MLE}$ 的方差是 $S^2=\left.-\frac{1}{\ell^{\prime\prime}(\theta)}\right\vert_{\theta=\hat{\theta}}$
- 當參數有多個時，$k$ 個 $\text{MLE}$ 的方差是一個 $k\times k$ 的對稱矩陣，其中二次微分矩陣 \@ref(eq:hessian-matrix) 的昵稱是**海森矩陣 Hessian matrix**：

$$
\begin{equation}
\underline{\ell^{\prime\prime}(\theta)} = \left(
\begin{array}{c}
\frac{\partial^2\ell}{\partial\theta^2_1} & \frac{\partial^2\ell}{\partial\theta_2\partial\theta_1} & \cdots & \frac{\partial^2\ell}{\partial\theta_k\partial\theta_1}  \\
\frac{\partial^2\ell}{\partial\theta_1\partial\theta_2} & \frac{\partial^2\ell}{\partial\theta^2_2} & \cdots & \frac{\partial^2\ell}{\partial\theta_k\partial\theta_2}  \\
\vdots & \vdots & \ddots & \vdots  \\
\frac{\partial^2\ell}{\partial\theta_1\partial\theta_k} & \frac{\partial^2\ell}{\partial\theta_2\partial\theta_k} & \cdots & \frac{\partial^2\ell}{\partial\theta^2_k}  \\
\end{array}
\right)
\end{equation}
(\#eq:hessian-matrix)
$$


$$
\Rightarrow \underline{\ell^{\prime\prime}(\theta)} |_{\color{red}{\theta=\hat\theta}} =  \left(
\begin{array}{c}
\frac{\partial^2\ell}{\partial\theta^2_1} & \frac{\partial^2\ell}{\partial\theta_2\partial\theta_1} & \cdots & \frac{\partial^2\ell}{\partial\theta_k\partial\theta_1}  \\
\frac{\partial^2\ell}{\partial\theta_1\partial\theta_2} & \frac{\partial^2\ell}{\partial\theta^2_2} & \cdots & \frac{\partial^2\ell}{\partial\theta_k\partial\theta_2}  \\
\vdots & \vdots & \ddots & \vdots  \\
\frac{\partial^2\ell}{\partial\theta_1\partial\theta_k} & \frac{\partial^2\ell}{\partial\theta_2\partial\theta_k} & \cdots & \frac{\partial^2\ell}{\partial\theta^2_k} \\
\end{array}
\right)_{\color{red}{\theta=\hat\theta}}
$$

$$
\Rightarrow \underline{\text{Var}(\hat\theta)} = - \left(
\begin{array}{c}
\frac{\partial^2\ell}{\partial\theta^2_1} & \frac{\partial^2\ell}{\partial\theta_2\partial\theta_1} & \cdots & \frac{\partial^2\ell}{\partial\theta_k\partial\theta_1}  \\
\frac{\partial^2\ell}{\partial\theta_1\partial\theta_2} & \frac{\partial^2\ell}{\partial\theta^2_2} & \cdots & \frac{\partial^2\ell}{\partial\theta_k\partial\theta_2}  \\
\vdots & \vdots & \ddots & \vdots  \\
\frac{\partial^2\ell}{\partial\theta_1\partial\theta_k} & \frac{\partial^2\ell}{\partial\theta_2\partial\theta_k} & \cdots & \frac{\partial^2\ell}{\partial\theta^2_k}  \\
\end{array}
\right)^{\color{red}{-1}}_{\color{red}{\theta=\hat\theta}}
$$

### 對數似然比檢驗

多個參數未知時的對數似然比檢驗可以被這樣拓展：

$$
\begin{aligned}
& \text{H}_0: \underline{\theta} = \underline{\theta_0} \\
& \Rightarrow -2llr(\underline{\theta_0}) = -2(\ell(\underline{\theta_0})- \ell(\hat{\underline{\theta}})) \stackrel{\cdot}{\sim} \chi^2_r \\
& \text{Where } r \text{ is the number of parameters restricted under H}_0
\end{aligned}
$$

## 多參數 Wald 檢驗 - Wald test

單個參數時的 Wald 檢驗的檢驗統計量：

$$
\begin{aligned}
& \text{H}_0: \theta=\theta_0 \Rightarrow W_\theta = (\frac{M-\theta_0}{S})^2 \stackrel{\cdot}{\sim} \chi^2_1 \\
& \text{Where } M=\hat\theta, S^2=\left.-\frac{1}{\ell^{\prime\prime}(\theta)}\right\vert_{\theta=\hat{\theta}} \\
& \Rightarrow W=(\hat\theta-\theta_0)^2(-\ell^{\prime\prime}(\hat\theta)) \stackrel{\cdot}{\sim} \chi^2_1
\end{aligned}
$$

如果是兩個參數 $\lambda, \psi$ 的 Wald 檢驗： $\text{H}_0: \lambda=\lambda_0, \psi=\psi_0 \text{ v.s. H}_1: \lambda \neq \lambda_0 \text{ or } \psi \neq \psi_0$。

- 我們可以先一個一個考慮參數：

$$
\begin{aligned}
& W_\lambda  = (\hat\lambda-\lambda_0)^2(-\ell^{\prime\prime}(\hat\lambda)) \stackrel{\cdot}{\sim} \chi^2_1 \\
& W_\psi     = (\hat\psi-\psi_0)^2(-\ell^{\prime\prime}(\hat\psi)) \stackrel{\cdot}{\sim} \chi^2_1 \\
& \Rightarrow W_\lambda + W_\psi \stackrel{\cdot}{\sim} \chi^2_2 \\
& \Rightarrow W = (\hat\lambda-\lambda_0)^2(-\ell^{\prime\prime}(\hat\lambda)) + (\hat\psi-\psi_0)^2(-\ell^{\prime\prime}(\hat\psi)) \stackrel{\cdot}{\sim} \chi^2_2
\end{aligned}
$$

- 也可以一開始就兩個參數一起考慮：

$$
\underline{\ell^\prime} = \left(
\begin{array}{c}
\frac{\partial\ell}{\partial\lambda}\\
\frac{\partial\ell}{\partial\psi}
\end{array}
\right)
\Rightarrow \underline{\ell^{\prime\prime}} = \left(
\begin{array}{c}
\frac{\partial^2\ell}{\partial\lambda^2} & \frac{\partial^2\ell}{\partial\lambda\partial\psi} \\
\frac{\partial^2\ell}{\partial\psi\partial\lambda} & \frac{\partial^2\ell}{\partial\psi^2}
\end{array}
\right)
$$

然後單參數時 $W$ 的分子 $(\theta_0-\hat\theta)^2$ 此時變爲：

$$
(\hat\lambda-\lambda_0)^2+(\hat\psi-\psi_0)^2 = (\hat\lambda-\lambda_0, \hat\psi-\psi_0)\left(
\begin{array}{c}
\hat\lambda-\lambda_0 \\
\hat\psi-\psi_0
\end{array}
\right)
$$

所以兩個參數時的 Wald 檢驗統計量爲：

$$
\begin{aligned}
W = & (\hat\lambda-\lambda_0, \hat\psi-\psi_0)(-\underline{\ell^{\prime\prime}}(\hat\lambda,\hat\psi))\left(
\begin{array}{c}
\hat\lambda-\lambda_0 \\
\hat\psi-\psi_0
\end{array}
\right) \\
= & - (\hat\lambda-\lambda_0, \hat\psi-\psi_0)\left(
\begin{array}{c}
\frac{\partial^2\ell}{\partial\lambda^2} & \frac{\partial^2\ell}{\partial\lambda\partial\psi} \\
\frac{\partial^2\ell}{\partial\psi\partial\lambda} & \frac{\partial^2\ell}{\partial\psi^2}
\end{array}
\right)_{\hat\lambda,\hat\psi}
\left(
\begin{array}{c}
\hat\lambda-\lambda_0 \\
\hat\psi-\psi_0
\end{array}
\right)\\
 & \text{ Because } \lambda \text{ and } \psi \text{ are independent,} \\
 & \text{ so their covariance } \frac{\partial^2\ell}{\partial\lambda\partial\psi} = \frac{\partial^2\ell}{\partial\psi\partial\lambda} = 0\\
 \Rightarrow  = & - (\hat\lambda-\lambda_0, \hat\psi-\psi_0)\left(
 \begin{array}{c}
 \ell^{\prime\prime}(\hat\lambda)  & 0 \\
 0 & \ell^{\prime\prime}(\hat\psi)
 \end{array}
 \right)
 \left(
 \begin{array}{c}
 \hat\lambda-\lambda_0 \\
 \hat\psi-\psi_0
 \end{array}
 \right)\\
 = &  - (\hat\lambda-\lambda_0, \hat\psi-\psi_0)\left(
 \begin{array}{c}
 \ell^{\prime\prime}(\hat\lambda)(\hat\lambda-\lambda_0) \\
 \ell^{\prime\prime}(\hat\psi)(\hat\psi-\psi_0)
 \end{array}
 \right) \\
= & (\hat\lambda-\lambda_0)^2(-\ell^{\prime\prime}(\hat\lambda)) + (\hat\psi-\psi_0)^2(-\ell^{\prime\prime}(\hat\psi)) \stackrel{\cdot}{\sim} \chi^2_2
\end{aligned}
$$

由此可見，兩個參數分開來考慮之後把統計量相加，和一開始就把兩個參數放在一起，利用矩陣計算後獲得的檢驗統計量完全相同。用矩陣的好處是可以把上面的推導過程直接擴展成 $k$ 個參數的形式，且標記簡便：

$$
W = -(\hat{\underline{\theta}} - \underline{\theta_0})^T\underline{\ell^{\prime\prime}(\hat\theta)}(\underline{\hat\theta} - \underline{\theta_0})  \stackrel{\cdot}{\sim} \chi^2_k
$$

## 多參數 Score 檢驗 - Score test

單個參數時的 Score 檢驗的檢驗統計量：

$$
 \text{H}_0: \theta=\theta_0 \text{ v.s. H}_1: \theta \neq \theta_0 \\
 \frac{U^2}{V} \stackrel{\cdot}{\sim} \chi^2_1 \\
 \text{Where } U=\ell^\prime(\theta_0), V=E[-\ell^{\prime\prime}(\theta_0)]
$$

類似 Wald 檢驗法的矩陣推導過程和標記法，$k$ 個參數的 Score 檢驗的統計量可以標記爲：

$$
\underline{U}^T\underline{V}^{-1}\underline{U} \stackrel{\cdot}{\sim} \chi^2_k \\
\text{Where } \underline{U} = \left.\frac{\partial\ell}{\partial\underline{\theta}} \right\vert_{\underline{\theta}=\underline{\theta_0}},
\underline{V} = E[-\underline{\ell^{\prime\prime}(\theta)}]_{\underline{\theta}=\underline{\theta_0}}
$$

所以如果是兩個參數 $\lambda, \psi$ 那麼檢驗 $\text{H}_0:\lambda = \lambda_0, \psi = \psi_0 \text{ v.s. H}_1: \lambda \neq \lambda_0 \text{ or } \psi\neq\psi_0$ 的 Score 檢驗統計量是：

$$
(\frac{\partial\ell}{\partial\lambda}, \frac{\partial\ell}{\partial\psi})_{\lambda_0, \psi_0}\left(
E\left[
-\left(
\begin{array}{c}
\frac{\partial^2\ell}{\partial\lambda^2} & \frac{\partial^2\ell}{\partial\lambda\partial\psi} \\
\frac{\partial^2\ell}{\partial\psi\partial\lambda} & \frac{\partial^2\ell}{\partial\psi^2}
\end{array}
\right)_{\lambda_0,\psi_0}
\right]
\right)^{-1}\left(
\begin{array}{c}
\frac{\partial\ell}{\partial\lambda}\\
\frac{\partial\ell}{\partial\psi}
\end{array}
\right)_{\lambda_0,\psi_0} \stackrel{\cdot}{\sim} \chi^2_2
$$

## 條件似然 conditional likelihood {#condilikeli}

現實的例子中，參數可能有非常多，但是我們可能只關心其中幾個。下章介紹的子集似然函數 (profile likelihood) 是可以在多種情況下應用的好方法。本節介紹的方法是**條件似然法**。簡單原理是，把模型中不能提供我們感興趣的參數的有效信息的那些參數 ("nuisance" parameters) 當作是固定的 (fixed)。由此可以定義一個新的概率模型 -- **條件概率模型 conditional probability model**。

我們用泊松模型來解釋如何建立這樣的模型。

兩個獨立的人羣追蹤樣本，在 $p_0, p_1$ 人年的隨訪中發生事件 A 的次數分別是 $k_0, k_1$。假設我們只關心兩組的事件 A 發生率的比 $\text{Rate ratio:} \theta=\frac{\lambda_1}{\lambda_0}$。

合併兩個人羣，發生事件 A 的總次數爲 $k=k_0+k_1$。只知道 $k$ 並不能讓我們推算兩個人羣中各發生了多少次事件 A，也無法用它來計算發生率的比 $\theta$，而這個 $k$ 就是條件概率模型中的條件。

$$
K_0 \sim Po(\mu_0); K_1 \sim Po(\mu_1) ; \text{ where } \mu_0 = \lambda_0 p_0 \mu_1 = \lambda_1 p_1\\
k=k_0 + k_1 \Rightarrow K_0+K_1 \sim Po(\mu_0 + \mu_1)
$$

$$
\begin{aligned}
  & \text{Prob}(k_0 \text{events in group 0} | k \text{ events in total}) \\
= & \frac{\text{Prob}(k_0 \text{ events in group }0 \text{ and } k-k_0 \text{ events in  group } 1)}
 {\text{Prob}(k \text{ events in total})} \\
\end{aligned}
(\#eq:infer9-1)
$$

由於兩個樣本是來自獨立的人羣，所以公式 \@ref(eq:infer9-1) 的分母，和分子分別是

$$
\begin{aligned}
\text{Prob}(k &\text{ events in total}) \\
 = & \frac{(\lambda_0 p_0 + \lambda_1 p_1)^k e^{-(\lambda_0 p_0 + \lambda_1 p_1)}}{k!} \\
\text{Prob}(k_0 &\text{ events in group }0 \text{ and } k-k_0 \text{ events in  group } 1) \\
 = & \frac{(\lambda_0 p_0)^{k_0}e^{-\lambda_0 p_0}}{k_0!}\times\frac{(\lambda_1 p_1)^{k-k_0}e^{-\lambda_1 p_1}}{(k-k_0)!}
\end{aligned}
$$

所以公式 \@ref(eq:infer9-1) 可以整理成：

$$
\begin{aligned}
& \frac{\frac{(\lambda_0 p_0)^{k_0}e^{-\lambda_0 p_0}}{k_0!}\times\frac{(\lambda_1 p_1)^{k-k_0}e^{-\lambda_1 p_1}}{(k-k_0)!}}
{\frac{(\lambda_0 p_0 + \lambda_1 p_1)^k e^{-(\lambda_0 p_0 + \lambda_1 p_1)}}{k!}} \\
= &  \frac{e^{-(\lambda_0 p_0 + \lambda_1 p_1)}(\lambda_0 p_0)^{k_0}(\lambda_1 p_1)^{k-k_0}\cdot k!}{e^{-(\lambda_0 p_0 + \lambda_1 p_1)}(\lambda_0p_0+\lambda_1p_1)^k\cdot k_0!\cdot (k-k_0)!}\\
= & (\frac{\lambda_0 p_0}{\lambda_0 p_0+\lambda_1 p_1})^{k_0}(\frac{\lambda_1 p_1}{\lambda_0 p_0+\lambda_1 p_1})^{k-k_0}\cdot\frac{k!}{k_0!(k-k_0)!} \\
= & (\pi)^{k_0}(1-\pi)^{k-k_0}\cdot\frac{k!}{k_0!(k-k_0)!} \\
\text{Where } & \pi = \frac{\lambda_0 p_0}{\lambda_0 p_0 + \lambda_1 p_1} = \frac{p_0}{p_0+(\lambda_1/\lambda_0)p_1} = \frac{p_0}{p_0+\theta p_1}\\
\Rightarrow &\text{ Given } K_0+K_1=K, K_0 \sim Bin(k, \pi=\frac{p_0}{p_0+\theta p_1})
\end{aligned}
$$

我們就把兩個泊松分佈的模型，變形成爲了一個條件二項分佈，而且只有一個未知參數 $\theta$。之後就可以用二項分佈的對數似然方程進行下一步的假設檢驗的構建：

$$
\begin{aligned}
               L(\pi) & = (\pi)^{k_0}(1-\pi)^{k-k_0}  \\
\Rightarrow \ell(\pi) & = k_0 \text{log}\pi + (k-k_0)\text{log} (1-\pi) \\
\text{Because }  \pi  & = \frac{p_0}{p_0+\theta p_1} \\
        \ell_c(\theta)  & = k_0 \text{log}(\frac{\pi}{1-\pi}) + k\text{log}(1-\pi) \\
                      & = k_0 \text{log}(\frac{p_0}{\theta p_1}) + k\text{log}(\frac{\theta p_1}{p_0 + \theta p_1}) \\
\text{Ignoring} & \text{ terms not involving } \theta \\
        \ell_c(\theta)& = k_1 \text{log}\theta - k\text{log}(p_0 + \theta p_1)
\end{aligned}
(\#eq:inference9-2)
$$

至此，推導發生率比 $\theta = \frac{\lambda_1}{\lambda_0}$ 的條件對數似然就完成了。Elegant and Bravo!

關於條件對數似然：

1. 推導出的條件對數似然是一個**真實**的以觀察數據爲條件的對數似然，可以用於假設檢驗；
2. 條件似然過程依賴於我們能否找到這樣一個“條件似然”，使得模型的對數似然**只取決於我們關心的參數**，我們幸運地找到了發生率比的對數似然方程，**但是至今沒有人找到發生率差 $\lambda_1-\lambda_0$ 的條件對數似然**；
3. 與此相對地是，下一章介紹的子集似然函數 (profile likelihood)，可以用於幾乎所有的多參數模型的假設檢驗之構建；
4. 但是，條件對數似然相當之重要，特別是它作爲 Cox proportional hazard model 模型的基本模型構架在生存分析 (survival analysis) 中的應用，以及在配對病例對照分析 (matched case-control study) 中用於條件邏輯迴歸 (conditional logistic regression) 的理論基礎 (將會在第二學期的碩士課程中介紹，敬請期待)。

## 練習

某項研究追蹤隨訪 50-69 歲男性的心臟病發病率。研究對象根據心臟病發病史的有無分成兩組。有心臟病史的對象被隨訪 512 人・年，觀察到 25 例新的心臟病發作病例；無心臟病史的對象被隨訪 4862 人・年，觀察到 52 例新的心臟病發作病例。

1. 如果需要檢驗的零假設是 $\text{H}_0:$ 有心臟病史的男性**發病率的對數**等於 $-3$，無心臟病史的男性發病率的對數等於 $-4.5$。請推導該實驗的**聯合**對數似然比檢驗，Wald 檢驗兩種檢驗法的檢驗統計量，並進行假設檢驗。

**解**

- 模型：

令隨機變量 $K_i$ 標記新發生的心臟病病例數，其中當 $i=0$ 時代表**無心臟病史組**；當 $i=1$ 時代表**有心臟病史組**。所以可以用下面的泊松模型來標記兩組的新發生心臟病病例數：

$$
K_i \sim \text{Poisson}(\mu_i); \mu_i = \lambda_i p_i\\
\text{Where } \lambda_i \text{ is the rate parameter in group } i, \\
p_i \text{ is the person-years at risk in group }i \\
$$

有無心臟病史組之間由於是相互獨立的，故兩組的對數似然相加之後就可得到合併後的對數似然。

- 數據：

$$
k_0 = 52, p_0 = 4862; k_1 = 25, p_1 = 512
$$


泊松模型的對數似然方程爲 (Section \@ref(likelihood-poi))：

$$
\ell(\lambda | \text{data}) = -\lambda p + k \text{log} \lambda
$$

令 $\psi = \text{log} \lambda$ 有：

$$
\ell(\psi) = k \psi - e^\psi p
$$

令 $\psi_0 = \text{log}\lambda_0; \psi_1 = \text{log}\lambda_1$，那麼本題中的假設檢驗可以寫成是：

$$\text{H}_0: {\psi_0}_0 = -4.5, {\psi_1}_0 = -3 \text{ v.s. H}_1: {\psi_0}_0 \neq -4.5 \text{ or } {\psi_1}_0 \neq -3$$

a. 對數似然比檢驗需要尋找的檢驗統計量是 $-2llr({\psi_0}_0,{\psi_1}_0)$，其中：

$$
llr({\psi_0}_0,{\psi_1}_0) = \ell({\psi_0}_0,{\psi_1}_0) - \ell(\hat\psi_0,\hat\psi_1)
$$

所以我們分別來計算 $\ell({\psi_0}_0,{\psi_1}_0)$ 和 $\ell(\hat\psi_0,\hat\psi_1)$：

$$
\begin{equation}
\ell(\psi_0, \psi_1) = k_0 \psi_0 - e^{\psi_0} p_0 + k_1 \psi_1 - e^{\psi_1} p_1
\end{equation}
(\#eq:infer9-prac-1)
$$

$$
\Rightarrow \frac{\partial\ell}{\partial\psi_0} = k_0 - e^{\psi_0}p_0 \\
\text{and} \\
\frac{\partial\ell}{\partial\psi_1} = k_1 - e^{\psi_1}p_1
$$

然後我們把這兩個偏微分式子等於零時的解作爲 $\psi_0, \psi_1$ 的 $\text{MLE}$：

$$
\begin{aligned}
\frac{\partial\ell}{\partial{\psi}_0} & = 0 \\
\Rightarrow          e^{{\hat\psi}_0} & = \frac{k_0}{p_0} \\
\Rightarrow             {\hat\psi}_0  & = \text{log}(\frac{k_0}{p_0}) \\
\text{And similarly }   {\hat\psi}_1  & = \text{log}(\frac{k_1}{p_1})
\end{aligned}
$$

所以，

$$
\begin{aligned}
\ell({\psi_0}_0,{\psi_1}_0) & = 52\times(-4.5) - e^{-4.5}\times4862+25\times(-3)-e^{-3}\times512 \\
                            & = -388.5029 \\
\ell(\hat\psi_0,\hat\psi_1) & = 52\times\text{log}\frac{52}{4862} - e^{\text{log}\frac{52}{4862}}\times4862 + 25\times\text{log}\frac{25}{512} - e^{\text{log}\frac{25}{512}}\times512 \\
                            & = 52\times\text{log}\frac{52}{4862} - 52 + 25\times\text{log}\frac{25}{512} - 25 \\
                            & = -388.4602 \\
\Rightarrow llr({\psi_0}_0,{\psi_1}_0)  & =   -388.5029 - (-388.4602) = - 0.0427 \\
\Rightarrow -2llr({\psi_0}_0,{\psi_1}_0)  & = 0.0854
\end{aligned}
$$

因爲在零假設條件下 $-2llr \stackrel{\cdot}{\sim} \chi^2_2$，本次檢驗的拒絕域是 $\mathfrak{R} > \chi^2_{2,0.95} = 5.99$，所以，檢驗的結果 $-2llr = 0.0854 < 5.99$，在顯著性水平爲 $5\%$ 時，沒有證據反對零假設。There is no evidence at the $5\%$ level against the null hypothesis.


b. Wald 檢驗時我們需要的檢驗統計量爲：

$$
W = (\hat\psi_0-{\psi_0}_0, \hat\psi_1-{\psi_1}_0)(-\underline{\ell^{\prime\prime}}(\hat\psi_0,\hat\psi_1))\left(
\begin{array}{c}
\hat\psi_0-{\psi_0}_0 \\
\hat\psi_1-{\psi_1}_0
\end{array}
\right)
$$

先處理中間那個看起來比較棘手的 $(-\underline{\ell^{\prime\prime}}(\hat\psi_0,\hat\psi_1))$：

$$
\begin{aligned}
\underline{\ell^\prime}(\psi_0, \psi_1) & = \left(
\begin{array}{c}
k_0 - e^{\psi_0}p_0 \\
k_1 - e^{\psi_1}p_1
\end{array}
\right) \\
\Rightarrow \underline{\ell^{\prime\prime}}(\psi_0,\psi_1) & = \left(
\begin{array}{c}
\frac{\partial^2\ell}{\partial\psi^2_0} & \frac{\partial^2\ell}{\partial\psi_1\partial\psi_0} \\
\frac{\partial^2\ell}{\partial\psi_0\partial\psi_1} & \frac{\partial^2\ell}{\partial\psi^2_1}
\end{array}
\right) = \left(
\begin{array}{c}
-e^{\psi_0}p_0  & 0\\
0  & -e^{\psi_1}p_1
\end{array}
\right) \\
\Rightarrow -\underline{\ell^{\prime\prime}}(\hat\psi_0,\hat\psi_1) & = \left(
\begin{array}{c}
-e^{\hat\psi_0}p_0  & 0\\
0  & -e^{\hat\psi_1}p_1
\end{array}
\right) \\
 & = \left(
\begin{array}{c}
-e^{\text{log}(\frac{52}{4862})}\times4862  & 0\\
0  & -e^{\text{log}(\frac{25}{512})}\times512
\end{array}
\right) \\
& = \left(
\begin{array}{c}
52  & 0\\
0  & 25
\end{array}
\right)
\end{aligned}
$$

又有 $\hat\psi_1-{\psi_1}_0 = \text{log}(\frac{25}{512})-(-3) = -0.0194$

和 $\hat\psi_0-{\psi_0}_0 = \text{log}(\frac{52}{4862})-(-4.5) = -0.0379$

所以

$$
\begin{aligned}
W & = (\hat\psi_0-{\psi_0}_0, \hat\psi_1-{\psi_1}_0)(-\underline{\ell^{\prime\prime}}(\hat\psi_0,\hat\psi_1))\left(
\begin{array}{c}
\hat\psi_0-{\psi_0}_0 \\
\hat\psi_1-{\psi_1}_0
\end{array}
\right) \\
  & = (-0.0379, -0.0194)\left(
  \begin{array}{c}
  52  & 0\\
  0  & 25
  \end{array}
  \right)\left(
  \begin{array}{c}
  -0.0379 \\
  -0.0194
  \end{array}
  \right) = 0.08439208
\end{aligned}
$$

Wald 檢驗的檢驗統計量也一樣服從 $\chi^2_2$，所以拒絕域同對數似然比檢驗法的$\mathfrak{R} > \chi^2_{2,0.95} = 5.99$，所以，檢驗的結果 $W = 0.08439208 < 5.99$，在顯著性水平爲 $5\%$ 時，沒有證據反對零假設。There is no evidence at the $5\%$ level against the null hypothesis.

2. 利用本節推導出的發生率比的**條件對數似然方程**，請嘗試進行對數似然比檢驗：心臟病發作率在無病史男性中和有病史男性中的比例爲 $0.2$。

本章推導的發生率的比值的條件對數似然方程爲：

$$
\ell_c(\theta)  = k_1 \text{log}\theta - k\text{log}(p_0 + \theta p_1) \\
\text{Where } \theta = \frac{\lambda_1}{\lambda_0}
$$

題目要求比較的是 $\frac{\lambda_0}{\lambda_1} = 0.2$，用本題中的 $\lambda_0$ 取代條件對數似然方程中的 $\lambda_1$ 則有：

$$
\ell_c{\theta} = k_0\text{log}\theta - k\text{log}(p_1 + \theta p_0) \\
\text{H}_0: \theta_0 = 0.2 \text{ v.s. H}_1: \theta_0 \neq 0.2
$$

對於條件對數似然比檢驗，需要的檢驗統計量是 $-2llr_c(\theta_0)$ 其中：

$$
llr_c(\theta_0) = \ell_c(\theta_0) - \ell_c(\hat\theta)
$$

先計算 $\ell_c(\hat\theta)$：

$$
\begin{aligned}
      \text{Let }\ell_c^\prime & = \frac{k_0}{\theta} - \frac{kp_0}{p_1+\theta p_0} = 0 \\
\Rightarrow \frac{k_0}{\theta} & = \frac{kp_0}{p_1+\theta p_0} \\
\Rightarrow \hat\theta         & = \frac{k_0p_1}{p_0k_1} = \frac{k_0/p_0}{k_1/p_1} \\
\Rightarrow \hat\theta         & = \frac{52\times512}{4862\times25} = 0.219037 \\
\Rightarrow \ell_c(\theta_0)   & = k_0\text{log}0.2 -  k\text{log}(p_1 + \theta p_0) \\
                               & = 52\times\text{log}0.2 - 77\times\text{log}(512 + 0.2\times4862) \\
                               & = -646.003 \\
          \ell_c{\hat\theta}   & = 52\times\text{log}0.219037 - 77\times\text{log}(512 + 0.219037\times4862)\\
                               & = -645.933 \\
\Rightarrow -2llr(\theta_0)    & = -2\times(-646.003-(-645.933)) = 0.14
\end{aligned}
$$

因爲在零假設條件下 $-2llr \stackrel{\cdot}{\sim} \chi^2_1$，本次檢驗的拒絕域是 $\mathfrak{R} > \chi^2_{1,0.95} = 3.84$，所以，檢驗的結果 $-2llr = 0.14 < 3.84$，在顯著性水平爲 $5\%$ 時，沒有證據反對零假設。There is no evidence at the $5\%$ level against the null hypothesis.


# 多個參數時的統計推斷 -- 子集似然函數 profile log-likelihoods {#profile-log-likelihood}

本章介紹的子集似然法是處理多個參數模型的主要方法。前章介紹的**條件似然法**也是相當出色的方法，但是許多情況下我們無法找到合適的“條件”來輔助我們擺脫那些模型中不需要的，**障礙 (或者叫噪音) 參數 nuisance parameters**。

我們還是沿用上一節的例子。

兩個獨立的人羣追蹤樣本，在 $p_0, p_1$ 人年的隨訪中發生事件 A 的次數分別是 $k_0, k_1$。我們只關心兩組的事件 A 發生率的比 $\text{Rate ratio:} \theta=\frac{\lambda_1}{\lambda_0}$。兩個人羣的聯合對數似然函數如下：

$$
\ell(\lambda_0, \lambda_1) = k_0\text{log}\lambda_0 - \lambda_0p0 + k_1\text{log}\lambda_1 - \lambda_1p1
$$

- Step 1. 先用 $\lambda_1 = \lambda_0\theta$ 取代掉上面式子中的 $\lambda_1$。

$$
\begin{aligned}
\Rightarrow \ell(\lambda_0, \theta) & = k\text{log}\lambda_0 + k_1\text{log}\theta - \lambda_0(P_0 + \theta p_1) \\
\text{Where } k & = k_0 + k_1
\end{aligned}
(\#eq:infer10-1)
$$

這一步先是消滅了一個障礙參數 $\lambda_1$，獲得了一個我們關心的參數 $\theta$，和 $\lambda_0$ 的對數似然方程。接下來，我們尋找用 $\theta$ 表示 $\lambda_0$ (用 $\hat\lambda_0(\theta)$ 標記) 的似然方程，使得只包含一個參數 $\theta$ 的對數似然方程可以在每個 $\lambda_0$ 時取得極大值。此時我們定義 $\theta$ 的子集對數似然方程 profile log-likelihood是：

$$
\ell_p(\theta) = \ell(\hat\lambda_0(\theta),\theta)
$$

- Step 2. 爲了求 $\hat\lambda_0(\theta)$，先視 $\theta$ 爲不變的，對上式 \@ref(eq:infer10-1) 求 $\lambda_0$ 的微分：

$$
\frac{\partial\ell(\lambda_0,\theta)}{\partial\lambda_0}=\frac{k}{\lambda_0} - (p_0+\theta p_1)
$$

把該微分方程等於0，推導出 $\hat\lambda_0=\frac{k}{p_0+\theta p_1}$ 就是 $\theta$ 在取值範圍內所有能使對數似然方程 \@ref(eq:infer10-1) 取極大值的對應 $\lambda_0$。

- Step 3. 將這個 $\theta$ 表示的 $\lambda_0\text{ MLE}$ 代替 $\lambda_0$ 代入對數似然方程 \@ref(eq:infer10-1) 中去：

$$
\begin{aligned}
\ell_p(\theta) &= k\text{log}\frac{k}{p_0 + \theta p_1} + k_1 \text{log}\theta - k \\
\text{Ignoring} &\text{ items not involving } \theta\\
\Rightarrow &= k_1\text{log}\theta - k\text{log}(p_0+\theta p_1)
\end{aligned}
$$

這個用子集似然法推導的關於參數 $\theta$ 的似然方程和前一章用條件似然法 (Section \@ref(condilikeli)) 推導的結果是完全一致的 \@ref(eq:inference9-2)。

## 子集似然法推導的過程總結

1. 多個參數中區分出我們感興趣的參數 $\psi$ 和其餘的障礙(噪音)參數 $\lambda$；
2. 爲了從對數似然方程中消除噪音參數，把它們一一通過微分求極值的辦法表達成用 $\psi$ 標記的表達式，用這些包含了 $\psi$ 的 $\text{MLE}$ 代替所有的噪音參數；
3. 整理最終獲得的只有感興趣的參數的對數似然方程，記得把不包含參數的部分忽略掉。

### 子集對數似然方程的分佈

$$
-2pllr(\psi) = -2\{ \ell_p(\psi) - \ell(\hat\psi)\} \stackrel{\cdot}{\sim} \chi^2_r
$$

其中自由度 $r$ 是想要檢驗的零假設中受限制的參數的個數。Degree of freedom $r$ is the number of parameters restricted under the null hypothesis. 所以，如果 $\psi$ 是一個維度 (dimension) 爲 $p$ 的向量，如果零假設是 $\text{H}_0: \psi = \psi_0$，那麼自由度就是 $p$。

### 假設檢驗過程舉例

兩個獨立的二項分佈樣本：$K_0 \sim \text{Bin}(n_0, \pi_0), K_1 \sim \text{Bin}(n_1, \pi_1)$。它們的聯合對數似然爲：

$$
\ell(\pi_0, \pi_1) = \ell(\pi_0) + \ell(\pi_1)
$$

如果要檢驗的零假設和替代假設分別是 $\text{H}_0: \pi_0 = \pi_1 \text{ v.s. H}_1: \pi_0 \neq \pi_1$。

如果令 $\theta=\frac{\pi_1}{\pi_0}$，那麼要檢驗的零假設和替代假設就變成了：

$$
\text{H}_0: \theta = 1 \text{ v.s. H}_1: \theta \neq 1 \\
\Rightarrow -2 pllr \stackrel{\cdot}{\sim} \chi^2_1
$$

而且在零假設條件下，$\text{H}_0: K_0+K_1 \sim \text{Bin}(n_0+n_1, \pi)$，那麼自己對數似然比檢驗的統計量是：

$$
\begin{aligned}
-2 pllr & = -2\{ \text{max}[\underset{\text{H}_0}{\ell(\pi_0,\theta\pi_0)}] -\text{max}[\underset{\text{H}_1}{\ell(\pi_0,\theta\pi_0)}] \} \\
\Rightarrow -2 pllr & =  -2\{ \text{max}[\underset{\text{H}_0}{\ell(\pi,\theta\pi)}] -\text{max}[\underset{\text{H}_1}{\ell(\pi_0,\pi_1)}] \} \\
\Rightarrow -2 pllr & = -2\{ \ell{(\hat\pi)} - \ell{(\hat\pi_0, \hat\pi_1)} \}
\end{aligned}
$$

## 子集對數似然比的近似

假如有兩個獨立樣本數據，參數分別只有一個 $\beta_0, \beta_1$，我們關心他們二者之間的差是否有意義 $\gamma = \beta_1-\beta_0$。如果 $\beta_0$ 的對數似然比檢驗統計量的相應的 Wald 檢驗統計量 (二次方程近似法 Section \@ref(Wald)) 可以用 $\hat\beta_0, S_0$ 定義，其中 $\beta_0$ 是 $\text{MLE}$，$S_0$ 是標準誤差。類似的，$\beta_1$ 的 Wald 檢驗統計量可以用 $\hat\beta_1, S_1$ 定義。那麼，我們關心的參數，$\gamma = \beta_1 - \beta_0$ 的 Wald 檢驗統計量可以用 $\hat\gamma = \hat\beta_1 - \hat\beta_1, S=\sqrt{S^2_1 + S^2_0}$ 定義：

$$
\begin{aligned}
pllr(\gamma) & = -\frac{1}{2}(\frac{\gamma-\hat\gamma}{\sqrt{S^2_1+S^2_0}})^2 \\
& = -\frac{1}{2}(\frac{(\beta_1-\beta_0)-(\hat\beta_1-\hat\beta_0)}{\sqrt{S^2_1+S^2_0}})^2
\end{aligned}
$$

### 子集對數似然比近似的一般化

如果我們關心的參數，和模型參數的關係可以用下面的表達式來表示：

$$
\gamma = W_0\beta_0 + W_1\beta_1 + \cdots \\
\text{ Where } W_i \text{ are arbitrary cosntants}
$$

如果，模型中的每個參數 $\beta_0, \beta_1, \cdots$ 的 $\text{MLE}$ 是 $\hat\beta_0, \hat\beta_1, \cdots$，標準誤是 $S=\sqrt{(W_0S_0)^2+(W_1S_2)^2+\cdots}$

### 事件發生率之比的 Wald 檢驗統計量

事件發生率 (Possion rate ratio) $\theta = \frac{\lambda_1}{\lambda_0}$

令 $\beta_1 = \text{log}\lambda_1, \beta_0 = \text{log}\lambda_0, \gamma = \text{log}\theta$。

所以有 $\gamma=\beta_1-\beta_0$。

由於

$$
\begin{aligned}
\hat\beta_0 & = \text{log}(\frac{k_0}{p_0}), \\
\hat\beta_1 & = \text{log}(\frac{k_1}{p_1}) \\
\end{aligned}
$$


因而

$$
\begin{aligned}
\hat\gamma & = \text{log}\frac{k_1}{p_1} - \text{log}\frac{k_0}{p_0} \\
           & = \text{log}\frac{k_1/p_1}{k_0/p_0}
\end{aligned}
$$

又由於 $S_0 = \frac{1}{\sqrt{k_0}}, S_1 = \frac{1}{\sqrt{k_1}}$ (Section \@ref(Possion-log-transform))。

所以 $S=\sqrt{\frac{1}{k_0}+\frac{1}{k_1}}$。

綜上，事件發生率之比的 Wald 檢驗統計量爲


$$
\begin{aligned}
pllr(\gamma) & = -\frac{1}{2}(\frac{\gamma - \hat\gamma}{\sqrt{\frac{1}{k_0}+\frac{1}{k_1}}})^2 \\
             & = -\frac{1}{2}(\frac{\text{log}\theta - \text{log}\frac{k_1/p_1}{k_0/p_0}}{\sqrt{\frac{1}{k_0}+\frac{1}{k_1}}})^2
\end{aligned}
$$

## 練習 Practical {#practical0}

$n$ 名肺癌 I 期患者的倖存時間 $X_1, X_2, \cdots, X_n$ 被認爲服從指數分佈 (參數 $\lambda_x$)，概率方程爲 $\lambda_x e^{-x\lambda_x},\text{ where } x > 0$。

1. 證明 $\lambda_x$ 的 $\text{MLE}$ 是 $\hat\lambda_x = \frac{1}{\bar{x}}$, 對數似然方程是 $$\ell(\lambda_x | \underline{x}) = n\text{log}\lambda_x - \lambda_x n \bar{x}$$

**解**

$$
\begin{aligned}
f(\underline{x}|\lambda_x) & = \lambda_x\cdot e^{-x\lambda_x} \\
F(\underline{x}|\lambda_x)  & = \prod_{i=1}^n\lambda_{x}\cdot e^{-x_i\lambda_{x}} \\
\Rightarrow L(\lambda_x | \underline{x}) & = \prod_{i=1}^n\lambda_xe^{-x_i\lambda_{x}} \\
\Rightarrow \ell(\lambda_x|\underline{x}) & = \sum_{i=1}^n(\text{log}\lambda_x + \text{log}e^{-x_i\lambda_{x}}) \\
                                        & = n\text{log}\lambda_x  + \sum_{i=1}^n(-x_i\lambda_{x}) \\
                                        & = n\text{log}\lambda_x - n\bar{x}\lambda_x \\
\Rightarrow \ell^\prime(\lambda_x) & = \frac{n}{\lambda_x} - n\bar{x}\lambda_x \\
\text{Let } \ell^\prime(\lambda_x) & = 0 \Rightarrow \text{ MLE of } \lambda_x \text{ is } \hat\lambda_x = \frac{1}{\bar{x}} \\
\because \ell^{\prime\prime} = -\frac{n}{\lambda^2_x} & < 0 \therefore \frac{1}{\bar{x}} \text{ is the MLE}
\end{aligned}
$$


2. 另一組獨立數據是樣本量爲 $n$ ，但是肺癌診斷爲 II 期的患者的倖存時間 $Y_1, \cdots, Y_n$。這組數據也被認爲服從參數爲 $\lambda_y$ 的指數分佈。用 $\theta=\frac{\lambda_x}{\lambda_y}$ 標記兩組患者倖存時間之比，用 $r=\frac{\bar{x}}{\bar{y}}$ 標記樣本的倖存時間均值之比。證明使兩個樣本數據的聯合對數似然取極大值的 $\hat\lambda_y(\theta) = \frac{2}{\bar{y}(\theta r+1)}$。

**解**

$$
\begin{aligned}
\ell(\lambda_x|\underline{x}) & = n\text{log}\lambda_x - n \bar{x} \lambda_x \\
\ell(\lambda_y|\underline{y}) & = n\text{log}\lambda_y - n \bar{y} \lambda_y \\
\Rightarrow \text{ Joint log-likelihood: } & \ell(\lambda_x, \lambda_y | \underline{x}, \underline{y}) = n\text{log}\lambda_x - n\bar{x}\lambda_x \\
& \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+  n\text{log} \lambda_y - n\bar{y}\lambda_y \\
\text{Subsitute } \lambda_x & =\theta\cdot\lambda_y \\
\Rightarrow \ell(\theta, \lambda_y) &= n\text{log}\theta\lambda_y - n\bar{x}\theta\lambda_y + n\text{log} \lambda_y - n\bar{y}\lambda_y \\
\ell(\theta, \lambda_y) & = n(\text{log}\theta + \text{log}\lambda_y - \bar{x}\theta\lambda_y + \text{log}\lambda_y - \bar{y}\lambda_y) \\
                        & = n[\text{log}\theta + 2\text{log}\lambda_y - \lambda_y(\bar{x}\theta + \bar{y})] \\
\Rightarrow \frac{\partial\ell(\theta, \lambda_y)}{\partial \lambda_y} & = n[\frac{2}{\lambda_y} - (\bar{x}\theta + \bar{y})] \\
\text{Let } \frac{\partial\ell(\theta, \lambda_y)}{\partial \lambda_y} & = 0 \text{ and because } r = \frac{\bar{x}}{\bar{y}} \\
\hat\lambda_y(\theta) & = \frac{2}{\bar{x}\theta + \bar{y}} = \frac{2}{\bar{y}(r\cdot\theta +1)}
\end{aligned}
$$

3. 證明參數 $\theta$ 的子集對數似然是 $\ell_p(\theta|r) = n\text{log}\theta - 2n \text{log}(\theta\cdot r + 1)$，且 $\text{MLE}$ 是 $\hat\theta = \frac{1}{r}$

**解**

$$
\begin{aligned}
\ell_p (\theta) & = n[\text{log}\theta + 2\cdot\text{log}\frac{2}{\bar{y}(r\cdot\theta +1)} - \text{log}\frac{2}{\bar{y}(r\cdot\theta +1)}(\bar{x}\theta+\bar{y})] \\
                & = n\{\text{log}\theta + 2\cdot\text{log}2 - 2\cdot\text{log}[\bar{y}(r\theta+1)] -2 \} \\
\text{Ignoring } & \text{ items not involving } \theta\\
                & = n[\text{log}\theta - 2\text{log}(r\theta+1)] \\
\Rightarrow \ell_p^{\prime}(\theta) & = n(\frac{1}{\theta} - \frac{2r}{r\theta+1}) \\
\text{Let } \ell_p^{\prime}(\theta) & = 0 \Rightarrow  n(\frac{1}{\theta} - \frac{2r}{r\theta+1}) = 0 , \hat\theta=\frac{1}{r}\\
\because  \ell_p^{\prime\prime}(\theta) & = -\frac{1}{\theta^2} - \frac{2r^2}{(r\theta^2+1)^2} < 0 \\
\therefore \hat\theta & =\frac{1}{r} \text{ is the MLE}
\end{aligned}
$$


4. 根據 $\text{MLE}$ 的恆定性，可以直接推導出 $\theta$ 的 $\text{MLE}$ 嗎?

**解**

$$
\because \hat\lambda_x = \frac{1}{x} , \hat\lambda_y = \frac{1}{y} \\
\therefore \theta = \frac{\lambda_x}{\lambda_y} \Rightarrow \hat\theta = \frac{\hat\lambda_x}{\hat\lambda_y} = \frac{1}{r}
$$


5. 證明檢驗下列假設 $\text{H}_0: \theta_0 = 1 \text{ v.s. H}_1: \theta_0 \neq 1$ 的子集對數似然比檢驗統計量是 $2n\text{log}\frac{(r+1)^2}{4r}$，並進行 $n=16, r=2$ 的假設檢驗。

**解**

$$
\begin{aligned}
\text{Under H}_0 & \Rightarrow \text{ test statistic is } \\
-2llr(\theta_0)  & = -2[\ell(\theta_0) - \ell(\hat\theta)] \stackrel{\cdot}{\sim} \chi^2_1 \\
\Rightarrow \ell_p(\theta_0) & = n\text{log}1 - 2n \text{log}(r+1) = -2n\text{log}(r+1) \\
          \ell_p(\hat\theta) & = n\text{log}\frac{1}{r} - 2n\text{log}(2) \\
                             & = -n\text{log}r-2n\text{log}2 = -n\text{log}4r\\
\Rightarrow \ell_p(\theta_0) - \ell_p(\hat\theta) & = -2n\text{log}(r+1) + n\text{log}4r = n\text{log}\frac{4r}{(r+1)^2} \\
\Rightarrow -2llr(\theta_0)  & = -2n\text{log}\frac{4r}{(r+1)^2} = 2n\text{log}\frac{(r+1)^2}{4r} \\
\text{ When } n=16, r=2 -2llr(\theta_0) & = 2\times16\times\text{log}(\frac{2+1}{4\times2})^2 = 3.769 < \chi^2_{1,0.95} = 3.84\\
\text{ We do not reject }&\text{ the null hypothesis at the } 5% \text{ level.}
\end{aligned}
$$

此時如果精確計算可以獲得 $p=0.052$，從檢驗統計量的計算值我們也能看出距離拒絕零假設的拒絕域十分接近。此時可以認爲是一個臨界的 $p$ 值。所以數據提供了臨界 $p=0.052$ 的證據證明肺癌 II 期患者的倖存時間平均要少於 I 期患者。

## 總結 {#summary}

推斷是十分具有挑戰性的一個章節，我們在此做個簡單的複習和總結，用一些常見的問題來結束本章。

### 快速複習

對於收集到的**樣本數據 data**，我們需要提出一個所謂的“科學問題 scientific question”。

爲了回答這個“科學問題”，我們會設想，並提出一個合適的 **統計學模型 statistical model**，確認提出的統計學模型中的**參數 parameters**。通過樣本數據的信息對參數進行**估計 estimation**，或者進行**假設檢驗 hypothesis tests**。

統計學模型具有自己的概率分佈，通過相應的參數，和模型的分佈可以解釋觀察數據的分佈，並且利用這些信息進行我們需要的推斷。同時，我們還需要利用觀察數據對我們提出的模型是否擬合數據做出合適的**診斷**。

估計和假設檢驗，是以**似然方程**爲基礎的。通常我們會利用便於計算的對數似然(比)，進行假設檢驗。

獲得似然方程以後，我們可以用對數似然比，進一步進行推斷：

1. 確認最佳估計 $MLE$，和它的方差 (標準誤)；
2. 計算參數的點估計量，和信賴區間；
3. 爲感興趣的參數實施假設檢驗。

### 試爲下面的醫學研究問題提出合適的統計學模型

1. 在一所醫院收集了 80 名患者的血壓和體重的數據，醫生想要分析血壓 (bp) 跟體重 (weight) 之間是否有相關性。

答： 用簡單線性迴歸模型。(r.v. = random variable)

$$
Y \text{ r.v. for bp } Y_j | \text{weight}_j \stackrel{i}{\sim} N(\alpha + \beta \text{weight}, \sigma^2), j = 1,2,\cdots,80; \text{H}_0: \beta=0
$$

2. 爲了調查某市青光眼的患病率 (prevalence)，從一般人羣中隨機抽取了 100 人進行眼部檢查。

答：用二項分佈模型。

$$
K \text{ r.v. for number of people found with glaucoma } \\
K \sim \text{Bin}(100, \pi); \text{ Estimate } \pi \text{ with CI.}
$$


3. 另一個醫生拿到了 2. 的數據，打算分析這100人中青光眼的患病與否是否和血壓相關。

答：用邏輯迴歸模型。 $\text{logit}\pi = \text{log}\frac{\pi}{1-\pi}$

$$
K_i | bp_i \sim \text{Bin}(100, \pi_i), \text{logit}(\pi_i) = \alpha + \beta bp_i; \text{H}_0: \beta = 0
$$


4. 有好事者打算調查 25 名研究對象的血清膽固醇水平是否在實驗前後 (實驗時間3個月) 發生有意義的改變。

答：正態分佈模型，單樣本 $t$ 檢驗。

$$
D \text{ r.v. for cholesterol change; } D_j \stackrel{i.i.d}{\sim} N(\delta, \sigma^2), j= 1,\cdots,25; \text{H}_0: \delta = 0\\
\text{Where } D_j = \text{chol}_{j,3m} - \text{chol}_{j,entry}
$$

5. 前一題的好事者，打算進一步分析膽固醇水平的變化在某些進行特殊飲食的觀察對象中是否更加顯著。

答：簡單線性迴歸模型。

$$
D_j | \text{diet}_j \stackrel{i}{\sim} N(\alpha + \beta \text{diet}_j, \sigma^2), j=1,\cdots,25; \text{H}_0: \beta = 0
$$

6. 某降壓藥物已知能有效地降低高血壓患者的血壓。某項實驗將收集來的高血壓患者分成 6 個小組，每組給予的藥物劑量不同，最低 1 毫克每次，最高 6 毫克每次，每組相差 1 毫克劑量。研究者希望通過實驗確定該藥物的降壓效果是否在某個劑量時達到最大，如果沒有，是否降壓藥物的效果隨着劑量增加而增加。

$$
\begin{aligned}
& bp_j | \text{dose}_j \stackrel{\cdot}{\sim} N(\alpha + \beta\text{dose}_j + \gamma\text{dose}^2_j, \sigma^2), j=1,\cdots,n;\\
\text{1) test } & \text{ H}_0: \gamma=0; \text{ if do not reject, then do next test } \\
& bp_j | \text{dose}_j \stackrel{\cdot}{\sim} N(\alpha + \beta\text{dose}_j, \sigma^2)
\text{2) test } & \text{ H}_0: \beta=0
\end{aligned}
$$

### 醫生來找統計學家問問題

7. 一個**“臨牀醫生”**來找你問了這樣的一個常見的問題：當我們使用 $t$ 檢驗的時候，爲什麼前提假設是數據服從 **正態分佈**? 而不使用**服從 $t$ 分佈** 這樣的前提條件，因爲我們實施該檢驗的時候明明就在用 $t$ 分佈？

答：我們從未假定**觀察數據服從 $t$ 分佈**，我們假定的前提是檢驗統計量，也就是樣本均值和標準誤服從 $t$ 分佈。因爲我們不知道收集獲得的數據來自的人羣的方差是多少，需要使用樣本數據對方差也進行估計的時候，不得已而必須使用 $t$ 分佈來獲得估計的樣本均值的標準誤差，用於計算信賴區間和實施假設檢驗。

8. 還是那個有好奇心的**“臨牀醫生”**又來問一個弱智問題：當我們使用正態分佈近似法對一個服從二項分佈的比例的單樣本檢驗的時候，我們把計算的檢驗統計量拿去跟正態分佈的特徵值作比較。然而，不用正態分佈近似，直接對連續型變量實施單樣本 $t$ 檢驗的時候卻把計算的檢驗統計量拿去和 $t$ 分佈的特徵值作比較，這是爲什麼？

答：對連續型變量實施單樣本 $t$ 檢驗的時候，我們需要用樣本數據同時估計均值和標準誤。但是對於二項分佈的數據來說，它的樣本比例的標準誤是總體比例的一個方程，所以只要用樣本比例估計總體比例以後，總體的標準誤就已經可以知道，不必再作估計。所以，二項分佈的正態近似法就真的使用標準正態分佈的特徵值，但是連續型變量的總體標準誤同時被估計，它的不確定性也要考慮進來，只能使用 $t$ 分佈。


9. 某**“臨牀醫生”**假裝很熱心想學習統計跑來問問題：該醫生實施的臨牀試驗，比較病例和對照之間某指標是否不同。但是，病例組看上去的年齡似乎比對照組要高一些，該醫生記得自己統計課上聽老師說過混雜因素的知識。所以他跑回家自己實施了一下病例組和對照組之間年齡是否有差別的 $t$ 檢驗，結果顯示病例組對照組的年齡沒有顯著性差異。所以他認爲可以從線性模型中去掉年齡這一變量。但是身爲統計學家的你堅持必須要保留年齡在模型裏。所以醫生問你是否關心年齡有差別所以才堅持要調整年齡。你的回答是“對不起大哥，我對病例對照之間的年齡差是否有統計學意義完全沒有興趣。”醫生更加困惑了。$\text{variable}_i = \alpha + \beta\text{patient}_i + \gamma\text{age}_i + \varepsilon_i$

答：年齡是否會混雜了病人分組和指標之間的關係，**不是通過比較兩組來自的人羣的年齡是否有差別來判斷的**。如果**樣本的年齡有差別**，就很有可能會對你想要分析的關係造成混淆。因爲你進行的年齡均值是否有差異的 $t$ 檢驗，比較的並不是樣本年齡的差別，而是用樣本估計來自的人羣的年齡之間的比較。
